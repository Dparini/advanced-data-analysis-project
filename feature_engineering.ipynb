{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Feature Engineering & Preprocessing\n",
    "\n",
    "- Create macro-areas (geographic regions)\n",
    "- Create macro-sectors (market categories)\n",
    "- Final feature engineering\n",
    "- Data preparation for modeling\n",
    "- Handle class imbalance\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# Autoreload everything\n",
    "%autoreload 2\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from helper import get_project_root\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "RANDOM_STATE = 2\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"✓ Libraries loaded\")\n",
    "\n",
    "PROJECT_ROOT: Path = get_project_root()\n",
    "DATA_DIR: Path = PROJECT_ROOT / \"data\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from Notebook 1\n",
    "\n",
    "finale_usa = pd.read_csv(config.PROCESSED_PATH / 'finale_usa_real_funding.csv', low_memory=False)\n",
    "\n",
    "# Parse dates\n",
    "for col in ['founded_at']:\n",
    "    if col in finale_usa.columns:\n",
    "        finale_usa[col] = pd.to_datetime(finale_usa[col])\n",
    "\n",
    "print(f\"Final: {finale_usa.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Remove Missing Values\n",
    "\n",
    "Create clean dataset without NAs for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Quality and Sample Selection\n",
    "\n",
    "After merging with Jay Ritter's IPO database and filtering for data \n",
    "completeness, our final sample consists of 15,476 US startups with \n",
    "verified funding dates and complete covariate information.\n",
    "\n",
    "We removed 21.1% of observations (4,147 startups) due to missing \n",
    "critical variables:\n",
    "- 19.6% lacked founding dates (essential for time-to-event calculation)\n",
    "- 3.0% lacked sector classification (key covariate)\n",
    "\n",
    "This conservative approach ensures data quality for survival analysis,\n",
    "though it disproportionately affects older IPOs (34.5% loss in IPO \n",
    "sample) due to incomplete historical records. Our final sample of \n",
    "269 IPOs remains sufficient for statistical inference (comparable to \n",
    "prior studies: Gompers & Lerner 2000: N=205; Hochberg et al. 2007: N=198)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# STEP 1: Check missing values BEFORE cleaning\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nMISSING VALUES ANALYSIS (Before):\")\n",
    "\n",
    "# Critical columns for survival analysis\n",
    "critical_cols = ['status', 'founded_at', 'first_funding_year', 'category_code', \n",
    "                 'country_code', 'funding_total_usd', 'funding_rounds']\n",
    "\n",
    "# Optional columns (nice to have but not critical)\n",
    "optional_cols = ['milestones', 'relationships', 'state_code']\n",
    "\n",
    "# Check critical columns\n",
    "print(\"\\nCritical columns:\")\n",
    "for col in critical_cols:\n",
    "    if col in finale_usa.columns:\n",
    "        missing = finale_usa[col].isna().sum()\n",
    "        pct = 100 * missing / len(finale_usa)\n",
    "        status = \"not okay\" if missing > 0 else \"okay\"\n",
    "        print(f\"{status} {col:25s} {missing:>6,} missing ({pct:>5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{col:25s} NOT FOUND\")\n",
    "\n",
    "# Check optional columns\n",
    "print(\"\\nOptional columns:\")\n",
    "for col in optional_cols:\n",
    "    if col in finale_usa.columns:\n",
    "        missing = finale_usa[col].isna().sum()\n",
    "        pct = 100 * missing / len(finale_usa)\n",
    "        print(f\"{col:25s} {missing:>6,} missing ({pct:>5.1f}%)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# STEP 2: Remove missing values (CONSERVATIVE approach)\n",
    "# ---------------------------------------------------------------------------\n",
    "# Start with full dataset\n",
    "finale_clean = finale_usa.copy()\n",
    "initial_rows = len(finale_clean)\n",
    "\n",
    "# 1. Remove rows with missing status (CRITICAL)\n",
    "if 'status' in finale_clean.columns:\n",
    "    before = len(finale_clean)\n",
    "    finale_clean = finale_clean[finale_clean['status'].notna()]\n",
    "    removed = before - len(finale_clean)\n",
    "    print(f\"Removed {removed:,} rows with missing status\")\n",
    "\n",
    "# 2. Remove rows with empty/missing category_code (CRITICAL)\n",
    "if 'category_code' in finale_clean.columns:\n",
    "    before = len(finale_clean)\n",
    "    finale_clean = finale_clean[\n",
    "        finale_clean['category_code'].notna() & \n",
    "        (finale_clean['category_code'] != '')\n",
    "    ]\n",
    "    removed = before - len(finale_clean)\n",
    "    print(f\"Removed {removed:,} rows with missing category_code\")\n",
    "\n",
    "# 3. Remove rows with empty/missing country_code (should be all USA)\n",
    "if 'country_code' in finale_clean.columns:\n",
    "    before = len(finale_clean)\n",
    "    finale_clean = finale_clean[\n",
    "        finale_clean['country_code'].notna() & \n",
    "        (finale_clean['country_code'] != '')\n",
    "    ]\n",
    "    removed = before - len(finale_clean)\n",
    "    print(f\"Removed {removed:,} rows with missing country_code\")\n",
    "\n",
    "# 4. Remove rows with missing founded_at (CRITICAL for survival)\n",
    "if 'founded_at' in finale_clean.columns:\n",
    "    before = len(finale_clean)\n",
    "    finale_clean = finale_clean[finale_clean['founded_at'].notna()]\n",
    "    removed = before - len(finale_clean)\n",
    "    print(f\"Removed {removed:,} rows with missing founded_at\")\n",
    "\n",
    "# 5. Remove rows with missing first_funding_year (CRITICAL - already filtered)\n",
    "if 'first_funding_year' in finale_clean.columns:\n",
    "    before = len(finale_clean)\n",
    "    finale_clean = finale_clean[finale_clean['first_funding_year'].notna()]\n",
    "    removed = before - len(finale_clean)\n",
    "    print(f\"Removed {removed:,} rows with missing first_funding_year\")\n",
    "\n",
    "# 6. Handle funding_total_usd (FILL 0 instead of removing)\n",
    "if 'funding_total_usd' in finale_clean.columns:\n",
    "    before_missing = finale_clean['funding_total_usd'].isna().sum()\n",
    "    finale_clean['funding_total_usd'] = finale_clean['funding_total_usd'].fillna(0)\n",
    "    print(f\"Filled {before_missing:,} missing funding_total_usd with 0\")\n",
    "    \n",
    "    # Recalculate log_fund_tot\n",
    "    finale_clean['log_fund_tot'] = finale_clean['funding_total_usd'].apply(\n",
    "        lambda x: 0 if x == 0 else np.log(x)\n",
    "    )\n",
    "\n",
    "# 7. Handle funding_rounds (FILL 0 instead of removing)\n",
    "if 'funding_rounds' in finale_clean.columns:\n",
    "    before_missing = finale_clean['funding_rounds'].isna().sum()\n",
    "    finale_clean['funding_rounds'] = finale_clean['funding_rounds'].fillna(0)\n",
    "    print(f\"Filled {before_missing:,} missing funding_rounds with 0\")\n",
    "\n",
    "# 8. Handle milestones (FILL 0 instead of removing)\n",
    "if 'milestones' in finale_clean.columns:\n",
    "    before_missing = finale_clean['milestones'].isna().sum()\n",
    "    finale_clean['milestones'] = finale_clean['milestones'].fillna(0)\n",
    "    print(f\"Filled {before_missing:,} missing milestones with 0\")\n",
    "\n",
    "# 9. Handle relationships (FILL 0 instead of removing)\n",
    "if 'relationships' in finale_clean.columns:\n",
    "    before_missing = finale_clean['relationships'].isna().sum()\n",
    "    finale_clean['relationships'] = finale_clean['relationships'].fillna(0)\n",
    "    print(f\"Filled {before_missing:,} missing relationships with 0\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# STEP 3: Drop unnecessary columns\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cols_to_drop = ['logo_width', 'logo_height', 'id', 'object_id', 'state_code', 'valuation_amount']\n",
    "dropped = [col for col in cols_to_drop if col in finale_clean.columns]\n",
    "\n",
    "if dropped:\n",
    "    finale_clean = finale_clean.drop(columns=dropped)\n",
    "    print(f\"\\nDropped {len(dropped)} unnecessary columns: {dropped}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# STEP 4: Convert data types (optimize memory)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nTYPE CONVERSIONS:\")\n",
    "\n",
    "# Categorical columns\n",
    "categorical_cols = ['status', 'category_code', 'country_code', 'market_cycle']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in finale_clean.columns:\n",
    "        finale_clean[col] = finale_clean[col].astype('category')\n",
    "        print(f\"{col} category\")\n",
    "\n",
    "# Integer columns\n",
    "integer_cols = ['person_financed', 'startup_financed', 'fin_org_financed', \n",
    "                'funding_rounds', 'milestones', 'relationships', 'num_prodotti',\n",
    "                'num_acquisizioni_effettuate']\n",
    "\n",
    "for col in integer_cols:\n",
    "    if col in finale_clean.columns:\n",
    "        finale_clean[col] = finale_clean[col].fillna(0).astype(int)\n",
    "        print(f\"{col} int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Data Strategy\n",
    "\n",
    "We employed complete-case analysis for critical survival variables \n",
    "(status, founded_at, category_code), excluding 4,342 observations \n",
    "(22.1%) with missing values. Optional variables (funding metrics, \n",
    "network measures) were zero-imputed where missingness indicated \n",
    "absence. We excluded state_code, despite 98.6% availability, to \n",
    "avoid model complexity (49 dummy variables) and redundancy with \n",
    "market-timing proxies that already capture geographic effects. \n",
    "Final sample: N=15,281 (Allison, 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Variable Selection\n",
    "\n",
    "We excluded IPO valuation from our analysis as it is only observable \n",
    "after the IPO event occurs, creating a fundamental issue of temporal \n",
    "causality. Including post-event variables would violate the assumptions \n",
    "of survival analysis, where covariates must be measured before or at \n",
    "the time of the event (Cox, 1972).\n",
    "\n",
    "Furthermore, valuation data was available for only 1.9% of observations \n",
    "(269 IPOs out of 15,476 startups), making it unsuitable as a predictor \n",
    "in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# STEP 5: Final verification\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\"CLEANING SUMMARY\")\n",
    "\n",
    "print(f\"\\nDATASET SIZE:\")\n",
    "print(f\"Before:{initial_rows:,} rows\")\n",
    "print(f\"After:{len(finale_clean):,} rows\")\n",
    "print(f\"Removed:{initial_rows - len(finale_clean):,} rows ({100*(initial_rows - len(finale_clean))/initial_rows:.1f}%)\")\n",
    "\n",
    "print(f\"\\nFINAL SHAPE:\")\n",
    "print(f\"Rows: {len(finale_clean):,}\")\n",
    "print(f\"Columns: {finale_clean.shape[1]}\")\n",
    "print(f\"Memory: {finale_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(f\"\\nMISSING VALUES REMAINING:\")\n",
    "total_missing = finale_clean.isnull().sum().sum()\n",
    "print(f\"Total: {total_missing:,}\")\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(f\"\\n  Columns with missing values:\")\n",
    "    for col in finale_clean.columns:\n",
    "        missing = finale_clean[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            pct = 100 * missing / len(finale_clean)\n",
    "            print(f\"{col:25s} {missing:>6,} ({pct:>5.1f}%)\")\n",
    "else:\n",
    "    print(f\"NO MISSING VALUES!\")\n",
    "\n",
    "print(f\"\\nSTATUS DISTRIBUTION:\")\n",
    "for status, count in finale_clean['status'].value_counts().items():\n",
    "    pct = 100 * count / len(finale_clean)\n",
    "    print(f\"{status:12s} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Success metrics\n",
    "success_rate = 100 * finale_clean['status'].isin(['ipo', 'acquired']).mean()\n",
    "print(f\"\\nSuccess rate: {success_rate:.2f}%\")\n",
    "\n",
    "finale_no_na = finale_clean.copy()\n",
    "print(\"Data cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. UNIVARIATE OUTLIERS (Tukey's fences)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def detect_outliers_iqr(data, column, k=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method (Tukey's fences).\n",
    "    k=3 for extreme outliers (vs k=1.5 for mild)\n",
    "    \"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_fence = Q1 - k * IQR\n",
    "    upper_fence = Q3 + k * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_fence) | (data[column] > upper_fence)]\n",
    "    \n",
    "    return outliers, lower_fence, upper_fence\n",
    "\n",
    "# Check funding_total_usd\n",
    "funded = finale_no_na[finale_no_na['funding_total_usd'] > 0].copy()\n",
    "\n",
    "outliers_funding, lower, upper = detect_outliers_iqr(funded, 'funding_total_usd', k=3)\n",
    "\n",
    "print(f\"\\nFunding distribution (non-zero):\")\n",
    "print(f\"Mean:${funded['funding_total_usd'].mean()/1e6:.1f}M\")\n",
    "print(f\"Median:${funded['funding_total_usd'].median()/1e6:.1f}M\")\n",
    "print(f\"Q1:${funded['funding_total_usd'].quantile(0.25)/1e6:.1f}M\")\n",
    "print(f\"Q3:${funded['funding_total_usd'].quantile(0.75)/1e6:.1f}M\")\n",
    "print(f\"Max:${funded['funding_total_usd'].max()/1e9:.2f}B\")\n",
    "\n",
    "print(f\"\\nOutliers detected (k=3):\")\n",
    "print(f\"Lower fence:${lower/1e6:.1f}M\")\n",
    "print(f\"Upper fence:${upper/1e6:.1f}M\")\n",
    "print(f\"N outliers:{len(outliers_funding)} ({100*len(outliers_funding)/len(funded):.1f}%)\")\n",
    "\n",
    "if len(outliers_funding) > 0:\n",
    "    print(f\"\\nTop 10 extreme funding amounts:\")\n",
    "    top_10 = outliers_funding.nlargest(10, 'funding_total_usd')[\n",
    "        ['funding_total_usd', 'status', 'category_code', 'funding_rounds']\n",
    "    ]\n",
    "    \n",
    "    for idx, row in top_10.iterrows():\n",
    "        print(f\"${row['funding_total_usd']/1e9:.2f}B  \"\n",
    "              f\"{row['status']:10s}  {row['category_code']:20s}  \"\n",
    "              f\"{row['funding_rounds']:.0f} rounds\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. RELATIONSHIPS OUTLIERS\n",
    "# ---------------------------------------------------------------------------\n",
    "with_relationships = finale_no_na[finale_no_na['relationships'] > 0].copy()\n",
    "\n",
    "outliers_rel, lower_rel, upper_rel = detect_outliers_iqr(\n",
    "    with_relationships, 'relationships', k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nRelationships distribution (non-zero):\")\n",
    "print(f\"Mean:{with_relationships['relationships'].mean():.1f}\")\n",
    "print(f\"Median:{with_relationships['relationships'].median():.0f}\")\n",
    "print(f\"Max:{with_relationships['relationships'].max():.0f}\")\n",
    "\n",
    "print(f\"\\nOutliers: {len(outliers_rel)} ({100*len(outliers_rel)/len(with_relationships):.1f}%)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. FUNDING ROUNDS OUTLIERS\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "with_rounds = finale_no_na[finale_no_na['funding_rounds'] > 0].copy()\n",
    "\n",
    "outliers_rounds, lower_rounds, upper_rounds = detect_outliers_iqr(\n",
    "    with_rounds, 'funding_rounds', k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nFunding rounds distribution (non-zero):\")\n",
    "print(f\"Mean:{with_rounds['funding_rounds'].mean():.1f}\")\n",
    "print(f\"Median:{with_rounds['funding_rounds'].median():.0f}\")\n",
    "print(f\"Max:{with_rounds['funding_rounds'].max():.0f}\")\n",
    "\n",
    "print(f\"\\nOutliers: {len(outliers_rounds)} ({100*len(outliers_rounds)/len(with_rounds):.1f}%)\")\n",
    "\n",
    "print(f\"\\nDECISION:KEEP ALL OUTLIERS\")\n",
    "\n",
    "# =============================================================================\n",
    "# OUTLIER SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "# Funding outliers\n",
    "funded = finale_no_na[finale_no_na['funding_total_usd'] > 0]\n",
    "Q1 = funded['funding_total_usd'].quantile(0.25)\n",
    "Q3 = funded['funding_total_usd'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_fence = Q3 + 3 * IQR\n",
    "n_outliers_funding = (funded['funding_total_usd'] > upper_fence).sum()\n",
    "\n",
    "print(f\"\\nFunding outliers (Tukey k=3):\")\n",
    "print(f\"N outliers: {n_outliers_funding:,} ({100*n_outliers_funding/len(funded):.1f}%)\")\n",
    "print(f\"Upper fence: ${upper_fence/1e6:.1f}M\")\n",
    "print(f\"Max funding: ${funded['funding_total_usd'].max()/1e9:.2f}B\")\n",
    "\n",
    "# Relationships outliers\n",
    "with_rel = finale_no_na[finale_no_na['relationships'] > 0]\n",
    "Q1_rel = with_rel['relationships'].quantile(0.25)\n",
    "Q3_rel = with_rel['relationships'].quantile(0.75)\n",
    "IQR_rel = Q3_rel - Q1_rel\n",
    "upper_rel = Q3_rel + 3 * IQR_rel\n",
    "n_outliers_rel = (with_rel['relationships'] > upper_rel).sum()\n",
    "\n",
    "print(f\"\\nRelationships outliers:\")\n",
    "print(f\"N outliers:{n_outliers_rel:,} ({100*n_outliers_rel/len(with_rel):.1f}%)\")\n",
    "print(f\"Max:{with_rel['relationships'].max():.0f}\")\n",
    "\n",
    "print(f\"\\nDECISION: KEEP ALL OUTLIERS (Cox PH is robust)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Geographic Variables\n",
    "\n",
    "Given our USA-only sample (N=15,476), we excluded geographic \n",
    "macro-area classification. All observations share the same \n",
    "country-level geography (country_code = 'USA'), making regional \n",
    "aggregation redundant. \n",
    "\n",
    "State-level granularity was excluded earlier (Section 3.4.3) to \n",
    "reduce model complexity. Market timing variables (market_heat, \n",
    "IPO activity) already capture geographic clustering effects, as \n",
    "major startup hubs (California, Massachusetts, New York) drive \n",
    "national IPO cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create Macro-Sectors\n",
    "\n",
    "Group market sectors into 11 main categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current sectors\n",
    "print(f\"CURRENT SECTORS:\")\n",
    "print(f\"Total unique: {finale_no_na['category_code'].nunique()}\")\n",
    "print(f\"Total startups: {len(finale_no_na):,}\")\n",
    "\n",
    "# Top 20 sectors\n",
    "print(f\"\\nTOP 20 SECTORS:\")\n",
    "top_sectors = finale_no_na['category_code'].value_counts().head(20)\n",
    "\n",
    "for sector, count in top_sectors.items():\n",
    "    pct = 100 * count / len(finale_no_na)\n",
    "    ipo_count = finale_no_na[(finale_no_na['category_code'] == sector) & (finale_no_na['status'] == 'ipo')].shape[0]\n",
    "    ipo_rate = 100 * ipo_count / count if count > 0 else 0\n",
    "    \n",
    "    bar = \"█\" * int(pct / 2)\n",
    "    print(f\"{sector:20s} {count:>5,} ({pct:>5.1f}%) IPO:{ipo_rate:>4.1f}% {bar}\")\n",
    "\n",
    "# Small sectors (< 50 startups)\n",
    "small_sectors = finale_no_na['category_code'].value_counts()\n",
    "small = small_sectors[small_sectors < 50]\n",
    "\n",
    "print(f\"\\nSMALL SECTORS (< 50 startups): {len(small)}\")\n",
    "print(f\"  These need consolidation:\")\n",
    "for sector, count in small.head(10).items():\n",
    "    print(f\"{sector:20s} {count:>3} startups\")\n",
    "\n",
    "# IPO rates by sector (top performers)\n",
    "print(f\"\\nSECTORS WITH HIGHEST IPO RATE (min 100 startups):\")\n",
    "sector_stats = finale_no_na[finale_no_na['category_code'].map(finale_no_na['category_code'].value_counts() >= 100)].groupby('category_code').agg({\n",
    "    'status': lambda x: 100 * (x == 'ipo').sum() / len(x)\n",
    "}).rename(columns={'status': 'ipo_rate'}).sort_values('ipo_rate', ascending=False)\n",
    "\n",
    "for sector, ipo_rate in sector_stats.head(10).iterrows():\n",
    "    count = finale_no_na[finale_no_na['category_code'] == sector].shape[0]\n",
    "    print(f\"{sector:20s} {ipo_rate['ipo_rate']:>5.2f}% ({count:>4,} startups)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sector Consolidation\n",
    "\n",
    "We consolidated 42 original sectors into 11 macro-sectors to reduce \n",
    "model complexity while preserving meaningful industry distinctions. \n",
    "Classification followed a hybrid approach combining GICS industry \n",
    "standards with VC-specific categories (CB Insights taxonomy).\n",
    "\n",
    "Key consolidation decisions:\n",
    "- **TECH CORE** (software, web, mobile, enterprise): 39.8% of sample\n",
    "- **LIFE SCIENCES** (biotech, health, medical): 15.3% of sample\n",
    "- **ENTERPRISE INFRA** (network, security, consulting): 4.5% of sample \n",
    "  but highest success rate (18.1%)\n",
    "\n",
    "Small sectors (<50 observations) were consolidated into 'OTHER' (2.8%), \n",
    "ensuring all macro-sectors have sufficient statistical power for \n",
    "survival modeling. IPO rates vary significantly across sectors \n",
    "(range: 0.8%-3.5%), justifying sector stratification in Cox models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_macro_sector(category):\n",
    "    \"\"\"\n",
    "    Consolidate 42 sectors into 10 macro-sectors\n",
    "    Based on actual distribution + IPO rates\n",
    "    \"\"\"\n",
    "    \n",
    "    # TECH CORE (Software, Web, Mobile) - Largest group, 16.7%+9.5%+6.9% = 33.1%\n",
    "    tech_core = ['software', 'web', 'mobile', 'enterprise', 'saas']\n",
    "    \n",
    "    # LIFE SCIENCES (Biotech, Healthcare) - High IPO rate (3.6%, 2.2%)\n",
    "    life_sciences = ['biotech', 'health', 'medical', 'healthcare']\n",
    "    \n",
    "    # HARDWARE & SEMICONDUCTORS - Very high IPO rate (5.4%)\n",
    "    hardware = ['hardware', 'semiconductor', 'manufacturing', 'automotive', 'transportation', 'nanotech']\n",
    "    \n",
    "    # FINTECH (Finance, Analytics, Payments)\n",
    "    fintech = ['finance', 'analytics', 'payments']\n",
    "    \n",
    "    # CLEANTECH - High IPO rate (3.1%)\n",
    "    cleantech = ['cleantech', 'green', 'solar', 'energy']\n",
    "    \n",
    "    # CONSUMER (E-commerce, Retail, Travel)\n",
    "    consumer = ['ecommerce', 'shopping', 'fashion', 'sports', 'travel', 'hospitality', 'food']\n",
    "    \n",
    "    # MEDIA & ENTERTAINMENT (Games, Video, Music)\n",
    "    media = ['games_video', 'photo_video', 'music', 'entertainment']\n",
    "    \n",
    "    # COMMUNICATION & MARKETING (Advertising, Social)\n",
    "    communication = ['advertising', 'social', 'messaging', 'public_relations', 'news']\n",
    "    \n",
    "    # ENTERPRISE INFRASTRUCTURE - High IPO rate (4.6%, 3.9%)\n",
    "    enterprise_infra = ['network_hosting', 'security', 'consulting', 'legal']\n",
    "    \n",
    "    # EDTECH & COMMUNITY\n",
    "    edtech = ['education', 'search', 'local']\n",
    "    \n",
    "    # OTHER (Small sectors < 50 startups)\n",
    "    other = ['design', 'pets', 'government', 'other']\n",
    "    \n",
    "    # Assign\n",
    "    if category in tech_core:\n",
    "        return 'TECH CORE'\n",
    "    elif category in life_sciences:\n",
    "        return 'LIFE SCIENCES'\n",
    "    elif category in hardware:\n",
    "        return 'HARDWARE'\n",
    "    elif category in fintech:\n",
    "        return 'FINTECH'\n",
    "    elif category in cleantech:\n",
    "        return 'CLEANTECH'\n",
    "    elif category in consumer:\n",
    "        return 'CONSUMER'\n",
    "    elif category in media:\n",
    "        return 'MEDIA'\n",
    "    elif category in communication:\n",
    "        return 'COMMUNICATION'\n",
    "    elif category in enterprise_infra:\n",
    "        return 'ENTERPRISE INFRA'\n",
    "    elif category in edtech:\n",
    "        return 'EDTECH'\n",
    "    elif category in other:\n",
    "        return 'OTHER'\n",
    "    else:\n",
    "        return 'OTHER'  # Catch-all\n",
    "\n",
    "# Apply macro-sector\n",
    "finale_no_na['macro_settore'] = finale_no_na['category_code'].apply(assign_macro_sector)\n",
    "finale_no_na['macro_settore'] = finale_no_na['macro_settore'].astype('category')\n",
    "\n",
    "# Show distribution\n",
    "print(\"MACRO-SECTOR DISTRIBUTION:\")\n",
    "\n",
    "macro_dist = finale_no_na['macro_settore'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "for sector, count in macro_dist.items():\n",
    "    pct = 100 * count / len(finale_no_na)\n",
    "    \n",
    "    # IPO rate per macro-sector\n",
    "    subset = finale_no_na[finale_no_na['macro_settore'] == sector]\n",
    "    ipo_count = (subset['status'] == 'ipo').sum()\n",
    "    ipo_rate = 100 * ipo_count / count\n",
    "    \n",
    "    # Success rate (IPO + M&A)\n",
    "    success_count = subset['status'].isin(['ipo', 'acquired']).sum()\n",
    "    success_rate = 100 * success_count / count\n",
    "    \n",
    "    bar = \"█\" * int(pct / 2)\n",
    "    print(f\"{sector:20s} {count:>6,} ({pct:>5.1f}%) IPO:{ipo_rate:>4.1f}% Success:{success_rate:>5.1f}% {bar}\")\n",
    "\n",
    "# Verify OTHER is not too large\n",
    "other_pct = 100 * (finale_no_na['macro_settore'] == 'OTHER').sum() / len(finale_no_na)\n",
    "print(f\"\\n{'-'*70}\")\n",
    "if other_pct > 15:\n",
    "    print(f\"WARNING: 'OTHER' is {other_pct:.1f}% - consider refinement\")\n",
    "else:\n",
    "    print(f\"'OTHER' is {other_pct:.1f}% - acceptable\")\n",
    "\n",
    "print(f\"\\nMacro-sectors created: {finale_no_na['macro_settore'].nunique()} categories\")\n",
    "print(f\"Original sectors: 42 Consolidated to: {finale_no_na['macro_settore'].nunique()}\")\n",
    "\n",
    "# Show which original sectors went into each macro-sector\n",
    "print(f\"\\nMAPPING DETAILS:\")\n",
    "\n",
    "for macro in macro_dist.index:\n",
    "    original = finale_no_na[finale_no_na['macro_settore'] == macro]['category_code'].unique()\n",
    "    n_original = len(original)\n",
    "    print(f\"\\n{macro} ({n_original} sectors):\")\n",
    "    \n",
    "    # Show top 5 original sectors in this macro\n",
    "    top_original = finale_no_na[finale_no_na['macro_settore'] == macro]['category_code'].value_counts().head(5)\n",
    "    for orig, count in top_original.items():\n",
    "        if count > 0:  # Only sectors with count > 0\n",
    "            pct_of_macro = 100 * count / len(subset)\n",
    "            print(f\"{orig:20s} {count:>5,} ({pct_of_macro:>5.1f}% of {macro})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "print(f\"{'Macro-Sector':20s} {'N':>7s} {'IPO':>6s} {'M&A':>6s} {'Closed':>6s} {'IPO%':>6s} {'M&A%':>6s} {'Success%':>8s}\")\n",
    "\n",
    "# Calculate and display\n",
    "results = []\n",
    "\n",
    "for macro in finale_no_na['macro_settore'].cat.categories:\n",
    "    subset = finale_no_na[finale_no_na['macro_settore'] == macro]\n",
    "    \n",
    "    n = len(subset)\n",
    "    n_ipo = (subset['status'] == 'ipo').sum()\n",
    "    n_ma = (subset['status'] == 'acquired').sum()\n",
    "    n_closed = (subset['status'] == 'closed').sum()\n",
    "    \n",
    "    ipo_rate = 100 * n_ipo / n\n",
    "    ma_rate = 100 * n_ma / n\n",
    "    success_rate = ipo_rate + ma_rate\n",
    "    \n",
    "    results.append({\n",
    "        'macro': macro,\n",
    "        'n': n,\n",
    "        'ipo': n_ipo,\n",
    "        'ma': n_ma,\n",
    "        'closed': n_closed,\n",
    "        'ipo_rate': ipo_rate,\n",
    "        'ma_rate': ma_rate,\n",
    "        'success_rate': success_rate\n",
    "    })\n",
    "\n",
    "# Sort by IPO rate\n",
    "results_sorted = sorted(results, key=lambda x: x['ipo_rate'], reverse=True)\n",
    "\n",
    "# Display\n",
    "for r in results_sorted:\n",
    "    print(f\"{r['macro']:20s} {r['n']:>7,} {r['ipo']:>6} {r['ma']:>6} {r['closed']:>6} \"\n",
    "          f\"{r['ipo_rate']:>5.1f}% {r['ma_rate']:>5.1f}% {r['success_rate']:>7.1f}%\")\n",
    "\n",
    "# Highlight insights\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "\n",
    "best_ipo = max(results_sorted, key=lambda x: x['ipo_rate'])\n",
    "best_success = max(results_sorted, key=lambda x: x['success_rate'])\n",
    "worst_ipo = min(results_sorted, key=lambda x: x['ipo_rate'])\n",
    "\n",
    "print(f\"Highest IPO rate:{best_ipo['macro']:20s} {best_ipo['ipo_rate']:.1f}%\")\n",
    "print(f\"Highest success rate:{best_success['macro']:20s} {best_success['success_rate']:.1f}%\")\n",
    "print(f\"Lowest IPO rate:{worst_ipo['macro']:20s} {worst_ipo['ipo_rate']:.1f}%\")\n",
    "\n",
    "# Compare with overall\n",
    "overall_ipo_rate = 100 * (finale_no_na['status'] == 'ipo').mean()\n",
    "overall_success_rate = 100 * finale_no_na['status'].isin(['ipo', 'acquired']).mean()\n",
    "\n",
    "print(f\"\\nOverall (all sectors):\")\n",
    "print(f\"IPO rate:{overall_ipo_rate:.2f}%\")\n",
    "print(f\"Success rate:{overall_success_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIX: Handle missing values in categorical variables\n",
    "# =============================================================================\n",
    "\n",
    "# Check which derived categories have missing values\n",
    "derived_cats = ['relationships_category', 'rounds_category', 'products_category', \n",
    "                'funding_quartile', 'market_heat_quartile', 'network_connectivity']\n",
    "\n",
    "for col in derived_cats:\n",
    "    if col in finale_no_na.columns:\n",
    "        missing = finale_no_na[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"\\n{col}: {missing} missing\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# FIX 1: relationships_category\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "if 'relationships_category' in finale_no_na.columns:\n",
    "    \n",
    "    missing_before = finale_no_na['relationships_category'].isnull().sum()\n",
    "    \n",
    "    if missing_before > 0:\n",
    "        print(f\"\\nFixing relationships_category ({missing_before} missing)...\")\n",
    "        \n",
    "        # Check underlying relationships value\n",
    "        missing_idx = finale_no_na[finale_no_na['relationships_category'].isnull()].index\n",
    "        \n",
    "        for idx in missing_idx:\n",
    "            rel_value = finale_no_na.loc[idx, 'relationships']\n",
    "            print(f\"  Row {idx}: relationships = {rel_value}\")\n",
    "            \n",
    "            # Assign appropriate category based on relationships value\n",
    "            if pd.isna(rel_value) or rel_value == 0:\n",
    "                finale_no_na.loc[idx, 'relationships_category'] = 'None (0)'\n",
    "            elif rel_value <= 3:\n",
    "                finale_no_na.loc[idx, 'relationships_category'] = 'Low (1-3)'\n",
    "            elif rel_value <= 7:\n",
    "                finale_no_na.loc[idx, 'relationships_category'] = 'Medium (4-7)'\n",
    "            else:\n",
    "                finale_no_na.loc[idx, 'relationships_category'] = 'High (8+)'\n",
    "        \n",
    "        # Re-categorize to update categories\n",
    "        finale_no_na['relationships_category'] = finale_no_na['relationships_category'].astype('category')\n",
    "        \n",
    "        missing_after = finale_no_na['relationships_category'].isnull().sum()\n",
    "        print(f\"Fixed! Missing:{missing_before} -> {missing_after}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# FIX 2: Other categorical variables\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# rounds_category\n",
    "if 'rounds_category' in finale_no_na.columns:\n",
    "    missing = finale_no_na['rounds_category'].isnull().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"\\nFixing rounds_category ({missing} missing)...\")\n",
    "        \n",
    "        # Fill missing with '0' category\n",
    "        missing_idx = finale_no_na[finale_no_na['rounds_category'].isnull()].index\n",
    "        for idx in missing_idx:\n",
    "            rounds_value = finale_no_na.loc[idx, 'funding_rounds']\n",
    "            if pd.isna(rounds_value) or rounds_value == 0:\n",
    "                finale_no_na.loc[idx, 'rounds_category'] = '0'\n",
    "        \n",
    "        finale_no_na['rounds_category'] = finale_no_na['rounds_category'].astype('category')\n",
    "        print(f\"Fixed\")\n",
    "\n",
    "# products_category\n",
    "if 'products_category' in finale_no_na.columns:\n",
    "    missing = finale_no_na['products_category'].isnull().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"\\nFixing products_category ({missing} missing)...\")\n",
    "        \n",
    "        missing_idx = finale_no_na[finale_no_na['products_category'].isnull()].index\n",
    "        for idx in missing_idx:\n",
    "            prod_value = finale_no_na.loc[idx, 'num_prodotti']\n",
    "            if pd.isna(prod_value) or prod_value == 0:\n",
    "                finale_no_na.loc[idx, 'products_category'] = 'None (0)'\n",
    "        \n",
    "        finale_no_na['products_category'] = finale_no_na['products_category'].astype('category')\n",
    "        print(f\"Fixed\")\n",
    "\n",
    "# market_heat_quartile\n",
    "if 'market_heat_quartile' in finale_no_na.columns:\n",
    "    missing = finale_no_na['market_heat_quartile'].isnull().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"\\nFixing market_heat_quartile ({missing} missing)...\")\n",
    "        \n",
    "        # For quartiles, we can either drop or assign to a category\n",
    "        # Let's assign to Q2 (median) if missing\n",
    "        finale_no_na['market_heat_quartile'] = finale_no_na['market_heat_quartile'].cat.add_categories(['Unknown'])\n",
    "        finale_no_na['market_heat_quartile'] = finale_no_na['market_heat_quartile'].fillna('Unknown')\n",
    "        \n",
    "        print(f\"Fixed\")\n",
    "\n",
    "# funding_quartile (from funded companies only)\n",
    "if 'funding_quartile' in finale_no_na.columns:\n",
    "    missing = finale_no_na['funding_quartile'].isnull().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"\\nNote:funding_quartile has {missing} missing (expected for non-funded companies)\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# FINAL VERIFICATION\n",
    "# -----------------------------------------------------------------------\n",
    "total_missing = finale_no_na.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"NO MISSING VALUES - DATASET READY!\")\n",
    "else:\n",
    "    print(f\"\\nStill {total_missing} missing values:\")\n",
    "    missing_cols = finale_no_na.isnull().sum()\n",
    "    missing_cols = missing_cols[missing_cols > 0]\n",
    "    for col, count in missing_cols.items():\n",
    "        print(f\"{col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 8.1: Relationships Impact\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if 'relationships' in finale_no_na.columns:\n",
    "    \n",
    "    # Create quartile categories\n",
    "    finale_no_na['relationships_category'] = pd.cut(\n",
    "        finale_no_na['relationships'],\n",
    "        bins=[-0.1, 0, 3, 7, 1000],\n",
    "        labels=['None (0)', 'Low (1-3)', 'Medium (4-7)', 'High (8+)']\n",
    "    )\n",
    "    \n",
    "    print(\"Created: relationships_category\") \n",
    "\n",
    "    rel_stats = []\n",
    "    \n",
    "    for cat in finale_no_na['relationships_category'].cat.categories:\n",
    "        subset = finale_no_na[finale_no_na['relationships_category'] == cat]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            n = len(subset)\n",
    "            ipo_rate = 100 * (subset['status'] == 'ipo').mean()\n",
    "            ma_rate = 100 * (subset['status'] == 'acquired').mean()\n",
    "            avg_funding = subset[subset['funding_total_usd'] > 0]['funding_total_usd'].mean()\n",
    "            \n",
    "            rel_stats.append({\n",
    "                'category': cat,\n",
    "                'n': n,\n",
    "                'ipo_rate': ipo_rate,\n",
    "                'ma_rate': ma_rate,\n",
    "                'success_rate': ipo_rate + ma_rate,\n",
    "                'avg_funding': avg_funding\n",
    "            })\n",
    "    \n",
    "    rel_df = pd.DataFrame(rel_stats)\n",
    "    \n",
    "    print(f\"\\n{'Category':<15} {'N':>8} {'IPO%':>7} {'M&A%':>7} {'Success%':>9} {'Avg Funding':>12}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for _, row in rel_df.iterrows():\n",
    "        print(f\"{row['category']:<15} {int(row['n']):>8,} {row['ipo_rate']:>6.2f}% \"\n",
    "              f\"{row['ma_rate']:>6.2f}% {row['success_rate']:>8.2f}% ${row['avg_funding']/1e6:>10.1f}M\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8.2: Milestones Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if 'milestones' in finale_no_na.columns:\n",
    "    \n",
    "    # Binary: has milestones or not\n",
    "    finale_no_na['has_milestones'] = (finale_no_na['milestones'] > 0).astype(int)\n",
    "    \n",
    "    print(\"Created: has_milestones\")\n",
    "\n",
    "    milestone_comparison = []\n",
    "    \n",
    "    for has_ms in [0, 1]:\n",
    "        label = \"With milestones\" if has_ms == 1 else \"No milestones\"\n",
    "        subset = finale_no_na[finale_no_na['has_milestones'] == has_ms]\n",
    "        \n",
    "        n = len(subset)\n",
    "        ipo_rate = 100 * (subset['status'] == 'ipo').mean()\n",
    "        ma_rate = 100 * (subset['status'] == 'acquired').mean()\n",
    "        \n",
    "        milestone_comparison.append({\n",
    "            'group': label,\n",
    "            'n': n,\n",
    "            'ipo_rate': ipo_rate,\n",
    "            'ma_rate': ma_rate,\n",
    "            'success_rate': ipo_rate + ma_rate\n",
    "        })\n",
    "    \n",
    "    ms_df = pd.DataFrame(milestone_comparison)\n",
    "    \n",
    "    print(f\"\\n{'Group':<18} {'N':>8} {'IPO%':>7} {'M&A%':>7} {'Success%':>9}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for _, row in ms_df.iterrows():\n",
    "        print(f\"{row['group']:<18} {int(row['n']):>8,} {row['ipo_rate']:>6.2f}% \"\n",
    "              f\"{row['ma_rate']:>6.2f}% {row['success_rate']:>8.2f}%\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8.3: Funding Sources Comparison\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "funding_sources = ['person_financed', 'startup_financed', 'fin_org_financed']\n",
    "available_sources = [s for s in funding_sources if s in finale_no_na.columns]\n",
    "\n",
    "if len(available_sources) > 0:\n",
    "    \n",
    "    source_stats = []\n",
    "    \n",
    "    for source in available_sources:\n",
    "        \n",
    "        # Funded by this source\n",
    "        funded_by = finale_no_na[finale_no_na[source] == 1]\n",
    "        not_funded_by = finale_no_na[finale_no_na[source] == 0]\n",
    "        \n",
    "        source_name = source.replace('_financed', '').replace('_', ' ').title()\n",
    "        \n",
    "        for label, subset in [('With', funded_by), ('Without', not_funded_by)]:\n",
    "            n = len(subset)\n",
    "            ipo_rate = 100 * (subset['status'] == 'ipo').mean()\n",
    "            ma_rate = 100 * (subset['status'] == 'acquired').mean()\n",
    "            \n",
    "            source_stats.append({\n",
    "                'source': f\"{source_name} ({label})\",\n",
    "                'n': n,\n",
    "                'ipo_rate': ipo_rate,\n",
    "                'ma_rate': ma_rate,\n",
    "                'success_rate': ipo_rate + ma_rate\n",
    "            })\n",
    "    \n",
    "    sources_df = pd.DataFrame(source_stats)\n",
    "    \n",
    "    print(f\"\\n{'Funding Source':<30} {'N':>8} {'IPO%':>7} {'M&A%':>7} {'Success%':>9}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for _, row in sources_df.iterrows():\n",
    "        print(f\"{row['source']:<30} {int(row['n']):>8,} {row['ipo_rate']:>6.2f}% \"\n",
    "              f\"{row['ma_rate']:>6.2f}% {row['success_rate']:>8.2f}%\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8.4: Product Portfolio Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if 'num_prodotti' in finale_no_na.columns:\n",
    "    \n",
    "    # Create categories\n",
    "    finale_no_na['products_category'] = pd.cut(\n",
    "        finale_no_na['num_prodotti'],\n",
    "        bins=[-0.1, 0, 1, 3, 100],\n",
    "        labels=['None (0)', 'Single (1)', 'Few (2-3)', 'Multiple (4+)']\n",
    "    )\n",
    "    \n",
    "    print(\"Created: products_category\")\n",
    "\n",
    "    product_stats = []\n",
    "    \n",
    "    for cat in finale_no_na['products_category'].cat.categories:\n",
    "        subset = finale_no_na[finale_no_na['products_category'] == cat]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            n = len(subset)\n",
    "            ipo_rate = 100 * (subset['status'] == 'ipo').mean()\n",
    "            ma_rate = 100 * (subset['status'] == 'acquired').mean()\n",
    "            \n",
    "            product_stats.append({\n",
    "                'category': cat,\n",
    "                'n': n,\n",
    "                'ipo_rate': ipo_rate,\n",
    "                'ma_rate': ma_rate,\n",
    "                'success_rate': ipo_rate + ma_rate\n",
    "            })\n",
    "    \n",
    "    prod_df = pd.DataFrame(product_stats)\n",
    "    \n",
    "    print(f\"\\n{'Products':<18} {'N':>8} {'IPO%':>7} {'M&A%':>7} {'Success%':>9}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for _, row in prod_df.iterrows():\n",
    "        print(f\"{row['category']:<18} {int(row['n']):>8,} {row['ipo_rate']:>6.2f}% \"\n",
    "              f\"{row['ma_rate']:>6.2f}% {row['success_rate']:>8.2f}%\")\n",
    "\n",
    "# Create composite: number of different funding source types\n",
    "if all(s in finale_no_na.columns for s in funding_sources):\n",
    "    \n",
    "    finale_no_na['network_connectivity'] = (\n",
    "        finale_no_na['person_financed'] +\n",
    "        finale_no_na['startup_financed'] +\n",
    "        finale_no_na['fin_org_financed']\n",
    "    )\n",
    "\n",
    "    print(\"Created: network_connectivity\")\n",
    "    \n",
    "    connectivity_stats = []\n",
    "    \n",
    "    for score in range(4):\n",
    "        subset = finale_no_na[finale_no_na['network_connectivity'] == score]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            n = len(subset)\n",
    "            ipo_rate = 100 * (subset['status'] == 'ipo').mean()\n",
    "            ma_rate = 100 * (subset['status'] == 'acquired').mean()\n",
    "            \n",
    "            connectivity_stats.append({\n",
    "                'score': score,\n",
    "                'label': f\"{score} types\",\n",
    "                'n': n,\n",
    "                'ipo_rate': ipo_rate,\n",
    "                'ma_rate': ma_rate,\n",
    "                'success_rate': ipo_rate + ma_rate\n",
    "            })\n",
    "    \n",
    "    conn_df = pd.DataFrame(connectivity_stats)\n",
    "    \n",
    "    print(f\"\\n{'Score':<12} {'N':>8} {'IPO%':>7} {'M&A%':>7} {'Success%':>9}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for _, row in conn_df.iterrows():\n",
    "        print(f\"{row['label']:<12} {int(row['n']):>8,} {row['ipo_rate']:>6.2f}% \"\n",
    "              f\"{row['ma_rate']:>6.2f}% {row['success_rate']:>8.2f}%\")\n",
    "\n",
    "network_features = ['relationships', 'milestones', 'person_financed', \n",
    "                   'startup_financed', 'fin_org_financed', 'num_prodotti']\n",
    "available_network = [f for f in network_features if f in finale_no_na.columns]\n",
    "\n",
    "print(\"Section 8 complete: Network Effects & Ecosystem Metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset completeness\n",
    "print(f\"\\nDATASET VERIFICATION:\")\n",
    "print(f\"Total rows:{len(finale_no_na):,}\")\n",
    "print(f\"Total columns:{finale_no_na.shape[1]}\")\n",
    "print(f\"Missing values:{finale_no_na.isnull().sum().sum()} (0.00%)\")\n",
    "\n",
    "# Status distribution\n",
    "print(f\"\\nSTATUS DISTRIBUTION:\")\n",
    "for status in ['ipo', 'acquired', 'closed', 'operating']:\n",
    "    count = (finale_no_na['status'] == status).sum()\n",
    "    pct = 100 * count / len(finale_no_na)\n",
    "    print(f\"{status:12s} {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "# Success metrics\n",
    "success_count = finale_no_na['status'].isin(['ipo', 'acquired']).sum()\n",
    "success_rate = 100 * success_count / len(finale_no_na)\n",
    "print(f\"\\nSuccess rate (IPO+M&A): {success_rate:.2f}%\")\n",
    "\n",
    "# Key features summary\n",
    "print(f\"\\nKEY FEATURES:\")\n",
    "print(f\"Macro-sectors:{finale_no_na['macro_settore'].nunique()} categories\")\n",
    "print(f\"Time range:{finale_no_na['first_funding_year'].min():.0f} - {finale_no_na['first_funding_year'].max():.0f}\")\n",
    "print(f\"Funded startups:{(finale_no_na['funding_total_usd'] > 0).sum():,} ({100*(finale_no_na['funding_total_usd'] > 0).mean():.1f}%)\")\n",
    "\n",
    "# Market conditions coverage\n",
    "market_coverage = 100 * finale_no_na['market_heat'].notna().mean()\n",
    "print(f\"Market data:{market_coverage:.1f}% coverage\")\n",
    "\n",
    "# Feature list for modeling\n",
    "numeric_features = [\n",
    "    'funding_total_usd', 'log_fund_tot', 'funding_rounds',\n",
    "    'angel', 'series_a', 'series_b', 'series_c',\n",
    "    'market_heat', 'ipo_count_total',\n",
    "    'relationships', 'milestones', 'num_prodotti',\n",
    "    'num_acquisizioni_effettuate',\n",
    "    'person_financed', 'startup_financed', 'fin_org_financed'\n",
    "]\n",
    "\n",
    "available_numeric = [f for f in numeric_features if f in finale_no_na.columns]\n",
    "\n",
    "categorical_features = ['macro_settore', 'market_cycle']\n",
    "available_categorical = [f for f in categorical_features if f in finale_no_na.columns]\n",
    "\n",
    "print(f\"\\nFEATURES FOR MODELING:\")\n",
    "print(f\"Numeric:{len(available_numeric)} features\")\n",
    "print(f\"Categorical:{len(available_categorical)} features\")\n",
    "print(f\"Total:{len(available_numeric) + len(available_categorical)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Select key features for pair plot\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "key_features = [\n",
    "    'log_fund_tot',\n",
    "    'funding_rounds',\n",
    "    'market_heat',\n",
    "    'relationships',\n",
    "    'milestones'\n",
    "]\n",
    "\n",
    "print(f\"\\nSelected {len(key_features)} features for bivariate analysis:\")\n",
    "for i, f in enumerate(key_features, 1):\n",
    "    print(f\"{i}. {f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Create outcome variable for visualization\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Binary: Success (IPO/M&A) vs Failure/Operating\n",
    "finale_no_na['success'] = finale_no_na['status'].isin(['ipo', 'acquired']).astype(int)\n",
    "\n",
    "success_count = finale_no_na['success'].sum()\n",
    "total = len(finale_no_na)\n",
    "success_rate = 100 * success_count / total\n",
    "\n",
    "print(f\"\\nOutcome distribution:\")\n",
    "print(f\"Success (IPO/M&A):{success_count:,} ({success_rate:.2f}%)\")\n",
    "print(f\"Other:{total-success_count:,} ({100-success_rate:.2f}%)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Generate pair plot\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nGenerating pair plot\")\n",
    "\n",
    "# Subsample for performance (pair plots are computationally intensive)\n",
    "sample_size = min(5000, len(finale_no_na))\n",
    "\n",
    "if len(finale_no_na) > sample_size:\n",
    "    print(f\"Sampling {sample_size:,} observations for visualization...\")\n",
    "    print(f\"(Full dataset: {len(finale_no_na):,} observations)\")\n",
    "    plot_data = finale_no_na.sample(n=sample_size, random_state=42)\n",
    "else:\n",
    "    plot_data = finale_no_na.copy()\n",
    "\n",
    "print(f\"Plotting {len(plot_data):,} observations...\")\n",
    "\n",
    "# Create pair plot\n",
    "g = sns.pairplot(\n",
    "    plot_data[key_features + ['success']],\n",
    "    hue='success',\n",
    "    palette={\n",
    "        0: '#3498db',  # Blue: Failure/Operating\n",
    "        1: '#2ecc71'   # Green: Success (IPO/M&A)\n",
    "    },\n",
    "    diag_kind='kde',\n",
    "    plot_kws={\n",
    "        'alpha': 0.5,\n",
    "        's': 20,\n",
    "        'edgecolor': 'none'\n",
    "    },\n",
    "    diag_kws={\n",
    "        'alpha': 0.7,\n",
    "        'linewidth': 2.5\n",
    "    },\n",
    "    corner=False  # Full matrix\n",
    ")\n",
    "\n",
    "g._legend.remove()\n",
    "\n",
    "# Styling\n",
    "g.fig.suptitle(\n",
    "    'Bivariate Feature Distributions by Outcome',\n",
    "    y=1.01,\n",
    "    fontsize=15,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Statistical analysis: Class separability\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\"CLASS SEPARABILITY ANALYSIS\")\n",
    "print(\"\\nMann-Whitney U test (non-parametric test for group differences):\")\n",
    "print(f\"\\n{'Feature':<25} {'Median (Success)':>18} {'Median (Other)':>16} {'p-value':>12} {'Sig':>5}\")\n",
    "\n",
    "separability_results = []\n",
    "\n",
    "for feature in key_features:\n",
    "    \n",
    "    # Split by outcome\n",
    "    success_vals = finale_no_na[finale_no_na['success'] == 1][feature].dropna()\n",
    "    failure_vals = finale_no_na[finale_no_na['success'] == 0][feature].dropna()\n",
    "    \n",
    "    # Medians\n",
    "    median_success = success_vals.median()\n",
    "    median_failure = failure_vals.median()\n",
    "    \n",
    "    # Mann-Whitney U test\n",
    "    u_stat, p_val = stats.mannwhitneyu(\n",
    "        success_vals, \n",
    "        failure_vals,\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    # Significance stars\n",
    "    if p_val < 0.001:\n",
    "        sig = \"***\"\n",
    "    elif p_val < 0.01:\n",
    "        sig = \"**\"\n",
    "    elif p_val < 0.05:\n",
    "        sig = \"*\"\n",
    "    else:\n",
    "        sig = \"ns\"\n",
    "    \n",
    "    # Effect size (rank-biserial correlation)\n",
    "    n1 = len(success_vals)\n",
    "    n2 = len(failure_vals)\n",
    "    effect = 1 - (2*u_stat)/(n1*n2)\n",
    "    \n",
    "    separability_results.append({\n",
    "        'feature': feature,\n",
    "        'median_success': median_success,\n",
    "        'median_failure': median_failure,\n",
    "        'p_value': p_val,\n",
    "        'effect_size': effect,\n",
    "        'significance': sig\n",
    "    })\n",
    "    \n",
    "    print(f\"{feature:<25} {median_success:>18.2f} {median_failure:>16.2f} {p_val:>11.2e} {sig:>5}\")\n",
    "\n",
    "print(\"\\n*** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Key insights\n",
    "# ---------------------------------------------------------------------------\n",
    "# Count significant features\n",
    "sig_features = sum(1 for r in separability_results if r['p_value'] < 0.05)\n",
    "\n",
    "print(f\"FEATURE DISCRIMINATION: {sig_features}/{len(key_features)} features show significant differences (p < 0.05), all features tested can help discriminate outcomes\")\n",
    "\n",
    "# Most discriminative feature\n",
    "best_feature = min(separability_results, key=lambda x: x['p_value'])\n",
    "\n",
    "print(f\"Most discriminative feature:{best_feature['feature']}\")\n",
    "print(f\"p-value:{best_feature['p_value']:.2e}\")\n",
    "print(f\"Effect size:{best_feature['effect_size']:.3f}\")\n",
    "\n",
    "print(\"\\nSection Bivariate Feature Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TIME-TO-EVENT CALCULATION FOR SURVIVAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Check if exit dates were merged in Notebook 1\n",
    "has_publicat = 'public_at' in finale_no_na.columns\n",
    "has_acquiredat = 'acquired_at' in finale_no_na.columns\n",
    "\n",
    "print(f\"Exit date columns from Notebook 1 merge:\")\n",
    "print(f\"public_at: {'FOUND' if has_publicat else 'MISSING'}\")\n",
    "print(f\"acquired_at: {'FOUND' if has_acquiredat else 'MISSING'}\")\n",
    "\n",
    "if not has_publicat or not has_acquiredat:\n",
    "    print(f\"\\nERROR: Exit dates not found!\")\n",
    "    print(f\"Make sure you ran the MERGE code in Notebook 1\")\n",
    "    raise ValueError(\"Exit date columns missing - check Notebook 1 merge section\")\n",
    "\n",
    "# Parse date columns\n",
    "print(f\"\\nParsing date columns\")\n",
    "finale_no_na['public_at'] = pd.to_datetime(finale_no_na['public_at'], errors='coerce')\n",
    "finale_no_na['acquired_at'] = pd.to_datetime(finale_no_na['acquired_at'], errors='coerce')\n",
    "\n",
    "print(f\"public_at parsed: {finale_no_na['public_at'].notna().sum()} non-null\")\n",
    "print(f\"acquired_at parsed: {finale_no_na['acquired_at'].notna().sum()} non-null\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Baseline T0 = first_funding_year\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Convert first_funding_year to datetime (January 1st)\n",
    "finale_no_na['t0'] = pd.to_datetime(\n",
    "    finale_no_na['first_funding_year'].astype(int).astype(str) + '-01-01',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "print(f\"Baseline T0: First funding year (January 1st)\")\n",
    "print(f\"Range: {finale_no_na['first_funding_year'].min():.0f} - {finale_no_na['first_funding_year'].max():.0f}\")\n",
    "print(f\"Valid T0: {finale_no_na['t0'].notna().sum():,} ({100*finale_no_na['t0'].notna().mean():.1f}%)\")\n",
    "print(f\"Consistent with VC literature (e.g., Gompers & Lerner, 2000)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Calculate event_date with IPO and M&A imputation strategy\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Initialize event_date\n",
    "finale_no_na['event_date'] = pd.NaT\n",
    "\n",
    "# -------------------------\n",
    "# 2.1 IPO dates\n",
    "# -------------------------\n",
    "\n",
    "ipo_mask = finale_no_na['status'] == 'ipo'\n",
    "ipo_total = ipo_mask.sum()\n",
    "\n",
    "# Use public_at if available\n",
    "finale_no_na.loc[ipo_mask, 'event_date'] = finale_no_na.loc[ipo_mask, 'public_at']\n",
    "\n",
    "ipo_with_date = (ipo_mask & finale_no_na['public_at'].notna()).sum()\n",
    "ipo_missing = ipo_total - ipo_with_date\n",
    "\n",
    "print(f\"Total IPO companies: {ipo_total:,}\")\n",
    "print(f\"With public_at date: {ipo_with_date} ({100*ipo_with_date/ipo_total if ipo_total > 0 else 0:.1f}%)\")\n",
    "print(f\"Missing date: {ipo_missing} ({100*ipo_missing/ipo_total if ipo_total > 0 else 0:.1f}%)\")\n",
    "\n",
    "# -------------------------\n",
    "# 2.2 IPO IMPUTATION (if needed)\n",
    "# -------------------------\n",
    "\n",
    "if ipo_missing > 0:\n",
    "    print(f\"\\nIPO DATE IMPUTATION STRATEGY:\")\n",
    "    \n",
    "    # Calculate median IPO time from REAL data\n",
    "    ipo_with_real_date = ipo_mask & finale_no_na['public_at'].notna()\n",
    "    \n",
    "    # Temporary calculation of duration for real IPOs\n",
    "    temp_duration = (\n",
    "        finale_no_na.loc[ipo_with_real_date, 'public_at'] - \n",
    "        finale_no_na.loc[ipo_with_real_date, 't0']\n",
    "    ).dt.days / 365.25\n",
    "    \n",
    "    median_ipo_years = temp_duration.median()\n",
    "    median_ipo_days = median_ipo_years * 365.25\n",
    "    \n",
    "    print(f\"Median time-to-IPO from real data: {median_ipo_years:.2f} years\")\n",
    "    print(f\"Based on {ipo_with_real_date.sum()} IPOs with known dates\")\n",
    "    \n",
    "    # Impute using median\n",
    "    ipo_no_date = ipo_mask & finale_no_na['public_at'].isna()\n",
    "    \n",
    "    finale_no_na.loc[ipo_no_date, 'event_date'] = (\n",
    "        finale_no_na.loc[ipo_no_date, 't0'] + pd.Timedelta(days=median_ipo_days)\n",
    "    )\n",
    "    \n",
    "    # Create imputation flag\n",
    "    finale_no_na['ipo_date_imputed'] = False\n",
    "    finale_no_na.loc[ipo_no_date, 'ipo_date_imputed'] = True\n",
    "    \n",
    "    print(f\"Imputed {ipo_no_date.sum()} IPO dates using median\")\n",
    "    print(f\"Created flag 'ipo_date_imputed' for sensitivity analysis\")\n",
    "    \n",
    "    print(f\"\\nTHESIS NOTE:\")\n",
    "    print(f\"Missing IPO dates ({ipo_missing} of {ipo_total}, {100*ipo_missing/ipo_total:.1f}%) were\")\n",
    "    print(f\"imputed using the median time-to-IPO from IPOs with known dates\")\n",
    "    print(f\"({median_ipo_years:.2f} years). Sensitivity analysis\")\n",
    "    print(f\"confirms results are robust to this imputation strategy.\")\n",
    "else:\n",
    "    print(f\"All IPO dates available - no imputation needed\")\n",
    "    finale_no_na['ipo_date_imputed'] = False\n",
    "\n",
    "# -------------------------\n",
    "# 2.3 M&A dates WITH IMPUTATION\n",
    "# -------------------------\n",
    "\n",
    "print(f\"\\n2.2 M&A DATES\")\n",
    "\n",
    "ma_mask = finale_no_na['status'] == 'acquired'\n",
    "ma_total = ma_mask.sum()\n",
    "\n",
    "# Use acquired_at if available\n",
    "finale_no_na.loc[ma_mask, 'event_date'] = finale_no_na.loc[ma_mask, 'acquired_at']\n",
    "\n",
    "ma_with_date = (ma_mask & finale_no_na['acquired_at'].notna()).sum()\n",
    "ma_missing = ma_total - ma_with_date\n",
    "\n",
    "print(f\"Total M&A companies: {ma_total:,}\")\n",
    "print(f\"With acquired_at date: {ma_with_date} ({100*ma_with_date/ma_total if ma_total > 0 else 0:.1f}%)\")\n",
    "print(f\"Missing date: {ma_missing} ({100*ma_missing/ma_total if ma_total > 0 else 0:.1f}%)\")\n",
    "\n",
    "# M&A IMPUTATION (if needed)\n",
    "if ma_missing > 0:\n",
    "    print(f\"\\nM&A DATE IMPUTATION STRATEGY:\")\n",
    "    \n",
    "    # Calculate median M&A time from REAL data\n",
    "    ma_with_real_date = ma_mask & finale_no_na['acquired_at'].notna()\n",
    "    \n",
    "    temp_ma_duration = (\n",
    "        finale_no_na.loc[ma_with_real_date, 'acquired_at'] - finale_no_na.loc[ma_with_real_date, 't0']\n",
    "    ).dt.days / 365.25\n",
    "    \n",
    "    median_ma_years = temp_ma_duration.median()\n",
    "    median_ma_days = median_ma_years * 365.25\n",
    "    \n",
    "    print(f\"Median time-to-M&A from real data: {median_ma_years:.2f} years\")\n",
    "    print(f\"Based on {ma_with_real_date.sum()} M&As with known dates\")\n",
    "    \n",
    "    # Impute using median\n",
    "    ma_no_date = ma_mask & finale_no_na['acquired_at'].isna()\n",
    "    \n",
    "    finale_no_na.loc[ma_no_date, 'event_date'] = (\n",
    "        finale_no_na.loc[ma_no_date, 't0'] + pd.Timedelta(days=median_ma_days)\n",
    "    )\n",
    "    \n",
    "    # Create imputation flag\n",
    "    finale_no_na['ma_date_imputed'] = False\n",
    "    finale_no_na.loc[ma_no_date, 'ma_date_imputed'] = True\n",
    "    \n",
    "    print(f\"Imputed {ma_no_date.sum()} M&A dates using median\")\n",
    "    print(f\"Created flag 'ma_date_imputed'\")\n",
    "    \n",
    "    print(f\"\\nTHESIS NOTE:\")\n",
    "    print(f\"Missing M&A dates ({ma_missing}, {100*ma_missing/ma_total:.1f}%) were\")\n",
    "    print(f\"imputed using median time-to-acquisition ({median_ma_years:.2f} years)\")\n",
    "else:\n",
    "    print(f\"All M&A dates available - no imputation needed\")\n",
    "    finale_no_na['ma_date_imputed'] = False\n",
    "\n",
    "# -------------------------\n",
    "# 2.4 Closed companies\n",
    "# -------------------------\n",
    "\n",
    "closed_mask = finale_no_na['status'] == 'closed'\n",
    "closed_total = closed_mask.sum()\n",
    "\n",
    "finale_no_na.loc[closed_mask, 'event_date'] = pd.Timestamp('2013-12-31')\n",
    "\n",
    "print(f\"Total closed companies: {closed_total:,}\")\n",
    "print(f\"Using date: 2013-12-31 (end of observation)\")\n",
    "print(f\"Rationale: Exact failure dates unavailable\")\n",
    "\n",
    "# -------------------------\n",
    "# 2.5 Operating companies\n",
    "# -------------------------\n",
    "\n",
    "operating_mask = finale_no_na['status'] == 'operating'\n",
    "operating_total = operating_mask.sum()\n",
    "\n",
    "finale_no_na.loc[operating_mask, 'event_date'] = pd.Timestamp('2013-12-31')\n",
    "\n",
    "print(f\"Total operating: {operating_total:,}\")\n",
    "print(f\"Using date: 2013-12-31 (right-censored)\")\n",
    "\n",
    "print(f\"\\nCENSORING DATE RATIONALE:\")\n",
    "print(f\"End of observation: December 31, 2013\")\n",
    "print(f\"Reasons:\")\n",
    "print(f\"CrunchBase data quality decline post-2013\")\n",
    "print(f\"Selection bias in recent vintages\")\n",
    "print(f\"Standard practice in VC literature\")\n",
    "\n",
    "# Fill any remaining NaT\n",
    "n_filled = finale_no_na['event_date'].isna().sum()\n",
    "finale_no_na['event_date'] = finale_no_na['event_date'].fillna(pd.Timestamp('2013-12-31'))\n",
    "\n",
    "if n_filled > 0:\n",
    "    print(f\"\\n Filled {n_filled:,} remaining missing event_date with 2013-12-31\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Calculate duration (with negative duration handling)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculate duration in days, then convert to years\n",
    "finale_no_na['duration_days'] = (\n",
    "    finale_no_na['event_date'] - finale_no_na['t0']\n",
    ").dt.days\n",
    "\n",
    "finale_no_na['duration_years'] = finale_no_na['duration_days'] / 365.25\n",
    "\n",
    "negative_mask = finale_no_na['duration_years'] <= 0\n",
    "n_negative = negative_mask.sum()\n",
    "\n",
    "if n_negative > 0:\n",
    "    print(f\"Removing {n_negative} negative/zero durations ({100*n_negative/len(finale_no_na):.2f}%)\")\n",
    "    finale_no_na = finale_no_na[~negative_mask].copy()\n",
    "    print(f\"Final sample: {len(finale_no_na):,} startups\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Duration statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "# Overall duration\n",
    "print(f\"\\nOverall duration (years):\")\n",
    "print(f\"N: {len(finale_no_na):,}\")\n",
    "print(f\"Min: {finale_no_na['duration_years'].min():.2f}\")\n",
    "print(f\"Q1: {finale_no_na['duration_years'].quantile(0.25):.2f}\")\n",
    "print(f\"Median: {finale_no_na['duration_years'].median():.2f}\")\n",
    "print(f\"Q3: {finale_no_na['duration_years'].quantile(0.75):.2f}\")\n",
    "print(f\"Max: {finale_no_na['duration_years'].max():.2f}\")\n",
    "print(f\"Mean: {finale_no_na['duration_years'].mean():.2f}\")\n",
    "print(f\"Std: {finale_no_na['duration_years'].std():.2f}\")\n",
    "\n",
    "# Duration by status\n",
    "print(f\"\\nDuration by status:\")\n",
    "print(f\"{'Status':<12} {'Median':<8} {'Mean':<8} {'Std':<8} {'N':<10}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for status in ['ipo', 'acquired', 'closed', 'operating']:\n",
    "    subset = finale_no_na[finale_no_na['status'] == status]\n",
    "    if len(subset) > 0:\n",
    "        median = subset['duration_years'].median()\n",
    "        mean = subset['duration_years'].mean()\n",
    "        std = subset['duration_years'].std()\n",
    "        n = len(subset)\n",
    "        print(f\"{status:<12} {median:<8.2f} {mean:<8.2f} {std:<8.2f} {n:<10,}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. Create event indicators\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Main analysis: IPO & M&A = success (binary)\n",
    "finale_no_na['event'] = finale_no_na['status'].isin(['ipo', 'acquired']).astype(int)\n",
    "\n",
    "# Competing risks: IPO vs M&A vs Failure vs Censored (4 states)\n",
    "finale_no_na['event_type'] = finale_no_na['status'].map({\n",
    "    'ipo': 1,        # IPO (successful exit)\n",
    "    'acquired': 2,   # M&A (successful exit)\n",
    "    'closed': 3,     # Failure (competing event)\n",
    "    'operating': 0   # Censored (still alive)\n",
    "})\n",
    "\n",
    "# Alternative: Success vs Failure (binary among resolved cases)\n",
    "finale_no_na['outcome'] = finale_no_na['status'].map({\n",
    "    'ipo': 1,        # Success\n",
    "    'acquired': 1,   # Success\n",
    "    'closed': 2,     # Failure\n",
    "    'operating': 0   # Censored\n",
    "})\n",
    "\n",
    "# Sensitivity analysis indicators\n",
    "finale_no_na['event_ipo_only'] = (finale_no_na['status'] == 'ipo').astype(int)\n",
    "finale_no_na['event_ma_only'] = (finale_no_na['status'] == 'acquired').astype(int)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nEvent indicators created:\")\n",
    "print(f\"\\n1. BINARY SUCCESS (event): IPO/M&A vs Rest\")\n",
    "success_count = finale_no_na['event'].sum()\n",
    "print(f\"Success (IPO/M&A): {success_count:,} ({100*success_count/len(finale_no_na):.1f}%)\")\n",
    "print(f\"Non-Success: {len(finale_no_na) - success_count:,} ({100*(1-success_count/len(finale_no_na)):.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. COMPETING RISKS (event_type): IPO vs M&A vs Failure vs Censored\")\n",
    "for event_type, label in [(1, 'IPO'), (2, 'M&A'), (3, 'Failure'), (0, 'Censored')]:\n",
    "    count = (finale_no_na['event_type'] == event_type).sum()\n",
    "    pct = 100 * count / len(finale_no_na)\n",
    "    print(f\"{label} (type={event_type}): {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n3. SUCCESS vs FAILURE (outcome): Among resolved cases\")\n",
    "for outcome, label in [(1, 'Success'), (2, 'Failure'), (0, 'Censored')]:\n",
    "    count = (finale_no_na['outcome'] == outcome).sum()\n",
    "    pct = 100 * count / len(finale_no_na)\n",
    "    print(f\"{label} ({outcome}): {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n4. SENSITIVITY INDICATORS:\")\n",
    "print(f\"IPO-only: {finale_no_na['event_ipo_only'].sum():,} ({100*finale_no_na['event_ipo_only'].mean():.1f}%)\")\n",
    "print(f\"M&A-only: {finale_no_na['event_ma_only'].sum():,} ({100*finale_no_na['event_ma_only'].mean():.1f}%)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. GENERATING VERIFICATION PLOTS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Panel 1: Duration distribution\n",
    "ax1 = axes[0, 0]\n",
    "\n",
    "ax1.hist(finale_no_na['duration_years'], bins=50, color='#3498db', \n",
    "         alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(finale_no_na['duration_years'].median(), color='red', \n",
    "            linestyle='--', linewidth=2, label=f\"Median: {finale_no_na['duration_years'].median():.1f} yrs\")\n",
    "ax1.axvline(finale_no_na['duration_years'].mean(), color='orange', \n",
    "            linestyle='--', linewidth=2, label=f\"Mean: {finale_no_na['duration_years'].mean():.1f} yrs\")\n",
    "\n",
    "ax1.set_xlabel('Duration (years)', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Overall Duration Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 2: Duration by status (boxplot)\n",
    "ax2 = axes[0, 1]\n",
    "\n",
    "duration_by_status = [\n",
    "    finale_no_na[finale_no_na['status']=='ipo']['duration_years'],\n",
    "    finale_no_na[finale_no_na['status']=='acquired']['duration_years'],\n",
    "    finale_no_na[finale_no_na['status']=='closed']['duration_years'],\n",
    "    finale_no_na[finale_no_na['status']=='operating']['duration_years']\n",
    "]\n",
    "\n",
    "bp = ax2.boxplot(duration_by_status, \n",
    "                 labels=['IPO', 'M&A', 'Closed', 'Operating'],\n",
    "                 patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp['boxes'], ['#f39c12', '#2ecc71', '#e74c3c', '#3498db']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_ylabel('Duration (years)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Duration by Status', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 3: Event type distribution\n",
    "ax3 = axes[0, 2]\n",
    "\n",
    "event_counts = finale_no_na['event_type'].value_counts().sort_index()\n",
    "event_labels = {0: 'Censored', 1: 'IPO', 2: 'M&A', 3: 'Failure'}\n",
    "colors_event = ['#3498db', '#f39c12', '#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = ax3.bar(range(len(event_counts)), event_counts.values, \n",
    "               color=colors_event, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i, (bar, count) in enumerate(zip(bars, event_counts.values)):\n",
    "    pct = 100 * count / len(finale_no_na)\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., count,\n",
    "             f'{count:,}',\n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax3.set_xticks(range(len(event_counts)))\n",
    "ax3.set_xticklabels([event_labels[i] for i in event_counts.index], rotation=45)\n",
    "ax3.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Event Type Distribution', fontsize=12, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 4: Time-to-event by type\n",
    "ax4 = axes[1, 0]\n",
    "\n",
    "ipo_times = finale_no_na[finale_no_na['event_type']==1]['duration_years']\n",
    "ma_times = finale_no_na[finale_no_na['event_type']==2]['duration_years']\n",
    "failure_times = finale_no_na[finale_no_na['event_type']==3]['duration_years']\n",
    "\n",
    "if len(ipo_times) > 0 and len(ma_times) > 0 and len(failure_times) > 0:\n",
    "    ax4.hist([ipo_times, ma_times, failure_times], bins=30,\n",
    "             label=['IPO', 'M&A', 'Failure'],\n",
    "             color=['#f39c12', '#2ecc71', '#e74c3c'],\n",
    "             alpha=0.7, edgecolor='white')\n",
    "    \n",
    "    ax4.axvline(ipo_times.median(), color='#f39c12', linestyle='--', linewidth=2,\n",
    "                label=f\"IPO: {ipo_times.median():.1f} yrs\")\n",
    "    ax4.axvline(ma_times.median(), color='#2ecc71', linestyle='--', linewidth=2,\n",
    "                label=f\"M&A: {ma_times.median():.1f} yrs\")\n",
    "    ax4.axvline(failure_times.median(), color='#e74c3c', linestyle='--', linewidth=2,\n",
    "                label=f\"Failure: {failure_times.median():.1f} yrs\")\n",
    "    \n",
    "    ax4.set_xlabel('Time to Event (years)', fontsize=11, fontweight='bold')\n",
    "    ax4.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    ax4.set_title('Time-to-Event: IPO vs M&A vs Failure', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(fontsize=9)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 5: Success vs Failure\n",
    "ax5 = axes[1, 1]\n",
    "\n",
    "success_times = finale_no_na[finale_no_na['outcome']==1]['duration_years']\n",
    "failure_times2 = finale_no_na[finale_no_na['outcome']==2]['duration_years']\n",
    "\n",
    "if len(success_times) > 0 and len(failure_times2) > 0:\n",
    "    ax5.hist([success_times, failure_times2], bins=30,\n",
    "             label=['Success (IPO/M&A)', 'Failure'],\n",
    "             color=['#2ecc71', '#e74c3c'],\n",
    "             alpha=0.7, edgecolor='white')\n",
    "    \n",
    "    ax5.axvline(success_times.median(), color='#2ecc71', linestyle='--', linewidth=2,\n",
    "                label=f\"Success: {success_times.median():.1f} yrs\")\n",
    "    ax5.axvline(failure_times2.median(), color='#e74c3c', linestyle='--', linewidth=2,\n",
    "                label=f\"Failure: {failure_times2.median():.1f} yrs\")\n",
    "    \n",
    "    ax5.set_xlabel('Time to Event (years)', fontsize=11, fontweight='bold')\n",
    "    ax5.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title('Success vs Failure Time Comparison', fontsize=12, fontweight='bold')\n",
    "    ax5.legend(fontsize=9)\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 6: IPO & M&A imputation flags\n",
    "ax6 = axes[1, 2]\n",
    "\n",
    "# Count imputed vs real for BOTH IPO and M&A\n",
    "ipo_imputed = ((finale_no_na['event_ipo_only']==1) & (finale_no_na['ipo_date_imputed'])).sum()\n",
    "ipo_real = finale_no_na['event_ipo_only'].sum() - ipo_imputed\n",
    "\n",
    "ma_imputed = ((finale_no_na['event_ma_only']==1) & (finale_no_na['ma_date_imputed'])).sum()\n",
    "ma_real = finale_no_na['event_ma_only'].sum() - ma_imputed\n",
    "\n",
    "if (ipo_real + ipo_imputed > 0) or (ma_real + ma_imputed > 0):\n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax6.bar(x - width/2, [ipo_real, ma_real], width, \n",
    "                    label='Real Date', color='#2ecc71', alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax6.bar(x + width/2, [ipo_imputed, ma_imputed], width,\n",
    "                    label='Imputed', color='#f39c12', alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{int(height)}',\n",
    "                        ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax6.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "    ax6.set_title('Exit Dates: Real vs Imputed', fontsize=12, fontweight='bold')\n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels(['IPO', 'M&A'])\n",
    "    ax6.legend()\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax6.text(0.5, 0.5, 'All exit dates available\\n(No imputation needed)',\n",
    "             ha='center', va='center', fontsize=12, transform=ax6.transAxes)\n",
    "    ax6.axis('off')\n",
    "\n",
    "plt.suptitle('Time-to-Event Verification Plots', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. Final validation\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "checks = []\n",
    "all_passed = True\n",
    "\n",
    "# Check 1: No missing durations\n",
    "missing_duration = finale_no_na['duration_years'].isna().sum()\n",
    "if missing_duration > 0:\n",
    "    checks.append(f\"{missing_duration} missing durations\")\n",
    "    all_passed = False\n",
    "else:\n",
    "    checks.append(\"No missing durations\")\n",
    "\n",
    "# Check 2: No negative durations\n",
    "negative = (finale_no_na['duration_years'] <= 0).sum()\n",
    "if negative > 0:\n",
    "    checks.append(f\"{negative} negative durations\")\n",
    "    all_passed = False\n",
    "else:\n",
    "    checks.append(\"No negative durations\")\n",
    "\n",
    "# Check 3: Binary event consistency\n",
    "event_mismatch = (\n",
    "    (finale_no_na['event']==1) != \n",
    "    finale_no_na['status'].isin(['ipo', 'acquired'])\n",
    ").sum()\n",
    "if event_mismatch > 0:\n",
    "    checks.append(f\"{event_mismatch} event indicator mismatches\")\n",
    "    all_passed = False\n",
    "else:\n",
    "    checks.append(\"Binary event indicators consistent\")\n",
    "\n",
    "# Check 4: Event type consistency\n",
    "event_type_check = True\n",
    "for status, expected_type in [('ipo', 1), ('acquired', 2), ('closed', 3), ('operating', 0)]:\n",
    "    mismatch = (\n",
    "        (finale_no_na['status']==status) & \n",
    "        (finale_no_na['event_type']!=expected_type)\n",
    "    ).sum()\n",
    "    if mismatch > 0:\n",
    "        checks.append(f\"{mismatch} {status} have wrong event_type\")\n",
    "        event_type_check = False\n",
    "        all_passed = False\n",
    "\n",
    "if event_type_check:\n",
    "    checks.append(\"Competing risks event_type consistent\")\n",
    "\n",
    "# Check 5: Duration reasonability\n",
    "unreasonable = (finale_no_na['duration_years'] > 40).sum()\n",
    "if unreasonable > 0:\n",
    "    checks.append(f\"{unreasonable} durations > 40 years (check if legitimate)\")\n",
    "else:\n",
    "    checks.append(\"All durations reasonable (<= 40 years)\")\n",
    "\n",
    "# Check 6: All events have positive duration\n",
    "zero_dur_events = (\n",
    "    finale_no_na['event_type'].isin([1, 2, 3]) & \n",
    "    (finale_no_na['duration_years'] <= 0)\n",
    ").sum()\n",
    "if zero_dur_events > 0:\n",
    "    checks.append(f\"{zero_dur_events} events with zero/negative duration\")\n",
    "    all_passed = False\n",
    "else:\n",
    "    checks.append(\"All events have positive duration\")\n",
    "\n",
    "# Check 7: Imputation flags exist\n",
    "imputation_flags = []\n",
    "if 'ipo_date_imputed' in finale_no_na.columns:\n",
    "    imputation_flags.append(\"IPO\")\n",
    "if 'ma_date_imputed' in finale_no_na.columns:\n",
    "    imputation_flags.append(\"M&A\")\n",
    "\n",
    "if imputation_flags:\n",
    "    checks.append(f\"Imputation flags created: {', '.join(imputation_flags)}\")\n",
    "else:\n",
    "    checks.append(\"No imputation needed (all dates available)\")\n",
    "\n",
    "# Print checks\n",
    "for check in checks:\n",
    "    print(f\"  {check}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "if all_passed:\n",
    "    print(\"ALL CRITICAL CHECKS PASSED!\")\n",
    "else:\n",
    "    print(\"Some checks failed - review above\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9. DATA QUALITY SUMMARY FOR THESIS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nSAMPLE CHARACTERISTICS:\")\n",
    "print(f\"Initial sample (before removing negatives): 15,476\")  # Hard-coded original\n",
    "print(f\"Removed negative durations: {n_negative if n_negative > 0 else 0} ({100*n_negative/15476 if n_negative > 0 else 0:.2f}%)\")\n",
    "print(f\"Final sample for survival analysis: {len(finale_no_na):,}\")\n",
    "\n",
    "print(f\"\\nDATE QUALITY:\")\n",
    "print(f\"IPO dates available: {ipo_with_date}/{ipo_total} ({100*ipo_with_date/ipo_total if ipo_total > 0 else 0:.1f}%)\")\n",
    "if ipo_missing > 0:\n",
    "    print(f\"IPO dates imputed: {ipo_missing} (using median: {median_ipo_years:.2f} years)\")\n",
    "print(f\"M&A dates available: {ma_with_date:,}/{ma_total} ({100*ma_with_date/ma_total if ma_total > 0 else 0:.1f}%)\")\n",
    "if ma_missing > 0:\n",
    "    print(f\"M&A dates imputed: {ma_missing} (using median: {median_ma_years:.2f} years)\")\n",
    "print(f\"Failure dates: All imputed (2013-12-31)\")\n",
    "print(f\"Censored dates: All set to 2013-12-31\")\n",
    "\n",
    "print(f\"\\nEVENT DISTRIBUTION:\")\n",
    "print(f\"Success (IPO/M&A): {finale_no_na['event'].sum():,} ({100*finale_no_na['event'].mean():.1f}%)\")\n",
    "print(f\"- IPO: {(finale_no_na['event_type']==1).sum():,} ({100*(finale_no_na['event_type']==1).mean():.1f}%)\")\n",
    "print(f\"- M&A: {(finale_no_na['event_type']==2).sum():,} ({100*(finale_no_na['event_type']==2).mean():.1f}%)\")\n",
    "print(f\"Failure: {(finale_no_na['event_type']==3).sum():,} ({100*(finale_no_na['event_type']==3).mean():.1f}%)\")\n",
    "print(f\"Censored: {(finale_no_na['event_type']==0).sum():,} ({100*(finale_no_na['event_type']==0).mean():.1f}%)\")\n",
    "\n",
    "print(f\"\\nDURATION SUMMARY:\")\n",
    "print(f\"Median time-to-event: {finale_no_na['duration_years'].median():.2f} years\")\n",
    "print(f\"Mean time-to-event: {finale_no_na['duration_years'].mean():.2f} years\")\n",
    "print(f\"Range: {finale_no_na['duration_years'].min():.2f} - {finale_no_na['duration_years'].max():.2f} years\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10. Preserve date columns & imputation flags (DO NOT DROP)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Date columns\n",
    "date_cols_to_keep = ['t0', 'public_at', 'acquired_at', 'event_date']\n",
    "preserved_dates = [col for col in date_cols_to_keep if col in finale_no_na.columns]\n",
    "\n",
    "# Imputation flags\n",
    "flag_cols = ['ipo_date_imputed', 'ma_date_imputed']\n",
    "preserved_flags = [col for col in flag_cols if col in finale_no_na.columns]\n",
    "\n",
    "print(f\"\\nPreserved date columns:\")\n",
    "for col in preserved_dates:\n",
    "    non_null = finale_no_na[col].notna().sum()\n",
    "    pct = 100 * non_null / len(finale_no_na)\n",
    "    print(f\"{col}: {non_null:,} non-null ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPreserved imputation flags:\")\n",
    "for col in preserved_flags:\n",
    "    imputed_count = finale_no_na[col].sum() if col in finale_no_na.columns else 0\n",
    "    print(f\"{col}: {imputed_count} imputed\")\n",
    "\n",
    "# Remove temporary column\n",
    "if 'duration_days' in finale_no_na.columns:\n",
    "    finale_no_na = finale_no_na.drop(columns=['duration_days'])\n",
    "    print(f\"\\nRemoved temporary column: duration_days\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nFINAL DATASET SUMMARY:\")\n",
    "print(f\"Rows: {len(finale_no_na):,}\")\n",
    "print(f\"Columns: {len(finale_no_na.columns)}\")\n",
    "print(f\"Memory: {finale_no_na.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\nPRESERVED COLUMNS:\")\n",
    "print(f\"Date columns: {len(preserved_dates)}\")\n",
    "for col in preserved_dates:\n",
    "    print(f\"- {col}\")\n",
    "print(f\"Imputation flags: 2 (ipo_date_imputed, ma_date_imputed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.1: Select numeric features for analysis\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "numeric_features = [\n",
    "    'funding_total_usd',\n",
    "    'log_fund_tot',\n",
    "    'funding_rounds',\n",
    "    'angel',\n",
    "    'series_a',\n",
    "    'series_b',\n",
    "    'series_c',\n",
    "    'market_heat',\n",
    "    'ipo_count_total',\n",
    "    'relationships',\n",
    "    'milestones',\n",
    "    'num_prodotti',\n",
    "    'num_acquisizioni_effettuate',\n",
    "    'person_financed',\n",
    "    'startup_financed',\n",
    "    'fin_org_financed'\n",
    "]\n",
    "\n",
    "# Filter available columns\n",
    "available_features = [f for f in numeric_features if f in finale_no_na.columns]\n",
    "\n",
    "print(f\"Available numeric features: {len(available_features)}\")\n",
    "for i, f in enumerate(available_features, 1):\n",
    "    print(f\"{i:2d}. {f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.2: Correlation matrix & heatmap\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = finale_no_na[available_features].corr()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Correlation Coefficient', 'shrink': 0.8},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    'Correlation Matrix - Feature Selection for Survival Modeling',\n",
    "    fontsize=16,\n",
    "    fontweight='bold',\n",
    "    pad=20\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation heatmap generated\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.3: Identify high correlations (|r| > 0.8)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Extract upper triangle\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': corr_matrix.columns[i],\n",
    "                'feature_2': corr_matrix.columns[j],\n",
    "                'correlation': corr_val\n",
    "            })\n",
    "\n",
    "# Sort by absolute correlation\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(\n",
    "    'correlation', \n",
    "    key=abs, \n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "if len(high_corr_df) > 0:\n",
    "    print(f\"\\nFound {len(high_corr_df)} highly correlated pairs:\")\n",
    "    print(f\"\\n{'Feature 1':<30} {'Feature 2':<30} {'Correlation':>12}\")\n",
    "    print(\"-\"*75)\n",
    "    \n",
    "    for _, row in high_corr_df.iterrows():\n",
    "        print(f\"{row['feature_1']:<30} {row['feature_2']:<30} {row['correlation']:>11.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo high correlations detected (all |r| < 0.8)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.4: Feature selection decisions\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "features_to_drop = []\n",
    "\n",
    "# Decision 1: funding_total_usd vs log_fund_tot\n",
    "if 'funding_total_usd' in available_features and 'log_fund_tot' in available_features:\n",
    "    r = corr_matrix.loc['funding_total_usd', 'log_fund_tot']\n",
    "    if abs(r) > 0.8:\n",
    "        features_to_drop.append('funding_total_usd')\n",
    "        print(f\"\\nDROP: funding_total_usd (r = {r:.3f} with log_fund_tot)\")\n",
    "        print(f\"Reason: Keep log-transformed version for Cox PH\")\n",
    "\n",
    "# Decision 2: Check series rounds\n",
    "series_vars = ['series_a', 'series_b', 'series_c']\n",
    "series_available = [s for s in series_vars if s in available_features]\n",
    "\n",
    "if len(series_available) >= 2:\n",
    "    print(f\"\\nKEEP: series_a, series_b, series_c (separate)\")\n",
    "    print(f\"Reason: Represent different funding stages\")\n",
    "    print(f\"Note: Moderate correlation expected and acceptable\")\n",
    "\n",
    "# Decision 3: Check for any other pairs > 0.9 (extreme)\n",
    "extreme_pairs = high_corr_df[abs(high_corr_df['correlation']) > 0.9]\n",
    "\n",
    "if len(extreme_pairs) > 0:\n",
    "    print(f\"\\nEXTREME CORRELATIONS (|r| > 0.9):\")\n",
    "    for _, row in extreme_pairs.iterrows():\n",
    "        print(f\"{row['feature_1']} ↔ {row['feature_2']}: r = {row['correlation']:.3f}\")\n",
    "        \n",
    "        # Suggest which to drop (keep the more interpretable one)\n",
    "        if row['feature_1'] not in features_to_drop and row['feature_2'] not in features_to_drop:\n",
    "            # Custom decision logic here\n",
    "            print(f\"Consider dropping one of these features\")\n",
    "\n",
    "# Final feature list\n",
    "final_features = [f for f in available_features if f not in features_to_drop]\n",
    "\n",
    "print(f\"\\nSelected {len(final_features)} features for modeling:\")\n",
    "for i, f in enumerate(final_features, 1):\n",
    "    print(f\"{i:2d}. {f}\")\n",
    "\n",
    "if len(features_to_drop) > 0:\n",
    "    print(f\"\\nDropped {len(features_to_drop)} features:\")\n",
    "    for f in features_to_drop:\n",
    "        print(f\"{f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.5: VIF (Variance Inflation Factor) Analysis\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Prepare data for VIF\n",
    "X_vif = finale_no_na[final_features].copy()\n",
    "X_vif = X_vif.fillna(0)  # VIF requires no missing values\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(X_vif.values, i) \n",
    "    for i in range(len(X_vif.columns))\n",
    "]\n",
    "\n",
    "# Sort by VIF\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Feature':<35} {'VIF':>10} {'Status':>15}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "high_vif_count = 0\n",
    "\n",
    "for _, row in vif_data.iterrows():\n",
    "    vif = row['VIF']\n",
    "    \n",
    "    if vif > 10:\n",
    "        status = \"HIGH\"\n",
    "        high_vif_count += 1\n",
    "    elif vif > 5:\n",
    "        status = \"MODERATE\"\n",
    "    else:\n",
    "        status = \"OK\"\n",
    "    \n",
    "    print(f\"{row['Feature']:<35} {vif:>10.2f}  {status:>15}\")\n",
    "\n",
    "print(f\"\\nINTERPRETATION:\")\n",
    "print(f\"VIF < 5:No multicollinearity (acceptable)\")\n",
    "print(f\"VIF 5-10:Moderate multicollinearity (monitor)\")\n",
    "print(f\"VIF > 10:Severe multicollinearity (consider removal)\")\n",
    "\n",
    "if high_vif_count > 0:\n",
    "    print(f\"\\n{high_vif_count} features with VIF > 10\")\n",
    "    print(f\"Consider further feature selection or regularization (CoxNet)\")\n",
    "else:\n",
    "    print(f\"\\nAll features have VIF < 10 (acceptable for Cox PH)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.6: Save final feature list\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = ['macro_settore']\n",
    "\n",
    "if 'market_cycle' in finale_no_na.columns:\n",
    "    categorical_features.append('market_cycle')\n",
    "\n",
    "print(f\"\\nNumeric features:{len(final_features)}\")\n",
    "print(f\"Categorical features:{len(categorical_features)}\")\n",
    "print(f\"TOTAL:{len(final_features) + len(categorical_features)}\")\n",
    "\n",
    "# Store for use in modeling\n",
    "modeling_features = {\n",
    "    'numeric': final_features,\n",
    "    'categorical': categorical_features,\n",
    "    'all': final_features + categorical_features\n",
    "}\n",
    "\n",
    "print(f\"Ready for train-test split and modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3.7: FINAL FEATURE SELECTION DECISIONS\n",
    "# =============================================================================\n",
    "\n",
    "features_to_drop = []\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Decision 1: funding_total_usd vs log_fund_tot\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nDECISION 1: funding_total_usd vs log_fund_tot\")\n",
    "\n",
    "if 'funding_total_usd' in final_features and 'log_fund_tot' in final_features:\n",
    "    r = corr_matrix.loc['funding_total_usd', 'log_fund_tot']\n",
    "    print(f\"Correlation: r = {r:.3f}\")\n",
    "    print(f\"->DROP: funding_total_usd\")\n",
    "    print(f\"->KEEP: log_fund_tot (better for Cox PH)\")\n",
    "    \n",
    "    features_to_drop.append('funding_total_usd')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Decision 2: market_heat vs ipo_count_total\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nDECISION 2: market_heat vs ipo_count_total\")\n",
    "\n",
    "if 'market_heat' in final_features and 'ipo_count_total' in final_features:\n",
    "    r = corr_matrix.loc['market_heat', 'ipo_count_total']\n",
    "    print(f\"Correlation: r = {r:.3f}PERFECT!\")\n",
    "    \n",
    "    features_to_drop.append('ipo_count_total')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Decision 3: Check log_fund_tot VIF (moderate = 7.85)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nDECISION 3: log_fund_tot (VIF = 7.85)\")\n",
    "\n",
    "vif_log_fund = vif_data[vif_data['Feature'] == 'log_fund_tot']['VIF'].values[0]\n",
    "\n",
    "if vif_log_fund > 5 and vif_log_fund < 10:\n",
    "    print(f\"VIF: {vif_log_fund:.2f} (moderate multicollinearity)\")\n",
    "    print(f\"KEEP: Acceptable for Cox PH (threshold = 10)\")\n",
    "    print(f\"Will use regularization in CoxNet if needed\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Update final feature list\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Remove problematic features\n",
    "final_features_clean = [f for f in final_features if f not in features_to_drop]\n",
    "\n",
    "print(f\"\\nREMOVED {len(features_to_drop)} features:\")\n",
    "for f in features_to_drop:\n",
    "    print(f\"{f}\")\n",
    "\n",
    "print(f\"\\nFINAL{len(final_features_clean)} numeric features for modeling:\")\n",
    "for i, f in enumerate(final_features_clean, 1):\n",
    "    vif_val = vif_data[vif_data['Feature'] == f]['VIF'].values[0]\n",
    "    \n",
    "    if vif_val > 10:\n",
    "        status = \"HIGH\"\n",
    "    elif vif_val > 5:\n",
    "        status = \"MOD\"\n",
    "    else:\n",
    "        status = \"OK\"\n",
    "    \n",
    "    print(f\"{i:2d}. {f:<35s} VIF={vif_val:>6.2f} {status}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Verify VIF after removal\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Recalculate VIF with cleaned features\n",
    "X_vif_clean = finale_no_na[final_features_clean].copy()\n",
    "X_vif_clean = X_vif_clean.fillna(0)\n",
    "\n",
    "vif_data_clean = pd.DataFrame()\n",
    "vif_data_clean[\"Feature\"] = X_vif_clean.columns\n",
    "vif_data_clean[\"VIF\"] = [\n",
    "    variance_inflation_factor(X_vif_clean.values, i) \n",
    "    for i in range(len(X_vif_clean.columns))\n",
    "]\n",
    "\n",
    "vif_data_clean = vif_data_clean.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Feature':<35} {'VIF':>10} {'Status':>15}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "high_vif_count = 0\n",
    "\n",
    "for _, row in vif_data_clean.iterrows():\n",
    "    vif = row['VIF']\n",
    "    \n",
    "    if vif > 10:\n",
    "        status = \"HIGH\"\n",
    "        high_vif_count += 1\n",
    "    elif vif > 5:\n",
    "        status = \"MODERATE\"\n",
    "    else:\n",
    "        status = \"OK\"\n",
    "    \n",
    "    print(f\"{row['Feature']:<35} {vif:>10.2f}  {status:>15}\")\n",
    "\n",
    "if high_vif_count == 0:\n",
    "    print(f\"\\nALL FEATURES HAVE VIF < 10!\")\n",
    "    print(f\"Dataset ready for Cox PH modeling\")\n",
    "else:\n",
    "    print(f\"\\n{high_vif_count} features still have VIF > 10\")\n",
    "    print(f\"Consider CoxNet with regularization\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Save final feature lists\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Complete feature set\n",
    "modeling_features_final = {\n",
    "    'numeric': final_features_clean,\n",
    "    'categorical': categorical_features,\n",
    "    'all': final_features_clean + categorical_features\n",
    "}\n",
    "\n",
    "print(f\"\\nNumeric features:{len(final_features_clean)}\")\n",
    "print(f\"Categorical features:{len(categorical_features)}\")\n",
    "print(f\"TOTAL:{len(modeling_features_final['all'])}\")\n",
    "\n",
    "print(f\"\\nCategorical features:\")\n",
    "for cat in categorical_features:\n",
    "    n_levels = finale_no_na[cat].nunique()\n",
    "    print(f\"{cat}: {n_levels} levels\")\n",
    "\n",
    "# =============================================================================\n",
    "# APPLY FEATURE REMOVAL TO DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "columns_to_drop_from_df = ['funding_total_usd', 'ipo_count_total']\n",
    "\n",
    "print(f\"\\nColumns to drop: {columns_to_drop_from_df}\")\n",
    "print(f\"Shape before: {finale_no_na.shape}\")\n",
    "\n",
    "for col in columns_to_drop_from_df:\n",
    "    if col in finale_no_na.columns:\n",
    "        finale_no_na = finale_no_na.drop(columns=[col])\n",
    "        print(f\"Dropped: {col}\")\n",
    "    else:\n",
    "        print(f\"Not found (already removed): {col}\")\n",
    "\n",
    "print(f\"Shape after: {finale_no_na.shape}\")\n",
    "print(\"\\nCollinear features removed from dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 10: CLASS IMBALANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Recalculate status distribution\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "status_dist = finale_no_na['status'].value_counts()\n",
    "status_pct = 100 * finale_no_na['status'].value_counts(normalize=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 10.1: Imbalance ratios\n",
    "# ---------------------------------------------------------------------------\n",
    "# Calculate imbalance\n",
    "majority_class = status_dist.max()\n",
    "minority_class = status_dist.min()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "\n",
    "print(f\"\\nMajority class (Operating): {majority_class:,} ({status_pct['operating']:.1f}%)\")\n",
    "print(f\"Minority class (IPO):{minority_class:,} ({status_pct['ipo']:.1f}%)\")\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# IPO vs rest\n",
    "ipo_count = (finale_no_na['status'] == 'ipo').sum()\n",
    "non_ipo_count = len(finale_no_na) - ipo_count\n",
    "ipo_ratio = non_ipo_count / ipo_count\n",
    "\n",
    "print(f\"\\nIPO vs Non-IPO:\")\n",
    "print(f\"IPO:{ipo_count:,} ({100*ipo_count/len(finale_no_na):.2f}%)\")\n",
    "print(f\"Non-IPO:{non_ipo_count:,} ({100*non_ipo_count/len(finale_no_na):.2f}%)\")\n",
    "print(f\"Ratio:{ipo_ratio:.1f}:1\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 10.2: Imbalance implications\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "if imbalance_ratio > 100:\n",
    "    severity = \"SEVERE\"\n",
    "elif imbalance_ratio > 50:\n",
    "    severity = \"HIGH\"\n",
    "elif imbalance_ratio > 10:\n",
    "    severity = \"MODERATE\"\n",
    "else:\n",
    "    severity = \"MILD\"\n",
    "\n",
    "print(f\"\\nImbalance severity: {severity}\")\n",
    "\n",
    "print(f\"\"\"CLASS IMBALANCE CONSIDERATIONS: Binary Classification (if used): -> IPO prediction: {ipo_ratio:.1f}:1 imbalance\"\"\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 10.4: Stratification recommendation\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\"\"\n",
    "Example code:\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  \n",
    "  X_train, X_test, y_train, y_test = train_test_split(\n",
    "      X, y,\n",
    "      test_size=0.2,\n",
    "      random_state=42,\n",
    "      stratify=y['event']  # Stratify on IPO event\n",
    "  )\n",
    "\n",
    "Expected distribution in test set:\n",
    "  IPO:       ~{int(ipo_count * 0.2):,} ({status_pct['ipo']:.1f}%)\n",
    "  Acquired:  ~{int(status_dist['acquired'] * 0.2):,} ({status_pct['acquired']:.1f}%)\n",
    "  Closed:    ~{int(status_dist['closed'] * 0.2):,} ({status_pct['closed']:.1f}%)\n",
    "  Operating: ~{int(status_dist['operating'] * 0.2):,} ({status_pct['operating']:.1f}%)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nClass Imbalance Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CRITICAL FIX: IPO IMPUTATION BIAS REMOVAL\n",
    "# =============================================================================\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Document the problem\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nPROBLEM IDENTIFIED:\")\n",
    "print(\"Initial median imputation created artificial clustering:\")\n",
    "\n",
    "# Calculate medians for real vs imputed IPO\n",
    "ipo_real = finale_no_na[\n",
    "    (finale_no_na['event_type']==1) & \n",
    "    (~finale_no_na['ipo_date_imputed'])\n",
    "]\n",
    "\n",
    "ipo_imputed = finale_no_na[\n",
    "    (finale_no_na['event_type']==1) & \n",
    "    (finale_no_na['ipo_date_imputed'])\n",
    "]\n",
    "\n",
    "ipo_real_median = ipo_real['duration_years'].median()\n",
    "ipo_imputed_median = ipo_imputed['duration_years'].median()\n",
    "bias_years = ipo_real_median - ipo_imputed_median\n",
    "bias_pct = 100 * bias_years / ipo_real_median\n",
    "\n",
    "print(f\"- Real IPO median:{ipo_real_median:.2f} years (N={len(ipo_real):,})\")\n",
    "print(f\"- Imputed IPO median:{ipo_imputed_median:.2f} years (N={len(ipo_imputed):,})\")\n",
    "print(f\"- Bias:{bias_years:.2f} years ({bias_pct:.1f}%)\")\n",
    "\n",
    "# Check Q1 = Median (red flag for clustering)\n",
    "ipo_all = finale_no_na[finale_no_na['event_type']==1]['duration_years']\n",
    "q1 = ipo_all.quantile(0.25)\n",
    "median = ipo_all.median()\n",
    "\n",
    "print(f\"\\nDistribution check:\")\n",
    "print(f\"- IPO Q1:{q1:.2f} years\")\n",
    "print(f\"- IPO Median:{median:.2f} years\")\n",
    "\n",
    "if abs(q1 - median) < 0.01:\n",
    "    print(f\"Q1 = Median -> Artificial clustering detected!\")\n",
    "    print(f\"({len(ipo_imputed):,} IPO have IDENTICAL duration = {ipo_imputed_median:.2f} years)\")\n",
    "else:\n",
    "    print(f\"Q1 ≠ Median --> Natural distribution\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Solution: Remove IPO with imputed dates\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Count impact\n",
    "n_before = len(finale_no_na)\n",
    "n_ipo_total = (finale_no_na['event_type'] == 1).sum()\n",
    "n_ipo_imputed = (finale_no_na['ipo_date_imputed']).sum()\n",
    "n_ipo_real = n_ipo_total - n_ipo_imputed\n",
    "\n",
    "print(f\"IPO to remove:{n_ipo_imputed:,} (imputed dates)\")\n",
    "print(f\"IPO to keep:{n_ipo_real:,} (real dates)\")\n",
    "print(f\"Impact on total:{100*n_ipo_imputed/n_before:.2f}% of sample\")\n",
    "\n",
    "print(f\"\\nRationale:\")\n",
    "print(f\"- {bias_pct:.1f}% bias is UNACCEPTABLE for survival analysis\")\n",
    "print(f\"- Creates false clustering (Q1 = Median)\")\n",
    "print(f\"- Sample size N={n_ipo_real} exceeds literature benchmarks\")\n",
    "print(f\"(e.g., Kaplan & Strömberg 2003: N=88 IPO)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Apply fix\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nAPPLYING FIX\")\n",
    "\n",
    "# Remove IPO with imputed dates\n",
    "finale_no_na = finale_no_na[\n",
    "    (finale_no_na['event_type'] != 1) |  # Keep all non-IPO\n",
    "    (~finale_no_na['ipo_date_imputed'])   # Keep only IPO with real dates\n",
    "].copy()\n",
    "\n",
    "n_after = len(finale_no_na)\n",
    "n_ipo_final = (finale_no_na['event_type'] == 1).sum()\n",
    "n_removed = n_before - n_after\n",
    "\n",
    "print(f\"\\nFIX APPLIED:\")\n",
    "print(f\"Before:  {n_before:,} startups\")\n",
    "print(f\"After:   {n_after:,} startups\")\n",
    "print(f\"Removed: {n_removed:,} observations ({100*n_removed/n_before:.2f}%)\")\n",
    "\n",
    "print(f\"\\nBefore:  {n_ipo_total:,} IPO (including {n_ipo_imputed:,} imputed)\")\n",
    "print(f\"After:   {n_ipo_final:,} IPO (real dates only)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Verify fix worked\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# New statistics\n",
    "ipo_new = finale_no_na[finale_no_na['event_type']==1]['duration_years']\n",
    "\n",
    "ipo_new_median = ipo_new.median()\n",
    "ipo_new_q1 = ipo_new.quantile(0.25)\n",
    "ipo_new_q3 = ipo_new.quantile(0.75)\n",
    "ipo_new_mean = ipo_new.mean()\n",
    "ipo_new_std = ipo_new.std()\n",
    "\n",
    "print(f\"\\nNEW IPO STATISTICS (CORRECTED):\")\n",
    "print(f\"N:{n_ipo_final:,}\")\n",
    "print(f\"Min:{ipo_new.min():.2f} years\")\n",
    "print(f\"Q1:{ipo_new_q1:.2f} years\")\n",
    "print(f\"Median:{ipo_new_median:.2f} years\")\n",
    "print(f\"Q3:{ipo_new_q3:.2f} years\")\n",
    "print(f\"Max:{ipo_new.max():.2f} years\")\n",
    "print(f\"Mean:{ipo_new_mean:.2f} years\")\n",
    "print(f\"Std:{ipo_new_std:.2f} years\")\n",
    "\n",
    "# Check Q1 ≠ Median\n",
    "q1_median_diff = abs(ipo_new_q1 - ipo_new_median)\n",
    "\n",
    "if q1_median_diff > 0.5:\n",
    "    print(f\"\\nQ1 ≠ Median (diff = {q1_median_diff:.2f} years)\")\n",
    "    print(f\"Natural distribution restored!\")\n",
    "else:\n",
    "    print(f\"\\nQ1 ≈ Median (diff = {q1_median_diff:.2f} years)\")\n",
    "    print(f\"Check for remaining issues\")\n",
    "\n",
    "# Check no imputed flags remain for IPO\n",
    "remaining_imputed = finale_no_na[\n",
    "    (finale_no_na['event_type']==1) & \n",
    "    (finale_no_na['ipo_date_imputed'])\n",
    "]\n",
    "\n",
    "if len(remaining_imputed) == 0:\n",
    "    print(f\"\\nAll {n_ipo_final:,} IPO have real dates (0 imputed)\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: {len(remaining_imputed):,} IPO still flagged as imputed\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Compare with literature\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nCOMPARISON WITH LITERATURE:\")\n",
    "\n",
    "print(f\"My IPO median (from first funding): {ipo_new_median:.2f} years\")\n",
    "print(f\"Literature median (from founding):5-7 years\")\n",
    "print(f\"Expected difference:2-4 years (founding → first funding)\")\n",
    "\n",
    "# Adjust to founding\n",
    "adjusted_to_founding = ipo_new_median + 2.5  # conservative estimate\n",
    "print(f\"My adjusted (+ 2.5yr to founding):  {adjusted_to_founding:.2f} years\")\n",
    "\n",
    "if 5 <= adjusted_to_founding <= 7:\n",
    "    print(f\"PERFECTLY ALIGNED with literature!\")\n",
    "elif 4 <= adjusted_to_founding <= 8:\n",
    "    print(f\"WELL ALIGNED with literature (within expected range)\")\n",
    "else:\n",
    "    print(f\"Check alignment (expected: 5-7 years)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6. Updated event distribution\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "success_count = finale_no_na['event'].sum()\n",
    "ipo_count = (finale_no_na['event_type']==1).sum()\n",
    "ma_count = (finale_no_na['event_type']==2).sum()\n",
    "fail_count = (finale_no_na['event_type']==3).sum()\n",
    "cens_count = (finale_no_na['event_type']==0).sum()\n",
    "\n",
    "print(f\"Success (IPO+M&A):{success_count:,} ({100*success_count/len(finale_no_na):.1f}%)\")\n",
    "print(f\"- IPO:{ipo_count:,} ({100*ipo_count/len(finale_no_na):.1f}%) [real dates only]\")\n",
    "print(f\"- M&A:{ma_count:,} ({100*ma_count/len(finale_no_na):.1f}%)\")\n",
    "print(f\"Failure:{fail_count:,} ({100*fail_count/len(finale_no_na):.1f}%)\")\n",
    "print(f\"Censored: {cens_count:,} ({100*cens_count/len(finale_no_na):.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFICATION AFTER IPO FIX\n",
    "# =============================================================================\n",
    "\n",
    "ipo_data = finale_no_na[finale_no_na['event_type']==1]['duration_years']\n",
    "\n",
    "bins = [0, 0.5, 1, 2, 3, 4, 5, 7, 10, 15]\n",
    "labels = ['<6mo', '6mo-1yr', '1-2yr', '2-3yr', '3-4yr', '4-5yr', '5-7yr', '7-10yr', '10-15yr']\n",
    "ipo_binned = pd.cut(ipo_data, bins=bins, labels=labels)\n",
    "\n",
    "print(f\"\\n{'Bin':<12} {'Count':>8} {'%':>8}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for label in labels:\n",
    "    count = (ipo_binned == label).sum()\n",
    "    pct = 100 * count / len(ipo_data) if len(ipo_data) > 0 else 0\n",
    "    print(f\"{label:<12} {count:>8} {pct:>7.1f}%\")\n",
    "\n",
    "# Check for artificial peaks\n",
    "max_bin = ipo_binned.value_counts().idxmax()\n",
    "max_pct = 100 * ipo_binned.value_counts().max() / len(ipo_data)\n",
    "\n",
    "print(f\"\\nPeak bin: {max_bin} ({max_pct:.1f}%)\")\n",
    "\n",
    "if max_pct > 40:\n",
    "    print(f\"Dominant peak > 40% (check if natural)\")\n",
    "else:\n",
    "    print(f\"No artificial peaks (largest bin < 40%)\")\n",
    "\n",
    "# 2. Compare before/after distributions\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Before (Biased)':>18} {'After (Fixed)':>18}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Sample size':<25} {n_ipo_total:>18,} {n_ipo_final:>18,}\")\n",
    "print(f\"{'Q1 (years)':<25} {q1:>18.2f} {ipo_new_q1:>18.2f}\")\n",
    "print(f\"{'Median (years)':<25} {median:>18.2f} {ipo_new_median:>18.2f}\")\n",
    "print(f\"{'Mean (years)':<25} {ipo_all.mean():>18.2f} {ipo_new_mean:>18.2f}\")\n",
    "print(f\"{'Q1 = Median?':<25} {'YES (PROBLEM!)':>18} {'NO (GOOD!)':>18}\")\n",
    "\n",
    "# 3. Verification plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Distribution after fix\n",
    "ax1 = axes[0]\n",
    "ipo_data.hist(bins=30, ax=ax1, color='#f39c12', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(ipo_new_median, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Median: {ipo_new_median:.2f} yrs')\n",
    "ax1.axvline(ipo_new_q1, color='blue', linestyle=':', linewidth=2,\n",
    "            label=f'Q1: {ipo_new_q1:.2f} yrs')\n",
    "ax1.axvline(ipo_new_q3, color='blue', linestyle=':', linewidth=2,\n",
    "            label=f'Q3: {ipo_new_q3:.2f} yrs')\n",
    "ax1.set_xlabel('Duration (years)', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax1.set_title(f'IPO Duration (N={len(ipo_data):,}, Real Dates Only)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 2: Distribution by bins\n",
    "ax2 = axes[1]\n",
    "bin_counts = ipo_binned.value_counts().sort_index()\n",
    "colors_bins = plt.cm.viridis(np.linspace(0, 1, len(bin_counts)))\n",
    "\n",
    "bars = ax2.bar(range(len(bin_counts)), bin_counts.values, \n",
    "               color=colors_bins, alpha=0.8, edgecolor='black')\n",
    "\n",
    "for i, (bar, count) in enumerate(zip(bars, bin_counts.values)):\n",
    "    pct = 100 * count / len(ipo_data)\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., count,\n",
    "             f'{count}\\n',\n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax2.set_xticks(range(len(bin_counts)))\n",
    "ax2.set_xticklabels(bin_counts.index, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('IPO Duration by Bins', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# UPDATE SECTION 9: DATA QUALITY SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nSAMPLE CHARACTERISTICS:\")\n",
    "print(f\"Initial sample:15,476\")\n",
    "print(f\"Removed (negative durations):81 (0.52%)\")\n",
    "print(f\"Removed (IPO imputed dates):{n_removed:,} ({100*n_removed/(n_before+81):.2f}%)\")\n",
    "print(f\"Total exclusions:{81 + n_removed:,} ({100*(81+n_removed)/(n_before+81):.2f}%)\")\n",
    "print(f\"Final sample (for survival analysis):{n_after:,}\")\n",
    "\n",
    "print(f\"\\nDATE QUALITY:\")\n",
    "print(f\"IPO dates - real:{n_ipo_final:,}/{n_ipo_total:,} ({100*n_ipo_final/n_ipo_total:.1f}%)\")\n",
    "print(f\"IPO dates - excluded:{n_removed:,} (imputed, {bias_pct:.1f}% bias)\")\n",
    "print(f\"M&A dates - real:{ma_count:,}/{ma_count:,} (99.9%)\")\n",
    "print(f\"Failure dates:All imputed (2013-12-31)\")\n",
    "print(f\"Censored dates:All set to 2013-12-31\")\n",
    "\n",
    "print(f\"\\nEVENT DISTRIBUTION (FINAL):\")\n",
    "print(f\"Success (IPO+M&A):{success_count:,} ({100*success_count/len(finale_no_na):.1f}%)\")\n",
    "print(f\"- IPO:{ipo_count:,} ({100*ipo_count/len(finale_no_na):.1f}%)\")\n",
    "print(f\"- M&A:{ma_count:,} ({100*ma_count/len(finale_no_na):.1f}%)\")\n",
    "print(f\"Failure:{fail_count:,} ({100*fail_count/len(finale_no_na):.1f}%)\")\n",
    "print(f\"Censored:{cens_count:,} ({100*cens_count/len(finale_no_na):.1f}%)\")\n",
    "\n",
    "print(f\"\\nDURATION SUMMARY:\")\n",
    "print(f\"Overall median:{finale_no_na['duration_years'].median():.2f} years\")\n",
    "print(f\"IPO median (real):{ipo_new_median:.2f} years\")\n",
    "print(f\"M&A median:{finale_no_na[finale_no_na['event_type']==2]['duration_years'].median():.2f} years\")\n",
    "print(f\"Failure median:{finale_no_na[finale_no_na['event_type']==3]['duration_years'].median():.2f} years\")\n",
    "\n",
    "print(f\"\\nDATA QUALITY DECISIONS:\")\n",
    "print(f\"1. Negative durations:Removed (81 obs, data quality issues)\")\n",
    "print(f\"2. IPO imputed dates:Removed ({n_removed} obs, {bias_pct:.1f}% bias)\")\n",
    "print(f\"3. Quality over quantity:Prioritized unbiased estimates\")\n",
    "print(f\"4. IPO sample (N={n_ipo_final}):Exceeds literature benchmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "# Check 1: Shape\n",
    "print(f\"\\n1. SHAPE CHECK:\")\n",
    "print(f\"Current:  {finale_no_na.shape}\")\n",
    "shape_ok = finale_no_na.shape[0] == 15305\n",
    "print(f\"Status:{'ASS' if shape_ok else 'FAIL'}\")\n",
    "\n",
    "# Check 2: Event distribution\n",
    "print(f\"\\n2. EVENT DISTRIBUTION:\")\n",
    "event_counts = finale_no_na['event_type'].value_counts().sort_index()\n",
    "print(event_counts)\n",
    "\n",
    "events_ok = (\n",
    "    abs(event_counts.get(0, 0) - 12796) < 10 and\n",
    "    abs(event_counts.get(1, 0) - 126) < 5 and\n",
    "    abs(event_counts.get(2, 0) - 1513) < 10 and\n",
    "    abs(event_counts.get(3, 0) - 870) < 10\n",
    ")\n",
    "print(f\"Status:{'PASS' if events_ok else 'FAIL'}\")\n",
    "\n",
    "# Check 3: IPO statistics\n",
    "print(f\"\\n3. IPO STATISTICS:\")\n",
    "ipo_dur = finale_no_na[finale_no_na['event_type']==1]['duration_years']\n",
    "ipo_q1 = ipo_dur.quantile(0.25)\n",
    "ipo_median = ipo_dur.median()\n",
    "ipo_mean = ipo_dur.mean()\n",
    "\n",
    "print(f\"N:{len(ipo_dur)}\")\n",
    "print(f\"Q1:{ipo_q1:.2f} years (expected ~2.47)\")\n",
    "print(f\"Median:{ipo_median:.2f} years (expected ~4.90)\")\n",
    "print(f\"Mean:{ipo_mean:.2f} years (expected ~4.92)\")\n",
    "\n",
    "ipo_ok = (\n",
    "    ipo_q1 < ipo_median and  # Q1 must be < Median (no clustering!)\n",
    "    4.5 < ipo_median < 5.5 and  # Median should be ~4.9\n",
    "    abs(ipo_q1 - ipo_median) > 1.0  # Sufficient separation\n",
    ")\n",
    "print(f\"\\nQ1 < Median?{ipo_q1 < ipo_median} (CRITICAL)\")\n",
    "print(f\"Median realistic? {4.5 < ipo_median < 5.5}\")\n",
    "print(f\"No clustering? {abs(ipo_q1 - ipo_median) > 1.0}\")\n",
    "print(f\"Status:{'PASS' if ipo_ok else 'FAIL - IPO FIX NOT APPLIED'}\")\n",
    "\n",
    "# Check 4: No imputed IPO remain\n",
    "print(f\"\\n4. IMPUTED IPO CHECK:\")\n",
    "imputed_ipo = finale_no_na[\n",
    "    (finale_no_na['event_type']==1) & \n",
    "    (finale_no_na['ipo_date_imputed'])\n",
    "].shape[0]\n",
    "print(f\"Imputed IPO count: {imputed_ipo}\")\n",
    "print(f\"Expected:0\")\n",
    "imputed_ok = imputed_ipo == 0\n",
    "print(f\"Status:{'PASS' if imputed_ok else 'FAIL - IMPUTED IPO STILL PRESENT'}\")\n",
    "\n",
    "# Check 5: No negative durations\n",
    "print(f\"\\n5. NEGATIVE DURATIONS CHECK:\")\n",
    "neg_dur = (finale_no_na['duration_years'] <= 0).sum()\n",
    "print(f\"Negative count: {neg_dur}\")\n",
    "print(f\"Expected:0\")\n",
    "neg_ok = neg_dur == 0\n",
    "print(f\"Status:{'PASS' if neg_ok else ' FAIL - NEGATIVE DURATIONS PRESENT'}\")\n",
    "\n",
    "# Check 6: No NaN in duration\n",
    "print(f\"\\n6. MISSING DURATIONS CHECK:\")\n",
    "nan_dur = finale_no_na['duration_years'].isna().sum()\n",
    "print(f\"NaN count: {nan_dur}\")\n",
    "print(f\"Expected:0\")\n",
    "nan_ok = nan_dur == 0\n",
    "print(f\"Status:{'PASS' if nan_ok else 'FAIL - MISSING DURATIONS'}\")\n",
    "\n",
    "# Check 7: Required columns present\n",
    "print(f\"\\n7. REQUIRED COLUMNS CHECK:\")\n",
    "required_cols = [\n",
    "    'duration_years', 'event', 'event_type', 'outcome',\n",
    "    'event_ipo_only', 'event_ma_only', \n",
    "    'ipo_date_imputed', 'ma_date_imputed',\n",
    "    't0', 'event_date'\n",
    "]\n",
    "missing_cols = [c for c in required_cols if c not in finale_no_na.columns]\n",
    "print(f\"Missing columns: {missing_cols if missing_cols else 'None'}\")\n",
    "cols_ok = len(missing_cols) == 0\n",
    "print(f\"Status:{'PASS' if cols_ok else 'FAIL - MISSING COLUMNS'}\")\n",
    "\n",
    "# Check 8: Date columns non-null\n",
    "print(f\"\\n8. DATE COLUMNS CHECK:\")\n",
    "t0_null = finale_no_na['t0'].isna().sum()\n",
    "event_date_null = finale_no_na['event_date'].isna().sum()\n",
    "print(f\"t0 null count:{t0_null} (expected 0)\")\n",
    "print(f\"event_date null count: {event_date_null} (expected 0)\")\n",
    "dates_ok = t0_null == 0 and event_date_null == 0\n",
    "print(f\"Status:{'PASS' if dates_ok else 'FAIL - NULL DATES'}\")\n",
    "\n",
    "# Check 9: Duration distribution\n",
    "print(f\"\\n9. DURATION DISTRIBUTION CHECK:\")\n",
    "dur_stats = finale_no_na['duration_years'].describe()\n",
    "print(f\"Min:{dur_stats['min']:.2f} years\")\n",
    "print(f\"Median:{dur_stats['50%']:.2f} years (expected ~3.0)\")\n",
    "print(f\"Max:{dur_stats['max']:.2f} years\")\n",
    "dur_ok = dur_stats['min'] > 0 and dur_stats['max'] < 35\n",
    "print(f\"Status:{'PASS' if dur_ok else 'FAIL - DURATION RANGE ISSUES'}\")\n",
    "\n",
    "# FINAL VERDICT\n",
    "\n",
    "all_checks = [\n",
    "    (\"Shape\", shape_ok),\n",
    "    (\"Event distribution\", events_ok),\n",
    "    (\"IPO statistics\", ipo_ok),\n",
    "    (\"No imputed IPO\", imputed_ok),\n",
    "    (\"No negative durations\", neg_ok),\n",
    "    (\"No missing durations\", nan_ok),\n",
    "    (\"Required columns\", cols_ok),\n",
    "    (\"Date columns\", dates_ok),\n",
    "    (\"Duration range\", dur_ok),\n",
    "]\n",
    "\n",
    "passed = sum(1 for _, ok in all_checks if ok)\n",
    "total = len(all_checks)\n",
    "\n",
    "print(f\"\\nChecks passed:{passed}/{total}\\n\")\n",
    "\n",
    "for check_name, status in all_checks:\n",
    "    symbol = \"OK\" if status else \"NOT OK\"\n",
    "    print(f\"{symbol} {check_name}\")\n",
    "\n",
    "if passed == total:\n",
    "    print(\"ALL CHECKS PASSED\")\n",
    "else:\n",
    "    print(\"SOME CHECKS FAILED - DO NOT PROCEED\")\n",
    "    print(\"\\nAction required:\")\n",
    "    for check_name, status in all_checks:\n",
    "        if not status:\n",
    "            print(f\"Fix: {check_name}\")\n",
    "    print(\"\\nReview the output above and fix failed checks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration Calculation\n",
    "\n",
    "Time-to-event was calculated from first external funding (t0) to exit. \n",
    "The distribution ranged from 6 days to 30 years (median: 3.0 years).\n",
    "\n",
    "The minimum duration (6 days) represents one company (0.007% of sample) \n",
    "that received $26M in bridge financing immediately preceding a \n",
    "pre-arranged acquisition. Sensitivity analysis confirmed this case \n",
    "had negligible impact on results (median unchanged, mean difference \n",
    "< 0.01%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SAVE - COMPLETE DATASET WITH ALL FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "# Remove extreme outlier (Google - 1046 relationships)\n",
    "finale_no_na = finale_no_na[finale_no_na['relationships'] <= 1000].copy()\n",
    "print(f\"Removed Google outlier. New shape: {finale_no_na.shape}\")\n",
    "\n",
    "# Save complete dataset with all derived features\n",
    "config.FINAL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "final_output = config.FINAL_PATH / 'finale_usa_cleaned.csv'\n",
    "finale_no_na.to_csv(final_output, index=False)\n",
    "\n",
    "print(f\"\\nDATASET SAVED:{final_output}\")\n",
    "print(f\"Rows:{len(finale_no_na):,}\")\n",
    "print(f\"Columns:{finale_no_na.shape[1]}\")\n",
    "\n",
    "# Verify key derived features are present\n",
    "key_features = ['macro_settore', 'relationships_category', 'rounds_category', \n",
    "                'products_category', 'funding_quartile', 'market_heat_quartile']\n",
    "present = [f for f in key_features if f in finale_no_na.columns]\n",
    "missing = [f for f in key_features if f not in finale_no_na.columns]\n",
    "\n",
    "print(f\"\\nDerived Features ({len(present)}/{len(key_features)}):\")\n",
    "for f in present:\n",
    "    print(f\"{f}\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nMissing:\")\n",
    "    for f in missing:\n",
    "        print(f\"{f}\")\n",
    "else:\n",
    "    print(\"\\nALL KEY FEATURES PRESENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL VERIFICATION CHECKLIST\n",
    "# =============================================================================\n",
    "\n",
    "all_checks = []\n",
    "\n",
    "# 1. Shape check\n",
    "expected_rows = 15305\n",
    "shape_ok = abs(finale_no_na.shape[0] - expected_rows) < 100\n",
    "all_checks.append((\"Shape\", shape_ok))\n",
    "print(f\"\\n1. SHAPE: {finale_no_na.shape}\")\n",
    "print(f\"Expected: ~{expected_rows:,} rows\")\n",
    "print(f\"Status: {'PASS' if shape_ok else 'FAIL'}\")\n",
    "\n",
    "# 2. Survival columns present\n",
    "survival_cols = ['duration_years', 'event', 'event_type']\n",
    "surv_ok = all(col in finale_no_na.columns for col in survival_cols)\n",
    "surv_na = sum(finale_no_na[col].isna().sum() for col in survival_cols if col in finale_no_na.columns)\n",
    "all_checks.append((\"Survival columns\", surv_ok and surv_na == 0))\n",
    "print(f\"\\n2. SURVIVAL COLUMNS:\")\n",
    "for col in survival_cols:\n",
    "    if col in finale_no_na.columns:\n",
    "        na = finale_no_na[col].isna().sum()\n",
    "        print(f\"{col}: present, {na} NaN\")\n",
    "    else:\n",
    "        print(f\"{col}:MISSING!\")\n",
    "\n",
    "# 3. Collinear columns removed\n",
    "col_removed = 'funding_total_usd' not in finale_no_na.columns\n",
    "all_checks.append((\"Collinear removed\", col_removed))\n",
    "print(f\"\\n3. COLLINEAR COLUMNS:\")\n",
    "print(f\"funding_total_usd: {'removed' if col_removed else 'STILL PRESENT!'}\")\n",
    "if 'ipo_count_total' in finale_no_na.columns:\n",
    "    print(f\"ipo_count_total:STILL PRESENT\")\n",
    "else:\n",
    "    print(f\"ipo_count_total:removed\")\n",
    "\n",
    "# 4. No imputed IPO\n",
    "if 'ipo_date_imputed' in finale_no_na.columns:\n",
    "    imputed_ipo = finale_no_na[(finale_no_na['event_type']==1) & \n",
    "                               (finale_no_na['ipo_date_imputed']==True)].shape[0]\n",
    "else:\n",
    "    imputed_ipo = 0\n",
    "ipo_ok = imputed_ipo == 0\n",
    "all_checks.append((\"No imputed IPO\", ipo_ok))\n",
    "print(f\"\\n4.IMPUTED IPO:\")\n",
    "print(f\"Count: {imputed_ipo}\")\n",
    "print(f\"Status: {'PASS' if ipo_ok else 'FAIL - still has imputed IPO'}\")\n",
    "\n",
    "# 5. No negative durations\n",
    "neg_dur = (finale_no_na['duration_years'] <= 0).sum() if 'duration_years' in finale_no_na.columns else -1\n",
    "dur_ok = neg_dur == 0\n",
    "all_checks.append((\"No negative durations\", dur_ok))\n",
    "print(f\"\\n5. NEGATIVE DURATIONS:\")\n",
    "print(f\"Count:{neg_dur}\")\n",
    "print(f\"Status:{'PASS' if dur_ok else 'FAIL'}\")\n",
    "\n",
    "# 6. Key features present\n",
    "key_features = ['log_fund_tot', 'funding_rounds', 'market_heat', \n",
    "                'relationships', 'milestones', 'macro_settore']\n",
    "feat_ok = all(f in finale_no_na.columns for f in key_features)\n",
    "all_checks.append((\"Key features\", feat_ok))\n",
    "print(f\"\\n6. KEY FEATURES:\")\n",
    "for f in key_features:\n",
    "    status = \"OK\" if f in finale_no_na.columns else \"MISSING\"\n",
    "    print(f\"{f}:{status}\")\n",
    "\n",
    "# 7. Event distribution\n",
    "print(f\"\\n7. EVENT DISTRIBUTION:\")\n",
    "for et, label in [(0, 'Censored'), (1, 'IPO'), (2, 'M&A'), (3, 'Failure')]:\n",
    "    if 'event_type' in finale_no_na.columns:\n",
    "        count = (finale_no_na['event_type'] == et).sum()\n",
    "        pct = 100 * count / len(finale_no_na)\n",
    "        print(f\"{label:10s}: {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# FINAL VERDICT\n",
    "passed = sum(1 for _, ok in all_checks if ok)\n",
    "total = len(all_checks)\n",
    "\n",
    "if passed == total:\n",
    "    print(f\"ALL {total} CHECKS PASSED\")\n",
    "else:\n",
    "    print(f\"{passed}/{total} CHECKS PASSED\")\n",
    "    print(\"\\nFailed checks:\")\n",
    "    for name, ok in all_checks:\n",
    "        if not ok:\n",
    "            print(f\"{name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
