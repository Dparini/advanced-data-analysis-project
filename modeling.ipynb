{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Survival Analysis and Predictive Modeling\n",
    "\n",
    "**Objective**: Predict probability and timing of exit events (IPO, M&A) for VC-backed startups.\n",
    "\n",
    "**Methods**:\n",
    "1. Kaplan-Meier estimation\n",
    "2. Cox Proportional Hazards\n",
    "3. Competing Risks (Fine-Gray)\n",
    "4. Random Survival Forest\n",
    "5. Classification comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# Autoreload everything\n",
    "%autoreload 2\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter, WeibullAFTFitter\n",
    "from lifelines.statistics import logrank_test, multivariate_logrank_test\n",
    "from lifelines import AalenJohansenFitter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Configurazione paths\n",
    "\n",
    "# Crea directories\n",
    "config.OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.FINAL_PATH / 'finale_usa_cleaned.csv', low_memory=False)\n",
    "\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Events: {df[\"event\"].sum()} / {len(df)} ({100*df[\"event\"].mean():.1f}%)')\n",
    "print(f'Duration: median={df[\"duration_years\"].median():.2f}, range=[{df[\"duration_years\"].min():.2f}, {df[\"duration_years\"].max():.2f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dist = df['event_type'].value_counts().sort_index()\n",
    "event_labels = {0: 'Censored', 1: 'IPO', 2: 'M&A', 3: 'Failure'}\n",
    "\n",
    "for idx, count in event_dist.items():\n",
    "    print(f'{event_labels[idx]}: {count} ({100*count/len(df):.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC = ['log_fund_tot', 'funding_rounds', 'market_heat', 'relationships', \n",
    "           'milestones', 'angel', 'series_a', 'series_b', 'series_c', 'venture']\n",
    "\n",
    "CATEGORICAL = ['macro_settore', 'market_cycle']\n",
    "\n",
    "T = 'duration_years'\n",
    "E = 'event'\n",
    "E_TYPE = 'event_type'\n",
    "\n",
    "df_model = df[NUMERIC + CATEGORICAL + [T, E, E_TYPE]].copy()\n",
    "print(f'Model dataset: {df_model.shape}, missing: {df_model.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kaplan-Meier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(df_model[T], df_model[E], label='Overall')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "kmf.plot_survival_function(ax=ax, ci_show=True)\n",
    "ax.set_xlabel('Years from First Funding')\n",
    "ax.set_ylabel('Survival Probability')\n",
    "ax.set_title('Overall Survival Curve')\n",
    "ax.set_xlim(0, 15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Median survival: {kmf.median_survival_time_:.2f} years')\n",
    "for t in [1, 3, 5, 10]:\n",
    "    print(f'S({t}yr): {kmf.predict(t):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colors = {1: '#2ecc71', 2: '#3498db', 3: '#e74c3c'}\n",
    "labels = {1: 'IPO', 2: 'M&A', 3: 'Failure'}\n",
    "\n",
    "for etype in [1, 2, 3]:\n",
    "    mask = df_model[E_TYPE].isin([0, etype])\n",
    "    event_bin = (df_model.loc[mask, E_TYPE] == etype).astype(int)\n",
    "    kmf_sub = KaplanMeierFitter()\n",
    "    kmf_sub.fit(df_model.loc[mask, T], event_bin, label=labels[etype])\n",
    "    kmf_sub.plot_survival_function(ax=ax, ci_show=False, color=colors[etype])\n",
    "\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_ylabel('Survival Probability')\n",
    "ax.set_title('Survival by Exit Type')\n",
    "ax.set_xlim(0, 15)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_km_by_group(data, group_col, title):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for grp in sorted(data[group_col].dropna().unique()):\n",
    "        mask = data[group_col] == grp\n",
    "        kmf_g = KaplanMeierFitter()\n",
    "        kmf_g.fit(data.loc[mask, T], data.loc[mask, E], label=str(grp))\n",
    "        kmf_g.plot_survival_function(ax=ax, ci_show=False)\n",
    "    \n",
    "    ax.set_xlabel('Years')\n",
    "    ax.set_ylabel('Survival Probability')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, 15)\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    result = multivariate_logrank_test(data[T], data[group_col], data[E])\n",
    "    print(f'Log-rank: chi2={result.test_statistic:.2f}, p={result.p_value:.2e}')\n",
    "    return result.p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sector = plot_km_by_group(df_model, 'macro_settore', 'Survival by Sector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cycle = plot_km_by_group(df_model, 'market_cycle', 'Survival by Market Cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model['funding_q'] = pd.qcut(df_model['log_fund_tot'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "p_funding = plot_km_by_group(df_model, 'funding_q', 'Survival by Funding Quartile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logrank_results = []\n",
    "\n",
    "test_vars = {\n",
    "    'macro_settore': df_model['macro_settore'],\n",
    "    'market_cycle': df_model['market_cycle'],\n",
    "    'funding_q': df_model['funding_q'],\n",
    "    'has_milestones': (df_model['milestones'] > 0).map({True: 'Yes', False: 'No'}),\n",
    "    'high_network': (df_model['relationships'] > df_model['relationships'].median()).map({True: 'High', False: 'Low'})\n",
    "}\n",
    "\n",
    "for name, var in test_vars.items():\n",
    "    res = multivariate_logrank_test(df_model[T], var, df_model[E])\n",
    "    logrank_results.append({'Variable': name, 'Chi2': res.test_statistic, \n",
    "                            'p_value': res.p_value, 'Sig': res.p_value < 0.05})\n",
    "\n",
    "lr_df = pd.DataFrame(logrank_results).sort_values('p_value')\n",
    "print(lr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cox Proportional Hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cox = df_model.drop(columns=['funding_q']).copy()\n",
    "df_cox = pd.get_dummies(df_cox, columns=CATEGORICAL, drop_first=True)\n",
    "print(f'Cox dataset: {df_cox.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df_cox.columns if c not in [T, E, E_TYPE]]\n",
    "uni_results = []\n",
    "\n",
    "for feat in features:\n",
    "    try:\n",
    "        cph_u = CoxPHFitter()\n",
    "        cph_u.fit(df_cox[[T, E, feat]], duration_col=T, event_col=E)\n",
    "        s = cph_u.summary\n",
    "        uni_results.append({\n",
    "            'Feature': feat,\n",
    "            'HR': s.loc[feat, 'exp(coef)'],\n",
    "            'CI_low': s.loc[feat, 'exp(coef) lower 95%'],\n",
    "            'CI_high': s.loc[feat, 'exp(coef) upper 95%'],\n",
    "            'p': s.loc[feat, 'p']\n",
    "        })\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "uni_df = pd.DataFrame(uni_results).sort_values('p')\n",
    "print('Univariate Cox (top 15):')\n",
    "print(uni_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# TRAIN/TEST SPLIT - UNICO PER TUTTI I MODELLI\n",
    "# ========================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Base dataset\n",
    "X_all = df_cox.drop(columns=[T, E, E_TYPE])\n",
    "y_t_all = df_cox[T]\n",
    "y_e_all = df_cox[E]\n",
    "\n",
    "# SPLIT UNICO\n",
    "X_train_all, X_test_all, y_t_train, y_t_test, y_e_train, y_e_test = train_test_split(\n",
    "    X_all, y_t_all, y_e_all,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y_e_all\n",
    ")\n",
    "\n",
    "train_idx = X_train_all.index\n",
    "test_idx = X_test_all.index\n",
    "\n",
    "print(f'Train: {len(train_idx):,} ({y_e_train.mean():.1%} events)')\n",
    "print(f'Test:  {len(test_idx):,} ({y_e_test.mean():.1%} events)')\n",
    "print('='*80)\n",
    "\n",
    "# Scaling per ML models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_all), \n",
    "    columns=X_all.columns, \n",
    "    index=train_idx\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_all), \n",
    "    columns=X_all.columns, \n",
    "    index=test_idx\n",
    ")\n",
    "\n",
    "# Ricrea df_cox_train/test\n",
    "df_cox_train = df_cox.loc[train_idx].copy()\n",
    "df_cox_test = df_cox.loc[test_idx].copy()\n",
    "\n",
    "print(f'\\nCox train: {len(df_cox_train)}, test: {len(df_cox_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# Cox PH: Train/Test Performance\n",
    "# ========================================================================\n",
    "\n",
    "print('COX PH TRAIN/TEST EVALUATION')\n",
    "\n",
    "cph = CoxPHFitter(penalizer=0.01)\n",
    "cph.fit(df_cox_train.drop(columns=[E_TYPE]), duration_col=T, event_col=E)\n",
    "\n",
    "# Train C-index\n",
    "ci_cox_train = cph.concordance_index_\n",
    "\n",
    "# Test C-index\n",
    "cph_pred_test = cph.predict_partial_hazard(df_cox_test.drop(columns=[E_TYPE]))\n",
    "ci_cox_test = concordance_index_censored(\n",
    "    df_cox_test[E].astype(bool),\n",
    "    df_cox_test[T],\n",
    "    cph_pred_test\n",
    ")[0]\n",
    "\n",
    "print(f'Cox PH C-index:')\n",
    "print(f'  Train: {ci_cox_train:.4f}')\n",
    "print(f'  Test:  {ci_cox_test:.4f}')\n",
    "print(f'  Overfit gap: {(ci_cox_train - ci_cox_test)*100:+.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHFitter(penalizer=0.01)\n",
    "cph.fit(df_cox.drop(columns=['event_type']), duration_col=T, event_col=E)\n",
    "\n",
    "print(f'C-index: {cph.concordance_index_:.4f}')\n",
    "print(f'LR test p-value: {cph.log_likelihood_ratio_test().p_value:.2e}')\n",
    "print('\\nSignificant (p<0.05):')\n",
    "sig = cph.summary[cph.summary['p'] < 0.05][\n",
    "    ['exp(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p']\n",
    "].round(4)\n",
    "print(sig.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.statistics import proportional_hazard_test\n",
    "\n",
    "ph_test = proportional_hazard_test(cph, df_cox.drop(columns=['event_type']), time_transform='rank')\n",
    "print(\"Test Proportional Hazards:\")\n",
    "print(ph_test.summary)\n",
    "\n",
    "cph.check_assumptions(df_cox.drop(columns=['event_type']), p_value_threshold=0.05, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cph.plot(ax=ax, hazard_ratios=True)\n",
    "ax.axvline(x=1, color='black', linestyle=':', alpha=0.5)\n",
    "ax.set_title('Hazard Ratios (95% CI)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# Cox PH Multivariate (Full Dataset) - per PH check e analisi esplorativa\n",
    "# ========================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('COX PH MULTIVARIATE MODEL (FULL DATASET)')\n",
    "print('='*80)\n",
    "\n",
    "cph_full = CoxPHFitter(penalizer=0.01)\n",
    "cph_full.fit(df_cox.drop(columns=[E_TYPE]), duration_col=T, event_col=E)\n",
    "\n",
    "print(f'\\nC-index (full dataset): {cph_full.concordance_index_:.4f}')\n",
    "print(f'LR test p-value: {cph_full.log_likelihood_ratio_test().p_value:.2e}')\n",
    "\n",
    "print('\\nTop significant variables (p < 0.05):')\n",
    "sig = cph_full.summary[cph_full.summary['p'] < 0.05][\n",
    "    ['exp(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p']\n",
    "].round(4).head(10)\n",
    "print(sig.to_string())\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# PH VIOLATIONS CHECK\n",
    "# ========================================================================\n",
    "\n",
    "print('PROPORTIONAL HAZARDS ASSUMPTIONS CHECK')\n",
    "\n",
    "ph_results = cph_full.check_assumptions(\n",
    "    df_cox.drop(columns=[E_TYPE]),\n",
    "    p_value_threshold=0.05, \n",
    "    show_plots=False\n",
    ")\n",
    "\n",
    "violations = []\n",
    "\n",
    "if hasattr(ph_results, 'summary'):\n",
    "    summary_df = ph_results.summary\n",
    "    for idx in summary_df.index:\n",
    "        p_val = summary_df.loc[idx, 'p']\n",
    "        if p_val < 0.05:\n",
    "            violations.append((idx, p_val))\n",
    "    \n",
    "    violations.sort(key=lambda x: x[1])\n",
    "    \n",
    "    if violations:\n",
    "        print(f\"\\nVariables violating PH assumption (p < 0.05):\")\n",
    "        print(f\"\\n{'Variable':<30} {'p-value':<12} {'Severity':<10}\")\n",
    "        print(\"-\"*52)\n",
    "        \n",
    "        for var, p_val in violations:\n",
    "            if p_val < 0.001:\n",
    "                severity = \"SEVERE\"\n",
    "            elif p_val < 0.01:\n",
    "                severity = \"MODERATE\"\n",
    "            else:\n",
    "                severity = \"MILD\"\n",
    "            print(f\"{var:<30} {p_val:<12.4f} {severity}\")\n",
    "        \n",
    "        print(f\"IMPLICATIONS:\")\n",
    "        print(f\"  • {len(violations)} variable(s) violate PH assumption\")\n",
    "        print(f\"  • Cox test C-index: {ci_cox_test:.4f} (may be biased)\")\n",
    "        print(f\"  • RSF is more reliable (no PH assumption)\")\n",
    "        print(f\"  • Recommend: Use RSF as primary model\")\n",
    "    else:\n",
    "        print(\"\\nPH assumptions satisfied for all covariates (p > 0.05)\")\n",
    "        print(f\"   Cox PH model is appropriate.\")\n",
    "        print(f\"   Cox test C-index: {ci_cox_test:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCould not extract PH test results\")\n",
    "    print(\"    Check the output above for details\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cph.plot_partial_effects_on_outcome('log_fund_tot', values=[10, 13, 16, 19], ax=ax, cmap='coolwarm')\n",
    "ax.set_title('Survival by Funding Level')\n",
    "ax.set_xlim(0, 15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Competing Risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajf = AalenJohansenFitter()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "colors = {1: '#2ecc71', 2: '#3498db', 3: '#e74c3c'}\n",
    "labels = {1: 'IPO', 2: 'M&A', 3: 'Failure'}\n",
    "\n",
    "for ecode in [1, 2, 3]:\n",
    "    ajf.fit(df_model[T], df_model[E_TYPE], event_of_interest=ecode)\n",
    "    ajf.plot(ax=ax, label=labels[ecode], color=colors[ecode])\n",
    "\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_ylabel('Cumulative Incidence')\n",
    "ax.set_title('Cumulative Incidence Functions')\n",
    "ax.set_xlim(0, 15)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cumulative incidence at 5 years:')\n",
    "for ecode in [1, 2, 3]:\n",
    "    ajf.fit(df_model[T], df_model[E_TYPE], event_of_interest=ecode)\n",
    "    ci = ajf.predict(5)\n",
    "    print(f'  {labels[ecode]}: {ci:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fine-Gray Subdistribution Hazards')\n",
    "\n",
    "def prepare_fine_gray_data(df, event_of_interest):\n",
    "    df_fg = df.copy()\n",
    "    df_fg['E_cr'] = (df_fg[E_TYPE] == event_of_interest).astype(int)\n",
    "    return df_fg\n",
    "\n",
    "df_fg_ipo = prepare_fine_gray_data(df_cox, 1)\n",
    "df_fg_ma = prepare_fine_gray_data(df_cox, 2)\n",
    "df_fg_fail = prepare_fine_gray_data(df_cox, 3)\n",
    "\n",
    "feat_cols = [c for c in df_cox.columns if c not in [T, E, E_TYPE]]\n",
    "\n",
    "print('IPO Subdistribution Model:')\n",
    "cph_ipo_sub = CoxPHFitter(penalizer=0.01)\n",
    "cph_ipo_sub.fit(df_fg_ipo[feat_cols + [T, 'E_cr']], duration_col=T, event_col='E_cr')\n",
    "print(f'C-index: {cph_ipo_sub.concordance_index_:.4f}')\n",
    "print(cph_ipo_sub.summary[cph_ipo_sub.summary['p'] < 0.05][['exp(coef)', 'p']].head())\n",
    "\n",
    "print('\\nM&A Subdistribution Model:')\n",
    "cph_ma_sub = CoxPHFitter(penalizer=0.01)\n",
    "cph_ma_sub.fit(df_fg_ma[feat_cols + [T, 'E_cr']], duration_col=T, event_col='E_cr')\n",
    "print(f'C-index: {cph_ma_sub.concordance_index_:.4f}')\n",
    "print(cph_ma_sub.summary[cph_ma_sub.summary['p'] < 0.05][['exp(coef)', 'p']].head())\n",
    "\n",
    "print('\\nFailure Subdistribution Model:')\n",
    "cph_fail_sub = CoxPHFitter(penalizer=0.01)\n",
    "cph_fail_sub.fit(df_fg_fail[feat_cols + [T, 'E_cr']], duration_col=T, event_col='E_cr')\n",
    "print(f'C-index: {cph_fail_sub.concordance_index_:.4f}')\n",
    "print(cph_fail_sub.summary[cph_fail_sub.summary['p'] < 0.05][['exp(coef)', 'p']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cause_specific_cox(data, cause, feat_cols):\n",
    "    df_c = data.copy()\n",
    "    df_c['e_cause'] = (df_c[E_TYPE] == cause).astype(int)\n",
    "    df_c = df_c[df_c[E_TYPE].isin([0, cause])]\n",
    "    \n",
    "    cph_c = CoxPHFitter(penalizer=0.01)\n",
    "    cph_c.fit(df_c[feat_cols + [T, 'e_cause']], duration_col=T, event_col='e_cause')\n",
    "    return cph_c\n",
    "\n",
    "feat_cols = [c for c in df_cox.columns if c not in [T, E, E_TYPE]]\n",
    "df_cr = df_cox.copy()\n",
    "df_cr[E_TYPE] = df_model[E_TYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_ipo = cause_specific_cox(df_cr, 1, feat_cols)\n",
    "print(f'IPO model C-index: {cph_ipo.concordance_index_:.4f}')\n",
    "print(cph_ipo.summary[cph_ipo.summary['p'] < 0.1][['exp(coef)', 'p']].round(4).head(8).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_ma = cause_specific_cox(df_cr, 2, feat_cols)\n",
    "print(f'M&A model C-index: {cph_ma.concordance_index_:.4f}')\n",
    "print(cph_ma.summary[cph_ma.summary['p'] < 0.05][['exp(coef)', 'p']].round(4).head(8).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_fail = cause_specific_cox(df_cr, 3, feat_cols)\n",
    "print(f'Failure model C-index: {cph_fail.concordance_index_:.4f}')\n",
    "print(cph_fail.summary[cph_fail.summary['p'] < 0.05][['exp(coef)', 'p']].round(4).head(8).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_feats = ['log_fund_tot', 'funding_rounds', 'market_heat', 'relationships', 'milestones']\n",
    "comp_data = []\n",
    "\n",
    "for f in compare_feats:\n",
    "    row = {'Feature': f}\n",
    "    for model, name in [(cph_ipo, 'IPO'), (cph_ma, 'M&A'), (cph_fail, 'Fail')]:\n",
    "        if f in model.summary.index:\n",
    "            row[name] = model.summary.loc[f, 'exp(coef)']\n",
    "    comp_data.append(row)\n",
    "\n",
    "print('HR comparison by outcome:')\n",
    "print(pd.DataFrame(comp_data).round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning: Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# Prepare survival data\n",
    "y_surv_train = Surv.from_dataframe(E, T, df_cox.loc[train_idx])\n",
    "y_surv_test = Surv.from_dataframe(E, T, df_cox.loc[test_idx])\n",
    "\n",
    "print(f'Train: {len(train_idx)}, Test: {len(test_idx)}')\n",
    "print(f'Event rate: train={y_e_train.mean():.2%}, test={y_e_test.mean():.2%}')\n",
    "\n",
    "# Scorer function\n",
    "def rsf_scorer(estimator, X, y):\n",
    "    return estimator.score(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [5, 8],\n",
    "    'min_samples_split': [50, 100],\n",
    "    'min_samples_leaf': [30, 50],\n",
    "    'max_features': ['sqrt'] \n",
    "}\n",
    "\n",
    "# Base RSF\n",
    "rsf_base = RandomSurvivalForest(random_state=SEED, n_jobs=-1)\n",
    "\n",
    "# Grid search with 3-fold CV\n",
    "grid_search = GridSearchCV(\n",
    "    rsf_base,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=rsf_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print('Fitting grid search')\n",
    "grid_search.fit(X_train_scaled, y_surv_train)\n",
    "\n",
    "print(f'\\nBest parameters found:')\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f'{param}: {value}')\n",
    "print(f'Best CV score: {grid_search.best_score_:.4f}')\n",
    "\n",
    "# Use best model\n",
    "rsf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RSF PERFORMANCE (TUNED MODEL)')\n",
    "\n",
    "# Train/Test C-index\n",
    "ci_rsf_train = rsf.score(X_train_scaled, y_surv_train)\n",
    "ci_rsf_test = rsf.score(X_test_scaled, y_surv_test)\n",
    "\n",
    "print(f'\\nC-index:')\n",
    "print(f'  Train: {ci_rsf_train:.4f}')\n",
    "print(f'  Test:  {ci_rsf_test:.4f}')\n",
    "print(f'  Overfit gap: {ci_rsf_train - ci_rsf_test:+.4f} ({(ci_rsf_train - ci_rsf_test)*100:+.2f}%)')\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(\n",
    "    rsf, X_train_scaled, y_surv_train,\n",
    "    cv=5,\n",
    "    scoring=rsf_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f'\\n5-Fold CV scores: {cv_scores}')\n",
    "print(f'CV Mean: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}')\n",
    "print(f'CV vs Test gap: {(cv_scores.mean() - ci_rsf_test)*100:+.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import brier_score\n",
    "\n",
    "times = np.percentile(df_cox[df_cox[E] == 1][T], np.linspace(10, 90, 9))\n",
    "max_time = min(y_surv_test['duration_years'].max(), rsf.unique_times_.max())\n",
    "times = times[times < max_time]\n",
    "\n",
    "surv_funcs_full = rsf.predict_survival_function(X_test_scaled, return_array=True)\n",
    "\n",
    "surv_funcs_at_times = np.zeros((len(X_test_scaled), len(times)))\n",
    "for i, t in enumerate(times):\n",
    "    time_idx = np.searchsorted(rsf.unique_times_, t)\n",
    "    if time_idx >= len(rsf.unique_times_):\n",
    "        time_idx = len(rsf.unique_times_) - 1\n",
    "    surv_funcs_at_times[:, i] = surv_funcs_full[:, time_idx]\n",
    "\n",
    "brier_scores = []\n",
    "for i, t in enumerate(times):\n",
    "    _, bs = brier_score(y_surv_train, y_surv_test, surv_funcs_at_times[:, i], t)\n",
    "    brier_scores.append(bs[0])\n",
    "\n",
    "ibs = np.trapezoid(brier_scores, times) / (times[-1] - times[0])\n",
    "\n",
    "print(f'Integrated Brier Score: {ibs:.4f}')\n",
    "print(f'Mean Brier Score: {np.mean(brier_scores):.4f}')\n",
    "print(f'Brier at each time:')\n",
    "for t, bs in zip(times, brier_scores):\n",
    "    print(f'  {t:.1f} years: {bs:.4f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(times, brier_scores, 'o-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Time (years)')\n",
    "ax.set_ylabel('Brier Score')\n",
    "ax.set_title('Brier Score over Time (RSF)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "t_eval = 5\n",
    "time_idx_5y = np.searchsorted(rsf.unique_times_, t_eval)\n",
    "if time_idx_5y >= len(rsf.unique_times_):\n",
    "    time_idx_5y = len(rsf.unique_times_) - 1\n",
    "\n",
    "surv_funcs_full = rsf.predict_survival_function(X_test_scaled, return_array=True)\n",
    "risk_pred_5y = 1 - surv_funcs_full[:, time_idx_5y]\n",
    "y_obs_5y = (y_surv_test['duration_years'] <= t_eval) & (y_surv_test['event'])\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_obs_5y, risk_pred_5y, n_bins=10, strategy='quantile'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(mean_predicted_value, fraction_of_positives, 'o-', linewidth=2, \n",
    "        markersize=8, label='RSF')\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax.set_xlabel('Predicted Risk at 5 Years')\n",
    "ax.set_ylabel('Observed Event Rate')\n",
    "ax.set_title('Calibration Plot (5-year Risk)')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 0.35)\n",
    "ax.set_ylim(0, 0.35)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean absolute calibration error: {np.mean(np.abs(fraction_of_positives - mean_predicted_value)):.4f}')\n",
    "print(f'Bins: {len(fraction_of_positives)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "times_eval = np.array([1, 3, 5, 10])\n",
    "\n",
    "risk_test = rsf.predict(X_test_scaled)\n",
    "\n",
    "auc_scores = []\n",
    "for t in times_eval:\n",
    "    mask = (y_surv_test['duration_years'] >= t) | (y_surv_test['event'] == True)\n",
    "    if mask.sum() < 10:\n",
    "        auc_scores.append(np.nan)\n",
    "        continue\n",
    "    \n",
    "    y_test_t = y_surv_test[mask]\n",
    "    risk_t = risk_test[mask]\n",
    "    \n",
    "    event_t = (y_test_t['duration_years'] <= t) & (y_test_t['event'] == True)\n",
    "    \n",
    "    try:\n",
    "        ci, _, _, _, _ = concordance_index_censored(event_t, y_test_t['duration_years'], risk_t)\n",
    "        auc_scores.append(ci)\n",
    "    except:\n",
    "        auc_scores.append(np.nan)\n",
    "\n",
    "auc_scores = np.array(auc_scores)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "\n",
    "print('Time-Dependent AUC (C-index):')\n",
    "for t, auc in zip(times_eval, auc_scores):\n",
    "    print(f'  {t} year(s): {auc:.4f}')\n",
    "print(f'Mean: {mean_auc:.4f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(times_eval, auc_scores, 'o-', linewidth=2, markersize=8)\n",
    "ax.axhline(y=0.5, color='k', linestyle='--', alpha=0.5, label='Random')\n",
    "ax.set_xlabel('Time (years)')\n",
    "ax.set_ylabel('C-index')\n",
    "ax.set_title('Discrimination Performance over Time (RSF)')\n",
    "ax.set_ylim([0.4, 1.0])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    rsf, \n",
    "    X_test_scaled,\n",
    "    y_surv_test,\n",
    "    n_repeats=10, \n",
    "    random_state=SEED, \n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "rsf_imp = pd.DataFrame({\n",
    "    'Feature': X_test_scaled.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "rsf_imp = rsf_imp.sort_values('Importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top = rsf_imp.head(15)\n",
    "ax.barh(range(len(top)), top['Importance'].values, color='blue')\n",
    "ax.set_yticks(range(len(top)))\n",
    "ax.set_yticklabels(top['Feature'].values)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('RSF Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "import matplotlib.image\n",
    "from pathlib import Path\n",
    "\n",
    "rf_for_tree = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=SEED, n_jobs=-1)\n",
    "rf_for_tree.fit(X_train_scaled, y_e_train)\n",
    "\n",
    "tree_idx = 3\n",
    "estimator = rf_for_tree.estimators_[tree_idx]\n",
    "\n",
    "dot_file = config.OUTPUT_PATH / 'tree.dot'\n",
    "png_file = config.OUTPUT_PATH / 'tree.png'\n",
    "\n",
    "export_graphviz(estimator, \n",
    "                out_file=str(dot_file),\n",
    "                feature_names=X_train_scaled.columns.tolist(),\n",
    "                class_names=['Censored', 'Event'],\n",
    "                rounded=True, \n",
    "                proportion=False,\n",
    "                precision=2, \n",
    "                filled=True)\n",
    "\n",
    "call(['dot', '-Tpng', str(dot_file), '-o', str(png_file), '-Gdpi=600'])\n",
    "\n",
    "img = matplotlib.image.imread(str(png_file))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f'Random Forest - Tree {tree_idx}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Saved to: {png_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bootstrap Confidence Intervals')\n",
    "\n",
    "n_bootstrap = 100\n",
    "bootstrap_preds = []\n",
    "\n",
    "sample_idx = 0\n",
    "X_sample = X_test_scaled.iloc[[sample_idx]]\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    idx = np.random.choice(len(X_train_scaled), len(X_train_scaled), replace=True)\n",
    "    X_boot = X_train_scaled.iloc[idx]\n",
    "    y_boot = y_surv_train[idx]\n",
    "    \n",
    "    rsf_boot = RandomSurvivalForest(\n",
    "        n_estimators=50, \n",
    "        max_depth=5,\n",
    "        random_state=i,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rsf_boot.fit(X_boot, y_boot)\n",
    "    \n",
    "    surv = rsf_boot.predict_survival_function(X_sample, return_array=True)\n",
    "    time_idx = np.searchsorted(rsf_boot.unique_times_, 5)\n",
    "    time_idx = min(time_idx, len(rsf_boot.unique_times_) - 1)\n",
    "    risk = 1 - surv[0, time_idx]\n",
    "    bootstrap_preds.append(risk)\n",
    "\n",
    "pred_mean = np.mean(bootstrap_preds)\n",
    "pred_ci = np.percentile(bootstrap_preds, [2.5, 97.5])\n",
    "\n",
    "print(f'Sample {sample_idx} - 5-year Risk:')\n",
    "print(f'Point estimate: {pred_mean:.3f}')\n",
    "print(f'95% CI: [{pred_ci[0]:.3f}, {pred_ci[1]:.3f}]')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(bootstrap_preds, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(pred_mean, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.axvline(pred_ci[0], color='orange', linestyle=':', linewidth=2, label='95% CI')\n",
    "ax.axvline(pred_ci[1], color='orange', linestyle=':', linewidth=2)\n",
    "ax.set_xlabel('Predicted 5-year Risk')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Bootstrap Distribution of Risk Prediction')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from xgbse import XGBSEKaplanNeighbors\n",
    "    from xgbse.converters import convert_to_structured\n",
    "    \n",
    "    y_train_xgb = convert_to_structured(df_cox.loc[train_idx, T], df_cox.loc[train_idx, E])\n",
    "    y_test_xgb = convert_to_structured(df_cox.loc[test_idx, T], df_cox.loc[test_idx, E])\n",
    "    \n",
    "    xgbse = XGBSEKaplanNeighbors(\n",
    "        n_neighbors=50,\n",
    "        xgb_params={\n",
    "            'objective': 'survival:aft',\n",
    "            'eval_metric': 'aft-nloglik',\n",
    "            'aft_loss_distribution': 'normal',\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.01,\n",
    "            'n_estimators': 500,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,  \n",
    "            'random_state': SEED,\n",
    "            'n_jobs': -1  \n",
    "        }\n",
    "    )\n",
    "    \n",
    "    xgbse.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_xgb,\n",
    "        num_boost_round=100,\n",
    "        validation_data=(X_test_scaled, y_test_xgb),\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    risk_train = -xgbse.predict(X_train_scaled).mean(axis=1)\n",
    "    \n",
    "    ci_xgb_train = concordance_index_censored(\n",
    "        y_train_xgb[y_train_xgb.dtype.names[0]],\n",
    "        y_train_xgb[y_train_xgb.dtype.names[1]],\n",
    "        risk_train\n",
    "    )[0]\n",
    "\n",
    "\n",
    "    risk_test = -xgbse.predict(X_test_scaled).mean(axis=1)\n",
    "\n",
    "    ci_xgb_test = concordance_index_censored(\n",
    "        y_test_xgb[y_test_xgb.dtype.names[0]],\n",
    "        y_test_xgb[y_test_xgb.dtype.names[1]],\n",
    "        risk_test\n",
    "    )[0]\n",
    "    \n",
    "    print(f'\\nXGBSE C-index: train={ci_xgb_train:.4f}')\n",
    "    print(f'XGBSE C-index: test={ci_xgb_test:.4f}')\n",
    "    \n",
    "except ImportError:\n",
    "    print('XGBSE not installed, skipping')\n",
    "    ci_xgb_test = None\n",
    "except Exception as e:\n",
    "    print(f'XGBSE error: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    ci_xgb_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# Model Comparison (C-index)\n",
    "# ========================================================================\n",
    "\n",
    "print('MODEL COMPARISON (C-INDEX)')\n",
    "\n",
    "print(f'\\n{\"Model\":<15} {\"Train\":>8} {\"Test\":>8} {\"Overfit\":>8}')\n",
    "\n",
    "# Cox PH\n",
    "print(f'{\"Cox PH\":<15} {ci_cox_train:>8.4f} {ci_cox_test:>8.4f} {ci_cox_train-ci_cox_test:>+8.4f}')\n",
    "\n",
    "# RSF\n",
    "print(f'{\"RSF\":<15} {ci_rsf_train:>8.4f} {ci_rsf_test:>8.4f} {ci_rsf_train-ci_rsf_test:>+8.4f}')\n",
    "\n",
    "# XGBSE\n",
    "if 'ci_xgb_test' in locals() and ci_xgb_test is not None:\n",
    "    print(f'{\"XGBSE\":<15} {ci_xgb_train:>8.4f} {ci_xgb_test:>8.4f} {ci_xgb_train-ci_xgb_test:>+8.4f}')\n",
    "\n",
    "# Best model\n",
    "models = [\n",
    "    ('Cox PH', ci_cox_test),\n",
    "    ('RSF', ci_rsf_test),\n",
    "]\n",
    "if 'ci_xgb_test' in locals() and ci_xgb_test is not None:\n",
    "    models.append(('XGBSE', ci_xgb_test))\n",
    "\n",
    "best_model = max(models, key=lambda x: x[1])\n",
    "print(f'Best Model: {best_model[0]} (C-index: {best_model[1]:.4f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.3 Neural Network: FastCPH (Cox-based Deep Learning)\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "\n",
    "device = torch.device('mps')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Minimal Network: n_features → 16 → 1 (single hidden layer!)\n",
    "# -----------------------------------------------------------------------------\n",
    "class FastCPHNet(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data\n",
    "# -----------------------------------------------------------------------------\n",
    "# Keep data on CPU initially to save GPU memory\n",
    "X_tr = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_te = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "T_tr = torch.tensor(y_t_train.astype(float), dtype=torch.float32)\n",
    "E_tr = torch.tensor(y_e_train.astype(float), dtype=torch.float32)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cox Loss\n",
    "# -----------------------------------------------------------------------------\n",
    "def cox_loss(log_h, T, E):\n",
    "    # T and E must be on the same device as log_h\n",
    "    idx = torch.argsort(T, descending=True)\n",
    "    log_h, E = log_h[idx], E[idx]\n",
    "    log_risk = torch.logcumsumexp(log_h, dim=0)\n",
    "    uncensored = log_h[E == 1] - log_risk[E == 1]\n",
    "    return -uncensored.mean() if uncensored.numel() > 0 else torch.tensor(0.0, device=log_h.device, requires_grad=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Training (MINIMAL)\n",
    "# -----------------------------------------------------------------------------\n",
    "model = FastCPHNet(X_tr.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_tr, T_tr, E_tr),\n",
    "    batch_size=2048,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "n_epochs = 5\n",
    "print(f\"Training {sum(p.numel() for p in model.parameters()):,} params for {n_epochs} epochs...\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, T_batch, E_batch in train_loader:\n",
    "        # Move batch to device\n",
    "        X_batch = X_batch.to(device)\n",
    "        T_batch = T_batch.to(device)\n",
    "        E_batch = E_batch.to(device)\n",
    "        \n",
    "        loss = cox_loss(model(X_batch), T_batch, E_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch == 0 or epoch == n_epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\" Done in {time.time()-start:.1f}s\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Move input to device, then move output back to cpu for numpy\n",
    "    risk_train = model(X_tr.to(device)).cpu().numpy()\n",
    "    risk_test = model(X_te.to(device)).cpu().numpy()\n",
    "    \n",
    "    ci_fastcph_train = concordance_index_censored(y_e_train.astype(bool), y_t_train, risk_train)[0]\n",
    "    ci_fastcph_test = concordance_index_censored(y_e_test.astype(bool), y_t_test, risk_test)[0]\n",
    "\n",
    "print('FASTCPH NEURAL NETWORK')\n",
    "print(f'Architecture: {X_tr.shape[1]} → 16 → 1 (single hidden layer)')\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Training: {n_epochs} epochs in {time.time()-start:.1f}s')\n",
    "print(f'\\nC-index:')\n",
    "print(f' Train: {ci_fastcph_train:.4f}')\n",
    "print(f' Test: {ci_fastcph_test:.4f}')\n",
    "print(f' Gap: {(ci_fastcph_train-ci_fastcph_test)*100:+.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = df_cox.copy()\n",
    "df_class['outcome'] = df_model[E_TYPE].map({0: 'Censored', 1: 'Exit', 2: 'Exit', 3: 'Failure'})\n",
    "df_class = df_class[df_class['outcome'] != 'Censored'].copy()\n",
    "df_class['y'] = (df_class['outcome'] == 'Exit').astype(int)\n",
    "\n",
    "print(f'Classification set: {len(df_class)}')\n",
    "print(df_class['outcome'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_class = train_idx.intersection(df_class.index)\n",
    "test_idx_class = test_idx.intersection(df_class.index)\n",
    "\n",
    "X_tr = df_class.loc[train_idx_class].drop(columns=[T, E, E_TYPE, 'outcome', 'y'])\n",
    "X_te = df_class.loc[test_idx_class].drop(columns=[T, E, E_TYPE, 'outcome', 'y'])\n",
    "y_tr = df_class.loc[train_idx_class, 'y']\n",
    "y_te = df_class.loc[test_idx_class, 'y']\n",
    "\n",
    "scaler_c = StandardScaler()\n",
    "X_tr_s = scaler_c.fit_transform(X_tr)\n",
    "X_te_s = scaler_c.transform(X_te)\n",
    "\n",
    "print(f'Train: {len(X_tr)}, Test: {len(X_te)}, Exit rate: {y_tr.mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clf(model, X_tr, X_te, y_tr, y_te, name):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_prob = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    res = {\n",
    "        'Model': name,\n",
    "        'Acc': accuracy_score(y_te, y_pred),\n",
    "        'Prec': precision_score(y_te, y_pred),\n",
    "        'Rec': recall_score(y_te, y_pred),\n",
    "        'F1': f1_score(y_te, y_pred),\n",
    "        'AUC': roc_auc_score(y_te, y_prob)\n",
    "    }\n",
    "    return res, y_prob, confusion_matrix(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
    "res1, prob1, cm1 = eval_clf(rf1, X_tr_s, X_te_s, y_tr, y_te, 'RF Unbalanced')\n",
    "\n",
    "print(f'{res1[\"Model\"]}: Acc={res1[\"Acc\"]:.3f}, Prec={res1[\"Prec\"]:.3f}, Rec={res1[\"Rec\"]:.3f}, F1={res1[\"F1\"]:.3f}, AUC={res1[\"AUC\"]:.3f}')\n",
    "print(f'Confusion matrix:\\n{cm1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=SEED, n_jobs=-1)\n",
    "res2, prob2, cm2 = eval_clf(rf2, X_tr_s, X_te_s, y_tr, y_te, 'RF Balanced')\n",
    "\n",
    "print(f'{res2[\"Model\"]}: Acc={res2[\"Acc\"]:.3f}, Prec={res2[\"Prec\"]:.3f}, Rec={res2[\"Rec\"]:.3f}, F1={res2[\"F1\"]:.3f}, AUC={res2[\"AUC\"]:.3f}')\n",
    "print(f'Confusion matrix:\\n{cm2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=SEED)\n",
    "res3, prob3, cm3 = eval_clf(gb, X_tr_s, X_te_s, y_tr, y_te, 'GradientBoosting')\n",
    "\n",
    "print(f'{res3[\"Model\"]}: Acc={res3[\"Acc\"]:.3f}, Prec={res3[\"Prec\"]:.3f}, Rec={res3[\"Rec\"]:.3f}, F1={res3[\"F1\"]:.3f}, AUC={res3[\"AUC\"]:.3f}')\n",
    "print(f'Confusion matrix:\\n{cm3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [10, 20],\n",
    "    'min_samples_leaf': [5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=SEED, n_jobs=-1),\n",
    "    param_grid, cv=5, scoring='f1', n_jobs=-1\n",
    ")\n",
    "grid.fit(X_tr_s, y_tr)\n",
    "\n",
    "print(f'Best params: {grid.best_params_}')\n",
    "print(f'Best CV F1: {grid.best_score_:.4f}')\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "y_pred_train = best_rf.predict(X_tr_s)\n",
    "y_pred_test = best_rf.predict(X_te_s)\n",
    "\n",
    "print(f'Train F1: {f1_score(y_tr, y_pred_train):.4f}')\n",
    "print(f'Test F1: {f1_score(y_te, y_pred_test):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid.best_estimator_\n",
    "res_best, prob_best, cm_best = eval_clf(best_rf, X_tr_s, X_te_s, y_tr, y_te, 'RF Tuned')\n",
    "\n",
    "print(f'{res_best[\"Model\"]}: Acc={res_best[\"Acc\"]:.3f}, Prec={res_best[\"Prec\"]:.3f}, Rec={res_best[\"Rec\"]:.3f}, F1={res_best[\"F1\"]:.3f}, AUC={res_best[\"AUC\"]:.3f}')\n",
    "print(f'Confusion matrix:\\n{cm_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "prec, rec, thresholds = precision_recall_curve(y_te, prob_best)\n",
    "f1_scores = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-8)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_thresh = thresholds[best_idx]\n",
    "\n",
    "axes[0].plot(rec, prec, 'b-', lw=2)\n",
    "axes[0].scatter([rec[best_idx]], [prec[best_idx]], color='red', s=100, zorder=5)\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title(f'PR Curve (best threshold={best_thresh:.2f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_te, prob_best)\n",
    "auc = roc_auc_score(y_te, prob_best)\n",
    "axes[1].plot(fpr, tpr, 'b-', lw=2, label=f'AUC={auc:.3f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[1].set_xlabel('FPR')\n",
    "axes[1].set_ylabel('TPR')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Optimal threshold: {best_thresh:.4f}')\n",
    "print(f'At threshold: Prec={prec[best_idx]:.3f}, Rec={rec[best_idx]:.3f}, F1={f1_scores[best_idx]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_results = pd.DataFrame([res1, res2, res3, res_best])\n",
    "print('Classification Model Comparison:')\n",
    "print(clf_results.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "models = {\n",
    "    'RF Unbalanced': rf1,\n",
    "    'RF Balanced': rf2,\n",
    "    'Gradient Boosting': gb,\n",
    "    'RF Tuned': best_rf\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name.upper())\n",
    "    \n",
    "    y_pred = model.predict(X_te_s)\n",
    "    \n",
    "    f1 = f1_score(y_te, y_pred, average='macro')\n",
    "    \n",
    "    print(f'Precision:{precision_score(y_te, y_pred):.4f}')\n",
    "    print(f'Recall:{recall_score(y_te, y_pred):.4f}')\n",
    "    print(f'F1 Score:{f1:.4f}')\n",
    "    print(f'\\nReport:\\n{classification_report(y_te, y_pred, digits=4)}')\n",
    "    \n",
    "    cm = confusion_matrix(y_te, y_pred)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',\n",
    "                xticklabels=['Censored', 'Event'],\n",
    "                yticklabels=['Censored', 'Event'],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},\n",
    "                linewidths=0,\n",
    "                linecolor='white',\n",
    "                cbar=True,\n",
    "                ax=ax)\n",
    "    ax.set_xlabel('Predicted label', fontsize=12)\n",
    "    ax.set_ylabel('True label', fontsize=12)\n",
    "    ax.set_title(f'Confusion Matrix - {name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('_' * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_imp = cph_full.summary['coef'].abs() \n",
    "cox_imp = cox_imp / cox_imp.max()\n",
    "\n",
    "rsf_imp_s = rsf_imp.set_index('Feature')['Importance']\n",
    "rsf_imp_s = rsf_imp_s / rsf_imp_s.max()\n",
    "\n",
    "rf_imp = pd.Series(best_rf.feature_importances_, index=X_tr.columns)  \n",
    "rf_imp = rf_imp / rf_imp.max()\n",
    "\n",
    "imp_df = pd.DataFrame({'Cox': cox_imp, 'RSF': rsf_imp_s, 'RF': rf_imp}).fillna(0)\n",
    "imp_df['Avg'] = imp_df.mean(axis=1)\n",
    "imp_df = imp_df.sort_values('Avg', ascending=False)\n",
    "\n",
    "print('Feature Importance (normalized):')\n",
    "print(imp_df.head(12).round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_f = imp_df.head(12).index\n",
    "plot_d = imp_df.loc[top_f, ['Cox', 'RSF', 'RF']]\n",
    "x = np.arange(len(top_f))\n",
    "w = 0.25\n",
    "\n",
    "ax.barh(x - w, plot_d['Cox'], w, label='Cox PH', color='#3498db')\n",
    "ax.barh(x, plot_d['RSF'], w, label='RSF', color='#2ecc71')\n",
    "ax.barh(x + w, plot_d['RF'], w, label='RF Class', color='#e74c3c')\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(top_f)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Normalized Importance')\n",
    "ax.set_title('Feature Importance Comparison')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(f'FINAL RESULTS SUMMARY')\n",
    "\n",
    "print(f'\\nDATASET')\n",
    "print(f'- Total samples: {len(df):,}')\n",
    "print(f'- Train: {len(train_idx):,} ({y_e_train.mean():.1%} events)')  \n",
    "print(f'- Test: {len(test_idx):,} ({y_e_test.mean():.1%} events)')  \n",
    "print(f'- Censored: {(df[E_TYPE]==0).sum():,} ({100*(df[E_TYPE]==0).mean():.1f}%)')\n",
    "print(f'- IPO: {(df[E_TYPE]==1).sum():,} ({100*(df[E_TYPE]==1).mean():.1f}%)')\n",
    "print(f'- M&A: {(df[E_TYPE]==2).sum():,} ({100*(df[E_TYPE]==2).mean():.1f}%)')\n",
    "print(f'- Failure: {(df[E_TYPE]==3).sum():,} ({100*(df[E_TYPE]==3).mean():.1f}%)')\n",
    "\n",
    "print(f'\\nSURVIVAL MODELS (C-index)')\n",
    "print(f'{\"Model\":<15} {\"Train\":>8} {\"Test\":>8} {\"Overfit\":>8}')\n",
    "print(f'{\"-\"*42}')\n",
    "print(f'{\"Cox PH\":<15} {ci_cox_train:>8.4f} {ci_cox_test:>8.4f} {ci_cox_train-ci_cox_test:>+8.4f}')\n",
    "print(f'{\"RSF\":<15} {ci_rsf_train:>8.4f} {ci_rsf_test:>8.4f} {ci_rsf_train-ci_rsf_test:>+8.4f}')\n",
    "if 'ci_xgb_test' in locals() and ci_xgb_test:\n",
    "    print(f'{\"XGBSE\":<15} {ci_xgb_train:>8.4f} {ci_xgb_test:>8.4f} {ci_xgb_train-ci_xgb_test:>+8.4f}')\n",
    "\n",
    "print(f'\\nMODEL CALIBRATION')\n",
    "print(f'- Integrated Brier Score: {ibs:.4f}')\n",
    "print(f'- Mean Brier Score: {np.mean(brier_scores):.4f}')\n",
    "print(f'- Mean Calibration Error (5y): {np.mean(np.abs(fraction_of_positives - mean_predicted_value)):.4f}')\n",
    "\n",
    "print(f'\\nCLASSIFICATION (Best Model: {res_best[\"Model\"]})')\n",
    "print(f'- Accuracy: {res_best[\"Acc\"]:.4f}')\n",
    "print(f'- Precision: {res_best[\"Prec\"]:.4f}')\n",
    "print(f'- Recall: {res_best[\"Rec\"]:.4f}')\n",
    "print(f'- F1: {res_best[\"F1\"]:.4f}')\n",
    "print(f'- AUC: {res_best[\"AUC\"]:.4f}')\n",
    "\n",
    "print(f'\\nTOP 5 FEATURES (Average Importance)')\n",
    "for i, (f, _) in enumerate(imp_df.head(5).iterrows(), 1):\n",
    "    print(f'{i}. {f}')\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# SAVE RESULTS (JSON COMPLETO)\n",
    "# ========================================================================\n",
    "\n",
    "results = {\n",
    "    'survival': {\n",
    "        'cox': {\n",
    "            'train': float(ci_cox_train),\n",
    "            'test': float(ci_cox_test),\n",
    "            'overfit': float(ci_cox_train - ci_cox_test)\n",
    "        },\n",
    "        'rsf': {\n",
    "            'train': float(ci_rsf_train),\n",
    "            'test': float(ci_rsf_test),\n",
    "            'overfit': float(ci_rsf_train - ci_rsf_test)\n",
    "        },\n",
    "        'xgbse': {\n",
    "            'train': float(ci_xgb_train) if 'ci_xgb_train' in locals() and ci_xgb_train else None,\n",
    "            'test': float(ci_xgb_test) if 'ci_xgb_test' in locals() and ci_xgb_test else None,\n",
    "            'overfit': float(ci_xgb_train - ci_xgb_test) if 'ci_xgb_train' in locals() and ci_xgb_train else None\n",
    "        }\n",
    "    },\n",
    "    'calibration': {\n",
    "        'ibs': float(ibs),\n",
    "        'brier_mean': float(np.mean(brier_scores)),\n",
    "        'calibration_mae_5y': float(np.mean(np.abs(fraction_of_positives - mean_predicted_value)))\n",
    "    },\n",
    "    'classification': clf_results.to_dict('records'),\n",
    "    'feature_importance': {\n",
    "        k: {k2: float(v2) for k2, v2 in v.items()} \n",
    "        for k, v in imp_df.head(10).to_dict().items()\n",
    "    },\n",
    "    'dataset': {\n",
    "        'n_samples': int(len(df)),\n",
    "        'n_train': int(len(train_idx)), \n",
    "        'n_test': int(len(test_idx)),   \n",
    "        'n_events': int(df[E].sum()),\n",
    "        'event_rate': float(df[E].mean()),\n",
    "        'event_distribution': {\n",
    "            'Censored': int((df[E_TYPE] == 0).sum()),\n",
    "            'IPO': int((df[E_TYPE] == 1).sum()),\n",
    "            'M&A': int((df[E_TYPE] == 2).sum()),\n",
    "            'Failure': int((df[E_TYPE] == 3).sum())\n",
    "        },\n",
    "        'event_percentages': {\n",
    "            'Censored': float((df[E_TYPE] == 0).mean()),\n",
    "            'IPO': float((df[E_TYPE] == 1).mean()),\n",
    "            'M&A': float((df[E_TYPE] == 2).mean()),\n",
    "            'Failure': float((df[E_TYPE] == 3).mean())\n",
    "        }\n",
    "    },\n",
    "    'best_model': {\n",
    "        'name': 'Random Survival Forest',\n",
    "        'test_c_index': float(ci_rsf_test),\n",
    "        'overfit_gap': float(ci_rsf_train - ci_rsf_test),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save outputs\n",
    "config.OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset_path = config.OUTPUT_PATH / 'finale_usa_cleaned.csv'\n",
    "df.to_csv(dataset_path, index=False)\n",
    "\n",
    "results_path = config.OUTPUT_PATH / 'model_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "print('ANALYSIS COMPLETE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
