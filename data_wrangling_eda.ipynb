{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival Analysis of VC-Backed Startups\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Daniele Parini \n",
    "**Date:** 2025  \n",
    "**Institution:** HEC-Lausanne\n",
    "\n",
    "---\n",
    "\n",
    "### Abstract\n",
    "\n",
    "This notebook implements a comprehensive survival analysis framework to predict the probability and timing of exit events (IPO or acquisition) for VC-backed startups. We employ a competing risks methodology where IPO and M&A are treated as mutually exclusive terminal events.\n",
    "\n",
    "### Methodology Overview\n",
    "\n",
    "1. **Data Preparation**: Crunchbase and Jay Ritter datasets\n",
    "2. **Exploratory Analysis**: Kaplan-Meier curves, cumulative incidence functions\n",
    "3. **Statistical Models**: Cox Proportional Hazards with stratification\n",
    "4. **Parametric Models**: Weibull, Log-Normal, Log-Logistic AFT\n",
    "5. **Machine Learning**: Random Survival Forest, Gradient Boosting, DeepSurv\n",
    "6. **Model Evaluation**: C-index, Brier Score, calibration plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Environment Setup & Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "# Autoreload everything\n",
    "%autoreload 2\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Survival analysis\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# Statistical testing\n",
    "from scipy import stats\n",
    "\n",
    "# ML preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Initial Inspection\n",
    "\n",
    "### Objectives of this section:\n",
    "1. **Load** the Crunchbase and Jay Ritter datasets\n",
    "2. **Explore** the structure, quality, and distributions of the data\n",
    "3. **Analyze** funding, success rates, and correlations\n",
    "4. **Merge** the datasets using a conservative strategy to validate IPO events\n",
    "5. **Identify** key features for survival analysis\n",
    "\n",
    "### Datasets used:\n",
    "| Dataset | Description | Primary use |\n",
    "|---------|-------------|----------------|\n",
    "| Crunchbase | ~196K companies (1970-2013) | Main dataset for survival |\n",
    "| Jay Ritter | ~15,700 IPO USA (1975-2025) | IPO validation + market variables |\n",
    "\n",
    "### 2.1 Setup and Path Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "config.DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root: {config.PROJECT_ROOT}\")\n",
    "print(f\"Data Directory: {config.DATA_DIR}\")\n",
    "\n",
    "config.RAW_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"justinas/startup-investments\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(f\"\\nCopying files to: {config.RAW_DIR}\")\n",
    "\n",
    "for item in os.listdir(path):\n",
    "    source_path = os.path.join(path, item)\n",
    "    dest_path = os.path.join(config.RAW_DIR, item)\n",
    "\n",
    "    if os.path.isfile(source_path):\n",
    "        shutil.copy2(source_path, dest_path)\n",
    "        print(f\"  Copied: {item}\")\n",
    "    elif os.path.isdir(source_path):\n",
    "        # If it's a directory, copy entire directory\n",
    "        shutil.copytree(source_path, dest_path, dirs_exist_ok=True)\n",
    "        print(f\"  Copied directory: {item}\")\n",
    "\n",
    "print(f\"\\n Dataset files saved in: {config.RAW_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.1 SETUP E DEFINIZIONE PATHS\n",
    "# =============================================================================\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Paths configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "config.RAW_DIR = config.DATA_DIR / \"raw\"\n",
    "\n",
    "print(\"ENVIRONMENT CONFIGURATION\")\n",
    "\n",
    "print(f\" Raw data path: {config.RAW_DIR}\")\n",
    "\n",
    "# Verifica esistenza directory\n",
    "if os.path.exists(config.RAW_DIR):\n",
    "    files = [f for f in os.listdir(config.RAW_DIR) if f.endswith((\".csv\", \".xlsx\"))]\n",
    "    print(f\"\\nDirectory trovata con {len(files)} file dati:\")\n",
    "    for f in sorted(files):\n",
    "        size_mb = os.path.getsize(os.path.join(config.RAW_DIR, f)) / (1024**2)\n",
    "        print(f\" {f} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"\\n ERROR: Directory not found: {config.RAW_DIR}\")\n",
    "    print(\"First, run the kagglehub download script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.2 CARICAMENTO DATASET CRUNCHBASE\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def load_csv_safe(filepath, parse_dates=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Load a CSV file with error handling and logging.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Full path to the CSV file\n",
    "    parse_dates : list of str, optional\n",
    "        Column names to parse as datetime objects\n",
    "    **kwargs : dict\n",
    "        Additional keyword arguments passed to pd.read_csv\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        Loaded dataframe if successful, None if error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, parse_dates=parse_dates, low_memory=False, **kwargs)\n",
    "        filename = os.path.basename(filepath)\n",
    "        print(f\" {filename}: {df.shape[0]:,} righe x {df.shape[1]} colonne\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Crunchbase file upload\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING CRUNCHBASE DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "crunchbase = {}\n",
    "\n",
    "# Critical files for survival analysis\n",
    "critical_files = {\n",
    "    \"objects\": {\n",
    "        \"parse_dates\": [\n",
    "            \"founded_at\",\n",
    "            \"closed_at\",\n",
    "            \"first_funding_at\",\n",
    "            \"last_funding_at\",\n",
    "            \"first_milestone_at\",\n",
    "        ]\n",
    "    },\n",
    "    \"funding_rounds\": {\"parse_dates\": [\"funded_at\", \"created_at\"]},\n",
    "    \"acquisitions\": {\"parse_dates\": [\"acquired_at\", \"created_at\"]},\n",
    "    \"ipos\": {\"parse_dates\": [\"public_at\", \"created_at\"]},\n",
    "}\n",
    "\n",
    "# Supplementary files for enrichment\n",
    "supplementary_files = {\n",
    "    \"offices\": {},\n",
    "    \"investments\": {},\n",
    "    \"people\": {},\n",
    "    \"relationships\": {},\n",
    "    \"degrees\": {},\n",
    "    \"funds\": {\"parse_dates\": [\"funded_at\"]},\n",
    "    \"milestones\": {\"parse_dates\": [\"milestone_at\"]},\n",
    "}\n",
    "\n",
    "print(\"\\n--- Critical files ---\")\n",
    "for name, params in critical_files.items():\n",
    "    filepath = os.path.join(config.RAW_DIR, f\"{name}.csv\")\n",
    "    crunchbase[name] = load_csv_safe(filepath, **params)\n",
    "\n",
    "print(\"\\n--- Supplementary files ---\")\n",
    "for name, params in supplementary_files.items():\n",
    "    filepath = os.path.join(config.RAW_DIR, f\"{name}.csv\")\n",
    "    crunchbase[name] = load_csv_safe(filepath, **params)\n",
    "\n",
    "# Summary statistics\n",
    "loaded_count = sum(1 for v in crunchbase.values() if v is not None)\n",
    "print(f\"\\n File loaded: {loaded_count}/{len(crunchbase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.3 CARICAMENTO DATASET JAY RITTER\n",
    "# =============================================================================\n",
    "\n",
    "import requests\n",
    "\n",
    "# 1. Check if file exists; if not, download it\n",
    "if not config.RITTER_PATH.exists():\n",
    "    print(f\"File non trovato: {config.RITTER_PATH}\")\n",
    "    print(f\"Tentativo di download da: {config.RITTER_URL} ...\")\n",
    "\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        config.RITTER_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Download the file\n",
    "        response = requests.get(config.RITTER_URL, timeout=30)\n",
    "        response.raise_for_status()  # Raises error if status code is not 200 (OK)\n",
    "\n",
    "        with open(config.RITTER_PATH, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(\"Download completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Errore critico durante il download: {e}\")\n",
    "        # We don't stop here; we let the next block handle the missing file error gracefully\n",
    "\n",
    "# 2. Attempt to load the data\n",
    "try:\n",
    "    ritter = pd.read_excel(config.RITTER_PATH)\n",
    "    print(f\" IPO-age.xlsx: {ritter.shape[0]:,} righe × {ritter.shape[1]} colonne\")\n",
    "    print(f\"\\n Colonne originali:\")\n",
    "    print(f\"   {ritter.columns.tolist()}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # This block runs if the download failed or was skipped\n",
    "    print(f\" File non trovato e download fallito: {config.RITTER_PATH}\")\n",
    "    ritter = None\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Errore caricamento: {e}\")\n",
    "    ritter = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Dataset Exploration: Crunchbase Objects\n",
    "\n",
    "The `objects.csv` file is the **central table** of the Crunchbase database. It contains all entities:\n",
    "- Companies (companies/startups)\n",
    "- People (founders, executives)\n",
    "- Financial Organizations (VC, PE, angels)\n",
    "\n",
    "**Focus:** We will filter only `Companies` for survival analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.4 ESPLORATION CRUNCHBASE OBJECTS\n",
    "# =============================================================================\n",
    "\n",
    "if crunchbase.get(\"objects\") is not None:\n",
    "    objects = crunchbase[\"objects\"]\n",
    "\n",
    "    print(\"CRUNCHBASE OBJECTS - STRUCTURE\")\n",
    "\n",
    "    # General Info\n",
    "    print(f\"\\n Shape: {objects.shape}\")\n",
    "    print(f\"\\n Info DataFrame:\")\n",
    "    objects.info(verbose=True, show_counts=True)\n",
    "\n",
    "else:\n",
    "    print(\"objects.csv not loaded!\")\n",
    "    objects = None\n",
    "\n",
    "# Distribuzione Entity Types\n",
    "if objects is not None:\n",
    "\n",
    "    print(\"DISTRIBUTION ENTITY TYPES\")\n",
    "\n",
    "    entity_dist = objects[\"entity_type\"].value_counts()\n",
    "    print(entity_dist)\n",
    "\n",
    "# Filtra solo Companies\n",
    "if objects is not None:\n",
    "    companies = objects[objects[\"entity_type\"] == \"Company\"].copy()\n",
    "\n",
    "    \n",
    "    print(f\"COMPANIES SUBSET: {len(companies):,} companies\")\n",
    "    \n",
    "\n",
    "    # Status distribution\n",
    "    print(\"\\n Distribution Status:\")\n",
    "    status_dist = companies[\"status\"].value_counts()\n",
    "    print(status_dist)\n",
    "\n",
    "    # Calcola percentuali per survival\n",
    "    total = len(companies)\n",
    "    operating = status_dist.get(\"operating\", 0)\n",
    "    acquired = status_dist.get(\"acquired\", 0)\n",
    "    ipo = status_dist.get(\"ipo\", 0)\n",
    "    closed = status_dist.get(\"closed\", 0)\n",
    "\n",
    "    print(f\"\\n Interpretation for Survival Analysis:\")\n",
    "    print(f\"Operating (censored): {operating:,} ({100 * operating / total:.1f}%)\")\n",
    "    print(f\"Acquired (event M&A): {acquired:,} ({100 * acquired / total:.1f}%)\")\n",
    "    print(f\"IPO (event IPO): {ipo:,} ({100 * ipo / total:.1f}%)\")\n",
    "    print(f\"Closed (censored/failure): {closed:,} ({100 * closed / total:.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYSIS OF ZERO VALUES - CRUNCHBASE COMPANIES\n",
    "# =============================================================================\n",
    "\n",
    "if objects is not None:\n",
    "    \n",
    "    print(\"ANALYSIS OF ZERO VALUES - COMPANIES\")\n",
    "    \n",
    "\n",
    "    # Calculate zero percentages for relevant columns\n",
    "    key_columns = [\n",
    "        \"name\",\n",
    "        \"category_code\",\n",
    "        \"status\",\n",
    "        \"founded_at\",\n",
    "        \"closed_at\",\n",
    "        \"country_code\",\n",
    "        \"state_code\",\n",
    "        \"city\",\n",
    "        \"funding_total_usd\",\n",
    "        \"funding_rounds\",\n",
    "        \"first_funding_at\",\n",
    "        \"last_funding_at\",\n",
    "    ]\n",
    "\n",
    "    null_analysis = []\n",
    "    for col in key_columns:\n",
    "        if col in companies.columns:\n",
    "            null_count = companies[col].isnull().sum()\n",
    "            null_pct = 100 * null_count / len(companies)\n",
    "            null_analysis.append(\n",
    "                {\n",
    "                    \"Column\": col,\n",
    "                    \"Null Count\": null_count,\n",
    "                    \"Null %\": null_pct,\n",
    "                    \"Non-Null\": len(companies) - null_count,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    null_df = pd.DataFrame(null_analysis).sort_values(\"Null %\", ascending=False)\n",
    "    print(null_df.to_string(index=False))\n",
    "\n",
    "    # Heatmap valori nulli\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Seleziona colonne per heatmap\n",
    "    heatmap_cols = [col for col in key_columns if col in companies.columns]\n",
    "    null_matrix = companies[heatmap_cols].isnull().astype(int)\n",
    "\n",
    "    # Sample per visualizzazione (troppi dati per heatmap completa)\n",
    "    sample_size = min(500, len(null_matrix))\n",
    "    null_sample = null_matrix.sample(sample_size, random_state=42)\n",
    "\n",
    "    sns.heatmap(\n",
    "        null_sample.T,\n",
    "        cmap=\"YlOrRd\",\n",
    "        cbar_kws={\"label\": \"Missing (1=Null)\"},\n",
    "        yticklabels=True,\n",
    "        xticklabels=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Pattern Null Values - Sample of {sample_size} Companies\", fontsize=14\n",
    "    )\n",
    "    ax.set_xlabel(\"Observations (sample)\")\n",
    "    ax.set_ylabel(\"Variables\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# DISTRIBUTION BY CATEGORY AND GEOGRAPHY\n",
    "# =============================================================================\n",
    "\n",
    "if objects is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    # Categories\n",
    "    ax1 = axes[0]\n",
    "    top_categories = companies[\"category_code\"].value_counts().head(42)\n",
    "    top_categories.plot(kind=\"barh\", ax=ax1, color=\"steelblue\")\n",
    "    ax1.set_title(\" Categories (Industries)\", fontsize=10)\n",
    "    ax1.set_xlabel(\"Number of Companies\")\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "    # Countries\n",
    "    ax2 = axes[1]\n",
    "    top_countries = companies[\"country_code\"].value_counts().head(40)\n",
    "    top_countries.plot(kind=\"barh\", ax=ax2, color=\"coral\")\n",
    "    ax2.set_title(\" Country\", fontsize=10)\n",
    "    ax2.set_xlabel(\"Number of Companies\")\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistiche\n",
    "    print(f\"\\n Unique Categories: {companies['category_code'].nunique()}\")\n",
    "    print(f\" Unique Country: {companies['country_code'].nunique()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DISTRIBUZIONE TEMPORALE - FOUNDING YEARS\n",
    "# =============================================================================\n",
    "\n",
    "if objects is not None:\n",
    "    # Estrai anno di fondazione\n",
    "    companies[\"founded_year\"] = pd.to_datetime(\n",
    "        companies[\"founded_at\"], errors=\"coerce\"\n",
    "    ).dt.year\n",
    "\n",
    "    # Filtra anni validi\n",
    "    valid_years = companies[\n",
    "        (companies[\"founded_year\"] >= 1950) & (companies[\"founded_year\"] <= 2015)\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Distribuzione founding years\n",
    "    ax1 = axes[0]\n",
    "    valid_years[\"founded_year\"].hist(\n",
    "        bins=50, ax=ax1, color=\"steelblue\", edgecolor=\"white\", alpha=0.7\n",
    "    )\n",
    "    ax1.set_title(\"Distribution Year of Foundation\", fontsize=14)\n",
    "    ax1.set_xlabel(\"Year\")\n",
    "    ax1.set_ylabel(\"Number of Companies\")\n",
    "    ax1.axvline(\n",
    "        valid_years[\"founded_year\"].median(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Median: {valid_years['founded_year'].median():.0f}\",\n",
    "    )\n",
    "    ax1.legend()\n",
    "\n",
    "    # Founding by decade\n",
    "    ax2 = axes[1]\n",
    "    valid_years[\"decade\"] = (valid_years[\"founded_year\"] // 10) * 10\n",
    "    decade_counts = valid_years[\"decade\"].value_counts().sort_index()\n",
    "    decade_counts.plot(kind=\"bar\", ax=ax2, color=\"coral\", edgecolor=\"black\")\n",
    "    ax2.set_title(\"Companies per Decade\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Decade\")\n",
    "    ax2.set_ylabel(\"Numbers of Companies\")\n",
    "    ax2.set_xticklabels([f\"{int(d)}s\" for d in decade_counts.index], rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n Range of years of foundation: {valid_years['founded_year'].min():.0f} - {valid_years['founded_year'].max():.0f}\")\n",
    "    print(f\" Median: {valid_years['founded_year'].median():.0f}\")\n",
    "    print(f\" Companies with founded_year valid: {len(valid_years):,} ({100 * len(valid_years) / len(companies):.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2.6 ESPLORAZIONE EXIT EVENTS - CRUNCHBASE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"CRUNCHBASE EXIT EVENTS\")\n",
    "\n",
    "# IPOs in Crunchbase\n",
    "if crunchbase.get(\"ipos\") is not None:\n",
    "    ipos_cb = crunchbase[\"ipos\"]\n",
    "\n",
    "    print(f\"\\n IPO (Crunchbase): {len(ipos_cb):,} record\")\n",
    "    print(f\"\\n Columns: {ipos_cb.columns.tolist()}\")\n",
    "\n",
    "    # Info\n",
    "    ipos_cb.info()\n",
    "\n",
    "    # Sample\n",
    "    print(\"Sample IPO records:\")\n",
    "\n",
    "    # Temporal distribution\n",
    "    ipos_cb[\"ipo_year\"] = pd.to_datetime(ipos_cb[\"public_at\"], errors=\"coerce\").dt.year\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ipos_cb[\"ipo_year\"].value_counts().sort_index().plot(\n",
    "        ax=ax, color=\"green\"\n",
    "    )\n",
    "    ax.set_title(\"IPO per Year (Crunchbase)\", fontsize=12)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Number IPO\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n Range IPO: {ipos_cb['ipo_year'].min():.0f} - {ipos_cb['ipo_year'].max():.0f}\")\n",
    "\n",
    "# Acquisitions in Crunchbase\n",
    "if crunchbase.get(\"acquisitions\") is not None:\n",
    "    acquisitions = crunchbase[\"acquisitions\"]\n",
    "\n",
    "    print(f\" ACQUISITIONS (Crunchbase): {len(acquisitions):,} record\")\n",
    "\n",
    "\n",
    "    print(f\"\\n Columns: {acquisitions.columns.tolist()}\")\n",
    "\n",
    "    # Sample\n",
    "    print(\"Sample acquisition records:\")\n",
    "    print(acquisitions.head().to_string())\n",
    "\n",
    "    # Temporal distribution\n",
    "    acquisitions[\"acq_year\"] = pd.to_datetime(\n",
    "        acquisitions[\"acquired_at\"], errors=\"coerce\"\n",
    "    ).dt.year\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    acquisitions[\"acq_year\"].value_counts().sort_index().plot(\n",
    "        ax=ax, color=\"red\"\n",
    "    )\n",
    "    ax.set_title(\"Acquisition per Year (Crunchbase)\", fontsize=12)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Number of Acquisitions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Price statistics\n",
    "    valid_prices = acquisitions[\"price_amount\"][acquisitions[\"price_amount\"] > 0]\n",
    "    print(f\"\\n Acquisition Price Statistics:\")\n",
    "    print(\n",
    "        f\"   Records with price: {len(valid_prices):,} ({100 * len(valid_prices) / len(acquisitions):.1f}%)\"\n",
    "    )\n",
    "    print(f\"   Mean: ${valid_prices.mean() / 1e6:.1f}M\")\n",
    "    print(f\"   Median: ${valid_prices.median() / 1e6:.1f}M\")\n",
    "    print(f\"   Max: ${valid_prices.max() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOADING DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "crunchbase = {}\n",
    "\n",
    "# ACQUISITIONS\n",
    "filepath = os.path.join(config.RAW_DIR, \"acquisitions.csv\")\n",
    "acquisitions = load_csv_safe(filepath, parse_dates=[\"acquired_at\"])\n",
    "if acquisitions is not None:\n",
    "    # Drop columns\n",
    "    acquisitions = acquisitions.drop(columns=[\n",
    "        'id', 'acquisition_id', 'created_at', 'updated_at', \n",
    "        'source_url', 'source_description'\n",
    "    ], errors='ignore')\n",
    "    acquisitions = acquisitions.drop_duplicates()\n",
    "    acquisitions['term_code'] = acquisitions['term_code'].astype('category')\n",
    "    acquisitions['price_currency_code'] = acquisitions['price_currency_code'].astype('category')\n",
    "\n",
    "# FUNDING_ROUNDS\n",
    "filepath = os.path.join(config.RAW_DIR, \"funding_rounds.csv\")\n",
    "funding_rounds = load_csv_safe(filepath, parse_dates=[\"funded_at\"])\n",
    "if funding_rounds is not None:\n",
    "    # Drop columns\n",
    "    funding_rounds = funding_rounds.drop(columns=[\n",
    "        'id', 'created_at', 'updated_at', 'created_by', 'raised_amount', \n",
    "        'raised_currency_code', 'pre_money_valuation', 'post_money_valuation_usd',\n",
    "        'pre_money_currency_code', 'post_money_valuation', 'pre_money_valuation_usd', \n",
    "        'post_money_currency_code', 'source_url'\n",
    "    ], errors='ignore')\n",
    "    funding_rounds = funding_rounds.drop_duplicates()\n",
    "    \n",
    "    # Type conversions\n",
    "    for col in ['funding_round_type', 'funding_round_code', 'is_first_round', 'is_last_round']:\n",
    "        if col in funding_rounds.columns:\n",
    "            funding_rounds[col] = funding_rounds[col].astype('category')\n",
    "    \n",
    "    # Fix misclassifications\n",
    "    funding_rounds.loc[\n",
    "        (funding_rounds['funding_round_code'] == 'angel') & \n",
    "        (funding_rounds['funding_round_type'] == 'series-a'), \n",
    "        'funding_round_type'\n",
    "    ] = 'angel'\n",
    "    \n",
    "    funding_rounds.loc[\n",
    "        (funding_rounds['funding_round_code'] == 'seed') & \n",
    "        (funding_rounds['funding_round_type'] == 'series-a'), \n",
    "        'funding_round_type'\n",
    "    ] = 'angel'\n",
    "\n",
    "# FUNDS\n",
    "filepath = os.path.join(config.RAW_DIR, \"funds.csv\")\n",
    "funds = load_csv_safe(filepath, parse_dates=[\"funded_at\"])\n",
    "if funds is not None:\n",
    "    funds = funds.drop(columns=['id', 'created_at', 'updated_at', 'source_url', 'source_description'], errors='ignore')\n",
    "    funds = funds.drop_duplicates()\n",
    "    funds['raised_currency_code'] = funds['raised_currency_code'].astype('category')\n",
    "\n",
    "# INVESTMENTS\n",
    "filepath = os.path.join(config.RAW_DIR, \"investments.csv\")\n",
    "investments = load_csv_safe(filepath)\n",
    "if investments is not None:\n",
    "    investments = investments.drop(columns=['id', 'created_at', 'updated_at'], errors='ignore')\n",
    "    investments = investments.drop_duplicates()\n",
    "\n",
    "# IPOS\n",
    "filepath = os.path.join(config.RAW_DIR, \"ipos.csv\")\n",
    "ipos = load_csv_safe(filepath, parse_dates=[\"public_at\"])\n",
    "if ipos is not None:\n",
    "    ipos = ipos.drop(columns=['id', 'created_at', 'updated_at', 'stock_symbol', 'source_url', 'source_description'], errors='ignore')\n",
    "    ipos = ipos.drop_duplicates()\n",
    "    ipos['valuation_currency_code'] = ipos['valuation_currency_code'].astype('category')\n",
    "    ipos['raised_currency_code'] = ipos['raised_currency_code'].astype('category')\n",
    "\n",
    "# MILESTONES\n",
    "filepath = os.path.join(config.RAW_DIR, \"milestones.csv\")\n",
    "milestones = load_csv_safe(filepath, parse_dates=[\"milestone_at\"])\n",
    "if milestones is not None:\n",
    "    milestones = milestones.drop(columns=['id', 'created_at', 'updated_at', 'source_url', 'milestone_code'], errors='ignore')\n",
    "    milestones = milestones.drop_duplicates()\n",
    "\n",
    "# OBJECTS\n",
    "filepath = os.path.join(config.RAW_DIR, \"objects.csv\")\n",
    "objects = load_csv_safe(\n",
    "    filepath, \n",
    "    parse_dates=['founded_at', 'closed_at', 'first_funding_at', 'last_funding_at',\n",
    "                 'first_milestone_at', 'last_milestone_at']\n",
    ")\n",
    "if objects is not None:\n",
    "    objects = objects.drop(columns=[\n",
    "        'normalized_name', 'permalink', 'created_at', 'updated_at',\n",
    "        'first_investment_at', 'last_investment_at', 'created_by',\n",
    "        'domain', 'twitter_username', 'logo_url', 'overview'\n",
    "    ], errors='ignore')\n",
    "    objects = objects.drop_duplicates()\n",
    "    \n",
    "    # Type conversions\n",
    "    for col in ['country_code', 'state_code', 'entity_type', 'category_code', 'status']:\n",
    "        if col in objects.columns:\n",
    "            objects[col] = objects[col].astype('category')\n",
    "    \n",
    "    for col in ['investment_rounds', 'invested_companies', 'funding_rounds', \n",
    "                'funding_total_usd', 'milestones', 'relationships']:\n",
    "        if col in objects.columns:\n",
    "            objects[col] = pd.to_numeric(objects[col], errors='coerce')\n",
    "\n",
    "# OFFICES\n",
    "filepath = os.path.join(config.RAW_DIR, \"offices.csv\")\n",
    "offices = load_csv_safe(filepath)\n",
    "if offices is not None:\n",
    "    offices = offices.drop(columns=['id', 'zip_code', 'created_at', 'updated_at'], errors='ignore')\n",
    "    offices = offices.drop_duplicates()\n",
    "    offices['state_code'] = offices['state_code'].astype('category')\n",
    "\n",
    "# PEOPLE\n",
    "filepath = os.path.join(config.RAW_DIR, \"people.csv\")\n",
    "people = load_csv_safe(filepath)\n",
    "if people is not None:\n",
    "    people = people.drop(columns=['id'], errors='ignore')\n",
    "    people = people.drop_duplicates()\n",
    "\n",
    "# RELATIONSHIPS\n",
    "filepath = os.path.join(config.RAW_DIR, \"relationships.csv\")\n",
    "relationships = load_csv_safe(filepath)\n",
    "if relationships is not None:\n",
    "    relationships = relationships.drop(columns=['id', 'sequence', 'created_at', 'updated_at'], errors='ignore')\n",
    "    relationships = relationships.drop_duplicates()\n",
    "    relationships['is_past'] = relationships['is_past'].astype('category')\n",
    "\n",
    "print(\"\\n All datasets loaded successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUBSETTING MAIN ENTITIES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"SUBSETTING MAIN ENTITIES\")\n",
    "\n",
    "# STARTUPS (Companies)\n",
    "STARTUPS = objects[\n",
    "    (objects['entity_type'] == 'Company') & \n",
    "    (objects['status'] != '') & \n",
    "    (~objects['country_code'].isin(['CSS', 'FST']))\n",
    "].copy()\n",
    "\n",
    "STARTUPS = STARTUPS.drop(columns=['entity_id'], errors='ignore')\n",
    "STARTUPS = STARTUPS.drop_duplicates()\n",
    "print(f\" STARTUPS: {len(STARTUPS):,} companies\")\n",
    "\n",
    "# FINANCIAL_ORG\n",
    "FINANCIAL_ORG = objects[objects['entity_type'] == 'FinancialOrg'].copy()\n",
    "FINANCIAL_ORG = FINANCIAL_ORG.drop(columns=[\n",
    "    'closed_at', 'entity_id', 'parent_id', 'category_code', 'status',\n",
    "    'funding_rounds', 'funding_total_usd', 'first_funding_at', 'last_funding_at',\n",
    "    'milestones', 'last_milestone_at', 'first_milestone_at'\n",
    "], errors='ignore')\n",
    "print(f\" FINANCIAL_ORG: {len(FINANCIAL_ORG):,} organizations\")\n",
    "\n",
    "# PRODUCT\n",
    "Product = objects[objects['entity_type'] == 'Product'].copy()\n",
    "Product = Product.drop(columns=[\n",
    "    'region', 'entity_type', 'investment_rounds', 'invested_companies',\n",
    "    'funding_rounds', 'first_funding_at', 'last_funding_at',\n",
    "    'funding_total_usd', 'relationships'\n",
    "], errors='ignore')\n",
    "print(f\" Product: {len(Product):,} products\")\n",
    "\n",
    "# PERSON\n",
    "Person = objects[objects['entity_type'] == 'Person'].copy()\n",
    "print(f\" Person: {len(Person):,} people\")\n",
    "\n",
    "# Date range\n",
    "if 'founded_at' in STARTUPS.columns:\n",
    "    min_date = STARTUPS['founded_at'].min()\n",
    "    max_date = STARTUPS['founded_at'].max()\n",
    "    print(f\"\\nDataset timespan: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\n All entities created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# USA MARKET ANALYSIS - STARTUP DEEP DIVE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"USA STARTUP MARKET ANALYSIS\")\n",
    "\n",
    "# Filter USA startups\n",
    "usa_startups = STARTUPS[STARTUPS['country_code'] == 'USA'].copy()\n",
    "\n",
    "# OVERVIEW\n",
    "print(\" OVERVIEW\")\n",
    "print(f\"Total USA startups: {len(usa_startups):,}\")\n",
    "print(f\"Percentage of dataset: {100 * len(usa_startups) / len(STARTUPS):.2f}%\")\n",
    "print(f\"Date range: {usa_startups['founded_at'].min().strftime('%Y')} - {usa_startups['founded_at'].max().strftime('%Y')}\")\n",
    "\n",
    "# STATUS DISTRIBUTION\n",
    "print(\"\\n STATUS DISTRIBUTION\")\n",
    "\n",
    "status_usa = usa_startups['status'].value_counts()\n",
    "status_usa_pct = usa_startups['status'].value_counts(normalize=True) * 100\n",
    "\n",
    "for status in status_usa.index:\n",
    "    count = status_usa[status]\n",
    "    pct = status_usa_pct[status]\n",
    "    bar = \"█\" * int(pct / 2)  # Visual bar\n",
    "    print(f\"{status:12s} {count:>7,} ({pct:>5.1f}%) {bar}\")\n",
    "\n",
    "# SUCCESS METRICS\n",
    "print(\"\\n SUCCESS METRICS\")\n",
    "\n",
    "success_rate = 100 * (status_usa.get('ipo', 0) + status_usa.get('acquired', 0)) / len(usa_startups)\n",
    "failure_rate = 100 * status_usa.get('closed', 0) / len(usa_startups)\n",
    "ipo_rate = 100 * status_usa.get('ipo', 0) / len(usa_startups)\n",
    "acquisition_rate = 100 * status_usa.get('acquired', 0) / len(usa_startups)\n",
    "\n",
    "print(f\"Success rate (IPO + M&A):  {success_rate:>6.2f}%\")\n",
    "print(f\"  ↳ IPO rate:              {ipo_rate:>6.2f}%\")\n",
    "print(f\"  ↳ Acquisition rate:      {acquisition_rate:>6.2f}%\")\n",
    "print(f\"Failure rate (Closed):     {failure_rate:>6.2f}%\")\n",
    "print(f\"Still operating:           {100 * status_usa.get('operating', 0) / len(usa_startups):>6.2f}%\")\n",
    "\n",
    "# TOP SECTORS IN USA\n",
    "print(\"\\n TOP 10 SECTORS IN USA\")\n",
    "\n",
    "usa_sectors = usa_startups['category_code'].value_counts().head(10)\n",
    "usa_sectors_pct = (usa_sectors / len(usa_startups)) * 100\n",
    "\n",
    "for i, (sector, count) in enumerate(usa_sectors.items(), 1):\n",
    "    pct = usa_sectors_pct[sector]\n",
    "    print(f\"{i:2d}. {sector:25s} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# TOP STATES\n",
    "if 'state_code' in usa_startups.columns:\n",
    "    print(\"\\n  TOP 10 STATES\")\n",
    "\n",
    "    usa_states = usa_startups['state_code'].value_counts().head(10)\n",
    "    usa_states_pct = (usa_states / len(usa_startups)) * 100\n",
    "    \n",
    "    for i, (state, count) in enumerate(usa_states.items(), 1):\n",
    "        pct = usa_states_pct[state]\n",
    "        print(f\"{i:2d}. {state:5s} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# FUNDING ANALYSIS\n",
    "print(\"\\n FUNDING STATISTICS (USA)\")\n",
    "\n",
    "usa_funded = usa_startups[usa_startups['funding_total_usd'] > 0]\n",
    "\n",
    "if len(usa_funded) > 0:\n",
    "    print(f\"Startups with funding:     {len(usa_funded):>7,} ({100*len(usa_funded)/len(usa_startups):>5.1f}%)\")\n",
    "    print(f\"Mean funding:              ${usa_funded['funding_total_usd'].mean()/1e6:>7.1f}M\")\n",
    "    print(f\"Median funding:            ${usa_funded['funding_total_usd'].median()/1e6:>7.1f}M\")\n",
    "    print(f\"Total capital raised:      ${usa_funded['funding_total_usd'].sum()/1e9:>7.1f}B\")\n",
    "    print(f\"Max funding (single co.):  ${usa_funded['funding_total_usd'].max()/1e9:>7.2f}B\")\n",
    "\n",
    "# FUNDING ROUNDS\n",
    "print(\"\\n FUNDING ROUNDS (USA)\")\n",
    "\n",
    "rounds_stats = usa_startups['funding_rounds'].describe()\n",
    "print(f\"Mean rounds:               {rounds_stats['mean']:>7.2f}\")\n",
    "print(f\"Median rounds:             {rounds_stats['50%']:>7.0f}\")\n",
    "print(f\"Max rounds:                {rounds_stats['max']:>7.0f}\")\n",
    "\n",
    "# Companies with 5+ rounds\n",
    "high_rounds = (usa_startups['funding_rounds'] >= 5).sum()\n",
    "print(f\"Companies with 5+ rounds:  {high_rounds:>7,} ({100*high_rounds/len(usa_startups):>5.1f}%)\")\n",
    "\n",
    "# TEMPORAL ANALYSIS\n",
    "print(\"\\n FOUNDATION TIMELINE\")\n",
    "\n",
    "usa_startups['founded_year'] = usa_startups['founded_at'].dt.year\n",
    "yearly = usa_startups['founded_year'].value_counts().sort_index()\n",
    "\n",
    "# Show recent years\n",
    "recent_years = yearly.tail(10)\n",
    "print(\"Recent years (last 10):\")\n",
    "for year, count in recent_years.items():\n",
    "    print(f\"  {year}: {count:>5,} startups\")\n",
    "\n",
    "# SECTOR SUCCESS RATES\n",
    "print(\"\\n TOP 10 SECTORS BY IPO RATE (USA)\")\n",
    "\n",
    "usa_sector_success = usa_startups.groupby('category_code').agg({\n",
    "    'id': 'count',\n",
    "    'status': lambda x: (x == 'ipo').sum()\n",
    "}).rename(columns={'id': 'total', 'status': 'ipos'})\n",
    "\n",
    "usa_sector_success = usa_sector_success[usa_sector_success['total'] >= 50]  # Min 50 companies\n",
    "usa_sector_success['ipo_rate'] = 100 * usa_sector_success['ipos'] / usa_sector_success['total']\n",
    "usa_sector_success = usa_sector_success.sort_values('ipo_rate', ascending=False).head(10)\n",
    "\n",
    "for sector, row in usa_sector_success.iterrows():\n",
    "    print(f\"{sector:25s} {row['ipo_rate']:>5.1f}% ({int(row['ipos']):>3}/{int(row['total']):>4})\")\n",
    "\n",
    "# VISUALIZATION\n",
    "\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Status distribution\n",
    "status_usa.plot(kind='bar', ax=axes[0,0], color='steelblue', alpha=0.8)\n",
    "axes[0,0].set_title('USA Startups - Status Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Status')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].grid(axis='y', alpha=0.3)\n",
    "axes[0,0].set_xticklabels(axes[0,0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# 2. Top 15 sectors\n",
    "top_15_sectors = usa_startups['category_code'].value_counts().head(15).sort_values()\n",
    "top_15_sectors.plot(kind='barh', ax=axes[0,1], color='coral')\n",
    "axes[0,1].set_title('Top 15 Sectors in USA', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Number of Startups')\n",
    "axes[0,1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Funding distribution (log scale)\n",
    "usa_funded_log = np.log10(usa_funded['funding_total_usd'] + 1)\n",
    "axes[1,0].hist(usa_funded_log, bins=50, color='green', alpha=0.7, edgecolor='white')\n",
    "axes[1,0].set_title('Funding Distribution (Log Scale)', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Log10(Funding USD)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].axvline(usa_funded_log.median(), color='red', linestyle='--', \n",
    "                  label=f'Median: ${10**usa_funded_log.median()/1e6:.1f}M')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Foundation timeline\n",
    "yearly_plot = usa_startups['founded_year'].value_counts().sort_index()\n",
    "yearly_plot.plot(kind='line', ax=axes[1,1], color='purple', linewidth=2)\n",
    "axes[1,1].set_title('USA Startups Founded per Year', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Year')\n",
    "axes[1,1].set_ylabel('Number of Startups Founded')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Dataset Exploration: Jay Ritter IPO\n",
    "\n",
    "Let's analyze the Jay Ritter database to understand:\n",
    "1. Data structure and quality\n",
    "2. Temporal distribution of IPOs\n",
    "3. Proportion of VC-backed IPOs\n",
    "4. Useful variables for matching with Crunchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.5 ESPLORAZIONE JAY RITTER IPO\n",
    "# =============================================================================\n",
    "\n",
    "if ritter is not None:\n",
    "\n",
    "    print(\"JAY RITTER IPO DATABASE - STRUCTURE\")\n",
    "\n",
    "\n",
    "    print(f\"\\n Shape: {ritter.shape}\")\n",
    "    print(f\"\\n Colonne: {ritter.columns.tolist()}\")\n",
    "\n",
    "    # Info\n",
    "    ritter.info()\n",
    "\n",
    "    # Prime righe\n",
    "    print(ritter.head().to_string())\n",
    "\n",
    "# %%\n",
    "# Statistiche descrittive Ritter\n",
    "if ritter is not None:\n",
    "\n",
    "    print(\"JAY RITTER - DESCRIPTIVE STATISTICS\")\n",
    "\n",
    "    # Pulizia colonne\n",
    "    # 'offer date' è in formato YYYYMMDD\n",
    "    ritter[\"ipo_date\"] = pd.to_datetime(\n",
    "        ritter[\"offer date\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\"\n",
    "    )\n",
    "    ritter[\"ipo_year\"] = ritter[\"ipo_date\"].dt.year\n",
    "\n",
    "    # 'Founding' ha -99 per missing\n",
    "    ritter[\"founding_year_clean\"] = ritter[\"Founding\"].replace(-99, np.nan)\n",
    "    ritter.loc[ritter[\"founding_year_clean\"] < 1800, \"founding_year_clean\"] = np.nan\n",
    "\n",
    "    # Calcola time-to-IPO\n",
    "    ritter[\"time_to_ipo\"] = ritter[\"ipo_year\"] - ritter[\"founding_year_clean\"]\n",
    "\n",
    "    print(\"\\n Distribution VC Status:\")\n",
    "    print(ritter[\"VC\"].value_counts())\n",
    "    print(\"\\n   Legend: 0=Non-VC, 1=VC-backed, 2=Growth Capital\")\n",
    "\n",
    "    # Visualizzazioni\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # IPO per anno\n",
    "    ax1 = axes[0, 0]\n",
    "    ritter[\"ipo_year\"].value_counts().sort_index().plot(ax=ax1, color=\"steelblue\")\n",
    "    ax1.set_title(\"IPO per Anno (Jay Ritter)\", fontsize=12)\n",
    "    ax1.set_xlabel(\"Year\")\n",
    "    ax1.set_ylabel(\"Number IPO\")\n",
    "\n",
    "    # IPO per decade\n",
    "    ax2 = axes[0, 1]\n",
    "    ritter[\"decade\"] = (ritter[\"ipo_year\"] // 10) * 10\n",
    "    ritter[\"decade\"].value_counts().sort_index().plot(kind=\"bar\", ax=ax2, color=\"coral\")\n",
    "    ax2.set_title(\"IPO per Decade\", fontsize=12)\n",
    "    ax2.set_xlabel(\"Decade\")\n",
    "    ax2.set_ylabel(\"Number IPO\")\n",
    "    ax2.set_xticklabels(\n",
    "        [f\"{int(d)}s\" for d in sorted(ritter[\"decade\"].dropna().unique())], rotation=45\n",
    "    )\n",
    "\n",
    "    # Time to IPO distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    valid_time = ritter[\"time_to_ipo\"][\n",
    "        (ritter[\"time_to_ipo\"] > 0) & (ritter[\"time_to_ipo\"] < 100)\n",
    "    ]\n",
    "    valid_time.hist(bins=50, ax=ax3, color=\"green\", alpha=0.7, edgecolor=\"white\")\n",
    "    ax3.axvline(\n",
    "        valid_time.median(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Median: {valid_time.median():.1f} anni\",\n",
    "    )\n",
    "    ax3.set_title(\"Distribution Time-to-IPO (Ritter)\", fontsize=12)\n",
    "    ax3.set_xlabel(\"Years from founding to IPO\")\n",
    "    ax3.set_ylabel(\"Frequency\")\n",
    "    ax3.legend()\n",
    "\n",
    "    # VC vs Non-VC time to IPO\n",
    "    ax4 = axes[1, 1]\n",
    "    vc_backed = ritter[ritter[\"VC\"].isin([1, 2])][\"time_to_ipo\"]\n",
    "    non_vc = ritter[ritter[\"VC\"] == 0][\"time_to_ipo\"]\n",
    "\n",
    "    ax4.boxplot([vc_backed.dropna(), non_vc.dropna()], labels=[\"VC-Backed\", \"Non-VC\"])\n",
    "    ax4.set_title(\"Time-to-IPO: VC vs Non-VC\", fontsize=12)\n",
    "    ax4.set_ylabel(\"Years\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistiche\n",
    "    print(f\"\\n Time-to-IPO Statistics:\")\n",
    "    print(f\"   Overall median: {valid_time.median():.1f} anni\")\n",
    "    print(\n",
    "        f\"   VC-backed median: {vc_backed[(vc_backed > 0) & (vc_backed < 100)].median():.1f} anni\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   Non-VC median: {non_vc[(non_vc > 0) & (non_vc < 100)].median():.1f} anni\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.7 MATCHING CRUNCHBASE ↔ JAY RITTER\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_company_name(name):\n",
    "    \"\"\"Normalizes a company name for matching.\n",
    "\n",
    "    Conservative strategy:\n",
    "    - Lowercase\n",
    "    - Remove legal suffixes\n",
    "    - Remove punctuation\n",
    "    - Strip spaces\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Original name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        Normalized name\n",
    "\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "\n",
    "    name = str(name).lower().strip()\n",
    "\n",
    "    # Rimuovi suffissi legali (ordine importante: più lunghi prima)\n",
    "    suffixes = [\n",
    "        \" incorporated\",\n",
    "        \" corporation\",\n",
    "        \" limited\",\n",
    "        \" company\",\n",
    "        \" inc.\",\n",
    "        \" inc\",\n",
    "        \" corp.\",\n",
    "        \" corp\",\n",
    "        \" llc\",\n",
    "        \" ltd.\",\n",
    "        \" ltd\",\n",
    "        \" co.\",\n",
    "        \" co\",\n",
    "        \" plc\",\n",
    "        \" sa\",\n",
    "        \" ag\",\n",
    "        \" gmbh\",\n",
    "        \" nv\",\n",
    "        \" bv\",\n",
    "        \", inc.\",\n",
    "        \", inc\",\n",
    "        \", corp.\",\n",
    "        \", corp\",\n",
    "        \", llc\",\n",
    "        \", ltd\",\n",
    "    ]\n",
    "\n",
    "    for suffix in suffixes:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[: -len(suffix)]\n",
    "\n",
    "    # Rimuovi punteggiatura\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "\n",
    "    # Rimuovi spazi multipli e strip\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "\n",
    "    return name if name else None\n",
    "\n",
    "\n",
    "# Test normalizzazione\n",
    "test_names = [\n",
    "    \"Apple Inc.\",\n",
    "    \"Microsoft Corporation\",\n",
    "    \"Google, Inc.\",\n",
    "    \"FACEBOOK INC\",\n",
    "    \"Tesla Motors, Inc.\",\n",
    "]\n",
    "\n",
    "print(\"NAMES STANDARDIZATION TEST\")\n",
    "\n",
    "for name in test_names:\n",
    "    print(f\"   '{name}' → '{normalize_company_name(name)}'\")\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARAZIONE DATI PER MATCHING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"PREPARATION MATCHING CRUNCHBASE ↔ RITTER\")\n",
    "\n",
    "\n",
    "# Prepara Crunchbase IPOs\n",
    "if crunchbase.get(\"ipos\") is not None and crunchbase.get(\"objects\") is not None:\n",
    "    # Unisci IPO con objects per avere i nomi\n",
    "    ipos_with_names = crunchbase[\"ipos\"].merge(\n",
    "        crunchbase[\"objects\"][[\"id\", \"name\", \"country_code\"]],\n",
    "        left_on=\"object_id\",\n",
    "        right_on=\"id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    ipos_with_names[\"name_normalized\"] = ipos_with_names[\"name\"].apply(\n",
    "        normalize_company_name\n",
    "    )\n",
    "    ipos_with_names[\"ipo_year\"] = pd.to_datetime(\n",
    "        ipos_with_names[\"public_at\"], errors=\"coerce\"\n",
    "    ).dt.year\n",
    "\n",
    "    # Filtra solo USA per matching con Ritter (che è solo USA)\n",
    "    ipos_usa = ipos_with_names[ipos_with_names[\"country_code\"] == \"USA\"].copy()\n",
    "\n",
    "    print(\n",
    "        f\" Crunchbase IPO prepared: {len(ipos_with_names):,} totals, {len(ipos_usa):,} USA\"\n",
    "    )\n",
    "\n",
    "# Prepara Ritter\n",
    "if ritter is not None:\n",
    "    ritter[\"name_normalized\"] = ritter[\"IPO name\"].apply(normalize_company_name)\n",
    "    ritter[\"ipo_year\"] = pd.to_datetime(\n",
    "        ritter[\"offer date\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\"\n",
    "    ).dt.year\n",
    "\n",
    "    print(f\" Ritter IPO preparate: {len(ritter):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ESECUZIONE MATCHING CONSERVATIVO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"MATCHING CONSERVATIVE: EXACT NAME + YEAR (±1)\")\n",
    "\n",
    "if \"ipos_usa\" in dir() and ritter is not None:\n",
    "    # Strategia 1: Exact match su nome normalizzato + stesso anno\n",
    "    matched_exact = pd.merge(\n",
    "        ipos_usa[[\"object_id\", \"name\", \"name_normalized\", \"ipo_year\", \"stock_symbol\"]],\n",
    "        ritter[[\"IPO name\", \"name_normalized\", \"ipo_year\", \"Ticker\", \"VC\", \"Founding\"]],\n",
    "        on=[\"name_normalized\", \"ipo_year\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_cb\", \"_ritter\"),\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Match esatti (nome + anno): {len(matched_exact):,}\")\n",
    "\n",
    "    # Strategia 2: Exact match su nome + anno ±1\n",
    "    matches_year_flex = []\n",
    "\n",
    "    for _, row_cb in ipos_usa.iterrows():\n",
    "        if pd.isna(row_cb[\"name_normalized\"]) or pd.isna(row_cb[\"ipo_year\"]):\n",
    "            continue\n",
    "\n",
    "        # Cerca in Ritter con tolleranza ±1 anno\n",
    "        mask = (ritter[\"name_normalized\"] == row_cb[\"name_normalized\"]) & (\n",
    "            abs(ritter[\"ipo_year\"] - row_cb[\"ipo_year\"]) <= 1\n",
    "        )\n",
    "\n",
    "        ritter_matches = ritter[mask]\n",
    "\n",
    "        if len(ritter_matches) == 1:  # Solo match univoci\n",
    "            matches_year_flex.append(\n",
    "                {\n",
    "                    \"object_id\": row_cb[\"object_id\"],\n",
    "                    \"name_cb\": row_cb[\"name\"],\n",
    "                    \"name_ritter\": ritter_matches.iloc[0][\"IPO name\"],\n",
    "                    \"year_cb\": row_cb[\"ipo_year\"],\n",
    "                    \"year_ritter\": ritter_matches.iloc[0][\"ipo_year\"],\n",
    "                    \"vc_ritter\": ritter_matches.iloc[0][\"VC\"],\n",
    "                    \"founding_ritter\": ritter_matches.iloc[0][\"Founding\"],\n",
    "                    \"match_type\": \"exact_name_year_flex\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    matched_flex = pd.DataFrame(matches_year_flex)\n",
    "\n",
    "    print(f\" Match with year tolerance ±1: {len(matched_flex):,}\")\n",
    "\n",
    "    # Statistiche matching\n",
    "    total_cb_usa = len(ipos_usa)\n",
    "    total_ritter = len(ritter)\n",
    "\n",
    "    print(f\"\\n STATISTICS MATCHING:\")\n",
    "    print(f\"Crunchbase IPO USA: {total_cb_usa:,}\")\n",
    "    print(f\"Jay Ritter IPO: {total_ritter:,}\")\n",
    "    print(f\"Match found: {len(matched_flex):,}\")\n",
    "    print(f\"Match rate (su CB): {100 * len(matched_flex) / total_cb_usa:.1f}%\")\n",
    "    print(f\"Match rate (su Ritter): {100 * len(matched_flex) / total_ritter:.1f}%\")\n",
    "\n",
    "    # Sample dei match\n",
    "    if len(matched_flex) > 0:\n",
    "        print(f\"\\n Sample Matches:\")\n",
    "        print(matched_flex.head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build Final Integrated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILDING FINAL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"BUILDING FINAL DATASET\")\n",
    "\n",
    "# Start with STARTUPS\n",
    "finale = STARTUPS.copy()\n",
    "\n",
    "# Merge with IPOs\n",
    "if ipos is not None:\n",
    "    finale = finale.merge(\n",
    "        ipos[['object_id', 'valuation_amount']],\n",
    "        left_on='id', right_on='object_id', how='left'\n",
    "    ).drop(columns=['object_id'], errors='ignore')\n",
    "\n",
    "# Drop redundant columns\n",
    "finale = finale.drop(columns=[\n",
    "    'homepage_url', 'parent_id', 'entity_type', 'short_description',\n",
    "    'description', 'tag_list'\n",
    "], errors='ignore')\n",
    "\n",
    "print(f\"{finale.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate funding by round type\n",
    "if funding_rounds is not None:\n",
    "    rounds_data = finale[['id']].merge(\n",
    "        funding_rounds[['object_id', 'funding_round_type', 'raised_amount_usd']],\n",
    "        left_on='id', right_on='object_id', how='left'\n",
    "    ).drop(columns=['object_id'])\n",
    "    \n",
    "    # Pivot to get funding per round type\n",
    "    rounds_pivot = rounds_data.pivot_table(\n",
    "        index='id',\n",
    "        columns='funding_round_type',\n",
    "        values='raised_amount_usd',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Ensure all round types exist\n",
    "    round_types = ['angel', 'crowdfunding', 'other', 'post-ipo', \n",
    "                   'private-equity', 'series-a', 'series-b', 'series-c+', 'venture']\n",
    "    for rt in round_types:\n",
    "        if rt not in rounds_pivot.columns:\n",
    "            rounds_pivot[rt] = 0\n",
    "    \n",
    "    # Rename columns\n",
    "    rounds_pivot = rounds_pivot.rename(columns={\n",
    "        'post-ipo': 'post_ipo',\n",
    "        'private-equity': 'private_equity',\n",
    "        'series-a': 'series_a',\n",
    "        'series-b': 'series_b',\n",
    "        'series-c+': 'series_c'\n",
    "    })\n",
    "    \n",
    "    # Merge with finale\n",
    "    finale = finale.merge(rounds_pivot, left_on='id', right_index=True, how='left')\n",
    "    for col in rounds_pivot.columns:\n",
    "        finale[col] = finale[col].fillna(0)\n",
    "\n",
    "print(f\"{finale.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of acquisitions made\n",
    "if acquisitions is not None:\n",
    "    num_acq = acquisitions.groupby('acquiring_object_id').size().reset_index(name='num_acquisizioni_effettuate')\n",
    "    finale = finale.merge(num_acq, left_on='id', right_on='acquiring_object_id', how='left')\n",
    "    finale['num_acquisizioni_effettuate'] = finale['num_acquisizioni_effettuate'].fillna(0).astype(int)\n",
    "    finale = finale.drop(columns=['acquiring_object_id'], errors='ignore')\n",
    "\n",
    "print(f\"{finale.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funding sources\n",
    "if investments is not None:\n",
    "    # Financial organizations\n",
    "    fin_org_inv = investments[\n",
    "        investments['investor_object_id'].isin(FINANCIAL_ORG['id'])\n",
    "    ].groupby('funded_object_id')['investor_object_id'].nunique().reset_index(name='fin_org_financed')\n",
    "    \n",
    "    finale = finale.merge(fin_org_inv, left_on='id', right_on='funded_object_id', how='left')\n",
    "    finale['fin_org_financed'] = (finale['fin_org_financed'] > 0).astype(int)\n",
    "    finale = finale.drop(columns=['funded_object_id'], errors='ignore')\n",
    "    \n",
    "    # People\n",
    "    person_inv = investments[\n",
    "        investments['investor_object_id'].isin(Person['id'])\n",
    "    ].groupby('funded_object_id').size().reset_index(name='person_financed')\n",
    "    \n",
    "    finale = finale.merge(person_inv, left_on='id', right_on='funded_object_id', how='left')\n",
    "    finale['person_financed'] = (finale['person_financed'] > 0).astype(int)\n",
    "    finale = finale.drop(columns=['funded_object_id'], errors='ignore')\n",
    "    \n",
    "    # Startups\n",
    "    startup_inv = investments[\n",
    "        investments['investor_object_id'].isin(STARTUPS['id'])\n",
    "    ].groupby('funded_object_id').size().reset_index(name='startup_financed')\n",
    "    \n",
    "    finale = finale.merge(startup_inv, left_on='id', right_on='funded_object_id', how='left')\n",
    "    finale['startup_financed'] = (finale['startup_financed'] > 0).astype(int)\n",
    "    finale = finale.drop(columns=['funded_object_id'], errors='ignore')\n",
    "\n",
    "print(f\"{finale.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of products\n",
    "num_prod = Product[\n",
    "    Product['parent_id'].notna() & (Product['parent_id'] != '')\n",
    "].groupby('parent_id').size().reset_index(name='num_prodotti')\n",
    "\n",
    "finale = finale.merge(num_prod, left_on='id', right_on='parent_id', how='left')\n",
    "finale['num_prodotti'] = finale['num_prodotti'].fillna(0).astype(int)\n",
    "finale = finale.drop(columns=['parent_id'], errors='ignore')\n",
    "\n",
    "print(f\"{finale.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Jay Ritter IPO dataset\n",
    "ritter_path = os.path.join(config.RAW_DIR, \"IPO-age.xlsx\")\n",
    "\n",
    "if os.path.exists(ritter_path):\n",
    "    print(f\"Found Ritter file: {ritter_path}\")\n",
    "    \n",
    "    # Load Excel file\n",
    "    ritter = pd.read_excel(ritter_path, sheet_name=0)\n",
    "    print(f\"Ritter loaded: {ritter.shape}\")\n",
    "    \n",
    "    # --- DATA CLEANING ---\n",
    "    \n",
    "    # 1. Parse offer date (format: YYYYMMDD → extract year)\n",
    "    if 'offer date' in ritter.columns:\n",
    "        ritter['offer_date_str'] = ritter['offer date'].astype(str)\n",
    "        ritter['ipo_year'] = ritter['offer_date_str'].str[:4].astype(float)\n",
    "    elif 'Offer Date' in ritter.columns:\n",
    "        ritter['Offer Date'] = pd.to_datetime(ritter['Offer Date'], errors='coerce')\n",
    "        ritter['ipo_year'] = ritter['Offer Date'].dt.year\n",
    "    else:\n",
    "        print(\"⚠ Warning: Date column not found, trying to infer...\")\n",
    "        date_col = [c for c in ritter.columns if 'date' in c.lower()][0]\n",
    "        ritter['ipo_year'] = pd.to_datetime(ritter[date_col], errors='coerce').dt.year\n",
    "    \n",
    "    # 2. Convert VC/Internet to binary\n",
    "    if 'VC' in ritter.columns:\n",
    "        ritter['is_vc_backed'] = (ritter['VC'] == 'Y').astype(int)\n",
    "    \n",
    "    if 'Internet' in ritter.columns:\n",
    "        ritter['is_tech'] = (ritter['Internet'] == 'Y').astype(int)\n",
    "    \n",
    "    # 3. Calculate time-to-IPO\n",
    "    if 'Founding' in ritter.columns:\n",
    "        ritter['time_to_ipo'] = ritter['ipo_year'] - ritter['Founding']\n",
    "    \n",
    "    # 4. Filter outliers\n",
    "    ritter_clean = ritter[\n",
    "        (ritter['ipo_year'] >= 1975) & \n",
    "        (ritter['ipo_year'] <= 2024)\n",
    "    ].copy()\n",
    "    \n",
    "    if 'time_to_ipo' in ritter_clean.columns:\n",
    "        ritter_clean = ritter_clean[\n",
    "            (ritter_clean['time_to_ipo'] >= 0) & \n",
    "            (ritter_clean['time_to_ipo'] <= 100)\n",
    "        ]\n",
    "    \n",
    "    print(f\"Ritter cleaned: {ritter_clean.shape}\")\n",
    "    print(f\"Date range: {ritter_clean['ipo_year'].min():.0f} - {ritter_clean['ipo_year'].max():.0f}\")\n",
    "    \n",
    "    # --- AGGREGATE BY YEAR ---\n",
    "    \n",
    "    print(\"AGGREGATING MARKET CONDITIONS BY YEAR\")\n",
    "    \n",
    "    market_conditions = ritter_clean.groupby('ipo_year').agg({\n",
    "        'IPO name': 'count',  # Total IPOs\n",
    "    }).rename(columns={'IPO name': 'ipo_count_total'})\n",
    "    \n",
    "    # Add VC and tech counts\n",
    "    if 'is_vc_backed' in ritter_clean.columns:\n",
    "        market_conditions['ipo_count_vc'] = ritter_clean.groupby('ipo_year')['is_vc_backed'].sum()\n",
    "        market_conditions['vc_share_pct'] = (\n",
    "            market_conditions['ipo_count_vc'] / market_conditions['ipo_count_total']\n",
    "        ) * 100\n",
    "    \n",
    "    if 'is_tech' in ritter_clean.columns:\n",
    "        market_conditions['ipo_count_tech'] = ritter_clean.groupby('ipo_year')['is_tech'].sum()\n",
    "        market_conditions['tech_share_pct'] = (\n",
    "            market_conditions['ipo_count_tech'] / market_conditions['ipo_count_total']\n",
    "        ) * 100\n",
    "    \n",
    "    # Add time-to-IPO statistics\n",
    "    if 'time_to_ipo' in ritter_clean.columns:\n",
    "        market_conditions['time_to_ipo_mean'] = ritter_clean.groupby('ipo_year')['time_to_ipo'].mean()\n",
    "        market_conditions['time_to_ipo_median'] = ritter_clean.groupby('ipo_year')['time_to_ipo'].median()\n",
    "    \n",
    "    # Market heat index (normalized by historical average)\n",
    "    historical_avg = market_conditions['ipo_count_total'].mean()\n",
    "    market_conditions['market_heat'] = market_conditions['ipo_count_total'] / historical_avg\n",
    "    \n",
    "    # Moving average (3-year smoothing)\n",
    "    market_conditions['ipo_count_ma3'] = market_conditions['ipo_count_total'].rolling(\n",
    "        window=3, center=True, min_periods=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Market cycle labels (manual categorization)\n",
    "    def assign_market_cycle(year):\n",
    "        if year in range(1999, 2001):\n",
    "            return 'boom'\n",
    "        elif year in range(2001, 2003):\n",
    "            return 'bust'\n",
    "        elif year in range(2008, 2010):\n",
    "            return 'bust'\n",
    "        elif year in range(2020, 2022):\n",
    "            return 'boom'\n",
    "        elif year in range(2022, 2025):\n",
    "            return 'contraction'\n",
    "        else:\n",
    "            return 'normal'\n",
    "    \n",
    "    market_conditions['market_cycle'] = market_conditions.index.map(assign_market_cycle)\n",
    "    \n",
    "    market_conditions = market_conditions.reset_index()\n",
    "    \n",
    "    print(f\"Market conditions aggregated: {market_conditions.shape}\")\n",
    "    print(f\"Years covered: {market_conditions['ipo_year'].min():.0f} - {market_conditions['ipo_year'].max():.0f}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample (recent years):\")\n",
    "    print(market_conditions.tail(10)[['ipo_year', 'ipo_count_total', 'market_heat', 'market_cycle']])\n",
    "    \n",
    "    # --- MERGE WITH FINALE ---\n",
    "    \n",
    "    print(\"MERGING MARKET CONDITIONS WITH STARTUPS\")\n",
    "    \n",
    "    # Extract first funding year from finale\n",
    "    if 'first_funding_at' in finale.columns:\n",
    "        # Use already existing first_funding_at if available\n",
    "        finale['first_funding_year'] = pd.to_datetime(finale['first_funding_at']).dt.year\n",
    "    else:\n",
    "        # Fallback: use founded_at\n",
    "        finale['first_funding_year'] = finale['founded_at'].dt.year\n",
    "    \n",
    "    # Merge\n",
    "    finale = finale.merge(\n",
    "        market_conditions,\n",
    "        left_on='first_funding_year',\n",
    "        right_on='ipo_year',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Drop duplicate year column\n",
    "    finale = finale.drop(columns=['ipo_year'], errors='ignore')\n",
    "    \n",
    "    print(f\"Merge complete: {finale.shape}\")\n",
    "    \n",
    "    # Check merge quality\n",
    "    merged_pct = 100 * finale['ipo_count_total'].notna().sum() / len(finale)\n",
    "    print(f\"Startups with market data: {merged_pct:.1f}%\")\n",
    "    \n",
    "    # Show new columns\n",
    "    new_cols = [c for c in market_conditions.columns if c != 'ipo_year']\n",
    "    print(f\"\\nNew columns added ({len(new_cols)}):\")\n",
    "    for col in new_cols[:5]:  # Show first 5\n",
    "        print(f\"• {col}\")\n",
    "    if len(new_cols) > 5:\n",
    "        print(f\"and {len(new_cols)-5} more\")\n",
    "    \n",
    "    print(f\"\\n{finale.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Ritter file not found: {ritter_path}\")\n",
    "    print(\"Download from: https://site.warrington.ufl.edu/ritter/files/IPO-age.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_market_data = finale['ipo_count_total'].notna()\n",
    "no_market_data = ~has_market_data\n",
    "\n",
    "print(f\"Startups WITH market data:    {has_market_data.sum():>8,} ({100*has_market_data.mean():>5.1f}%)\")\n",
    "print(f\"Startups WITHOUT market data:  {no_market_data.sum():>8,} ({100*no_market_data.mean():>5.1f}%)\")\n",
    "\n",
    "# 2. Why missing? Analyze first_funding_year distribution\n",
    "print(\"\\nFIRST FUNDING YEAR DISTRIBUTION\")\n",
    "\n",
    "# Overall distribution\n",
    "funding_year_stats = finale['first_funding_year'].describe()\n",
    "print(f\"Min year:    {funding_year_stats['min']:.0f}\")\n",
    "print(f\"25% year:    {funding_year_stats['25%']:.0f}\")\n",
    "print(f\"Median year: {funding_year_stats['50%']:.0f}\")\n",
    "print(f\"75% year:    {funding_year_stats['75%']:.0f}\")\n",
    "print(f\"Max year:    {funding_year_stats['max']:.0f}\")\n",
    "\n",
    "# Ritter coverage\n",
    "ritter_min_year = 1975\n",
    "ritter_max_year = 2024\n",
    "\n",
    "print(f\"\\nRitter coverage: {ritter_min_year} - {ritter_max_year}\")\n",
    "\n",
    "# Count startups by year range\n",
    "before_ritter = (finale['first_funding_year'] < ritter_min_year).sum()\n",
    "in_ritter_range = (\n",
    "    (finale['first_funding_year'] >= ritter_min_year) & \n",
    "    (finale['first_funding_year'] <= ritter_max_year)\n",
    ").sum()\n",
    "after_ritter = (finale['first_funding_year'] > ritter_max_year).sum()\n",
    "missing_year = finale['first_funding_year'].isna().sum()\n",
    "\n",
    "print(f\"\\nStartups by year range:\")\n",
    "print(f\"Before 1975:{before_ritter:>8,} ({100*before_ritter/len(finale):>5.1f}%)\")\n",
    "print(f\"1975-2024 (Ritter):{in_ritter_range:>8,} ({100*in_ritter_range/len(finale):>5.1f}%)\")\n",
    "print(f\"After 2024:{after_ritter:>8,} ({100*after_ritter/len(finale):>5.1f}%)\")\n",
    "print(f\"Missing year:{missing_year:>8,} ({100*missing_year/len(finale):>5.1f}%)\")\n",
    "\n",
    "# 3. Match quality for in-range startups\n",
    "in_range = finale[\n",
    "    (finale['first_funding_year'] >= ritter_min_year) & \n",
    "    (finale['first_funding_year'] <= ritter_max_year)\n",
    "].copy()\n",
    "\n",
    "if len(in_range) > 0:\n",
    "    matched = in_range['ipo_count_total'].notna().sum()\n",
    "    match_rate = 100 * matched / len(in_range)\n",
    "    \n",
    "    print(f\"Startups in range (1975-2024):{len(in_range):,}\")\n",
    "    print(f\"Successfully matched:{matched:,} ({match_rate:.1f}%)\")\n",
    "    \n",
    "    if match_rate < 100:\n",
    "        print(f\"\\n{100-match_rate:.1f}% in-range startups have no match!\")\n",
    "        print(\"Likely cause: first_funding_year has NaN values\")\n",
    "\n",
    "# 4. Sample comparison: WITH vs WITHOUT market data\n",
    "with_data = finale[has_market_data].copy()\n",
    "without_data = finale[no_market_data].copy()\n",
    "\n",
    "if len(with_data) > 0 and len(without_data) > 0:\n",
    "    print(f\"\\nIPO Rate:\")\n",
    "    print(f\"WITH market data:{100 * (with_data['status'] == 'ipo').mean():>5.2f}%\")\n",
    "    print(f\"WITHOUT market data:{100 * (without_data['status'] == 'ipo').mean():>5.2f}%\")\n",
    "    \n",
    "    print(f\"\\nAcquisition Rate:\")\n",
    "    print(f\"WITH market data:{100 * (with_data['status'] == 'acquired').mean():>5.2f}%\")\n",
    "    print(f\"WITHOUT market data:{100 * (without_data['status'] == 'acquired').mean():>5.2f}%\")\n",
    "    \n",
    "    print(f\"\\nMean funding (for funded companies):\")\n",
    "    with_funding = with_data[with_data['funding_total_usd'] > 0]['funding_total_usd'].mean()\n",
    "    without_funding = without_data[without_data['funding_total_usd'] > 0]['funding_total_usd'].mean()\n",
    "    print(f\"WITH market data:${with_funding/1e6:>7.1f}M\")\n",
    "    print(f\"WITHOUT market data:${without_funding/1e6:>7.1f}M\")\n",
    "\n",
    "# 5. Show examples with market data\n",
    "sample_with = with_data[with_data['ipo_count_total'].notna()].head(3)\n",
    "\n",
    "for idx, row in sample_with.iterrows():\n",
    "    print(f\"\\nStartup ID: {row.get('id', 'N/A')}\")\n",
    "    print(f\"First funding year:{row['first_funding_year']:.0f}\")\n",
    "    print(f\"IPOs that year:{row['ipo_count_total']:.0f}\")\n",
    "    print(f\"Market heat:{row['market_heat']:.2f}\")\n",
    "    print(f\"Market cycle:{row['market_cycle']}\")\n",
    "    print(f\"Status:{row['status']}\")\n",
    "\n",
    "# 6. Recommendation\n",
    "coverage_pct = 100 * has_market_data.mean()\n",
    "\n",
    "if coverage_pct < 30:\n",
    "    print(\"\\nLOW COVERAGE (<30%)\")\n",
    "    print(\"Consider using founded_year instead of first_funding_year\")\n",
    "    print(\"Or extrapolate market conditions for years outside Ritter range\")\n",
    "elif coverage_pct < 70:\n",
    "    print(\"\\nMEDIUM COVERAGE (30-70%)\")\n",
    "    print(\"Current approach OK for analysis\")\n",
    "    print(\"Consider imputation for missing values in modeling\")\n",
    "else:\n",
    "    print(\"\\nGOOD COVERAGE (>70%)\")\n",
    "    print(\"Market conditions successfully merged!\")\n",
    "\n",
    "print(f\"\\nCurrent coverage: {coverage_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "finale = finale.drop(columns=[\n",
    "    'first_milestone_at', 'last_milestone_at', 'last_funding_at',\n",
    "    'first_funding_at', 'name', 'city', 'region', 'closed_at'\n",
    "], errors='ignore')\n",
    "\n",
    "finale = finale.drop_duplicates()\n",
    "\n",
    "# Type conversions\n",
    "for col in ['category_code', 'status', 'country_code']:\n",
    "    if col in finale.columns:\n",
    "        finale[col] = finale[col].astype('category')\n",
    "\n",
    "for col in ['fin_org_financed', 'person_financed', 'startup_financed']:\n",
    "    if col in finale.columns:\n",
    "        finale[col] = finale[col].astype(int)\n",
    "\n",
    "# Log transformation for funding\n",
    "if 'funding_total_usd' in finale.columns:\n",
    "    finale['log_fund_tot'] = finale['funding_total_usd'].apply(\n",
    "        lambda x: 0 if pd.isna(x) or x == 0 else np.log(x)\n",
    "    )\n",
    "\n",
    "print(f\"\\nFinal dataset: {finale.shape}\")\n",
    "print(f\"\\nColumns: {list(finale.columns[:15])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. Merge IPO dates\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "if 'ipos' in locals() or 'ipos' in globals():\n",
    "    # Prepare IPO dates\n",
    "    ipos_clean = ipos[['object_id', 'public_at']].copy()\n",
    "    ipos_clean = ipos_clean.drop_duplicates(subset=['object_id'])\n",
    "    \n",
    "    print(f\"  IPO records available: {len(ipos_clean):,}\")\n",
    "    \n",
    "    # Count before merge\n",
    "    n_ipo_before = (finale['status'] == 'ipo').sum()\n",
    "    \n",
    "    # Merge\n",
    "    finale = finale.merge(\n",
    "        ipos_clean,\n",
    "        left_on='id',\n",
    "        right_on='object_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Drop duplicate column\n",
    "    finale = finale.drop(columns=['object_id'], errors='ignore')\n",
    "    \n",
    "    # Check merge quality\n",
    "    n_matched = finale[finale['status'] == 'ipo']['public_at'].notna().sum()\n",
    "    \n",
    "    print(f\"IPO companies in dataset: {n_ipo_before:,}\")\n",
    "    print(f\"Dates matched: {n_matched:,} ({100*n_matched/n_ipo_before:.1f}%)\")\n",
    "    \n",
    "    if n_matched < n_ipo_before:\n",
    "        print(f\"Missing dates: {n_ipo_before - n_matched:,} (will be handled in Notebook 2)\")\n",
    "else:\n",
    "    print(f\"'ipos' not found - creating empty column\")\n",
    "    finale['public_at'] = pd.NaT\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Merge M&A dates\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "if 'acquisitions' in locals() or 'acquisitions' in globals():\n",
    "    # Prepare M&A dates\n",
    "    acq_clean = acquisitions[['acquired_object_id', 'acquired_at']].copy()\n",
    "    acq_clean = acq_clean.drop_duplicates(subset=['acquired_object_id'])\n",
    "    \n",
    "    print(f\"M&A records available: {len(acq_clean):,}\")\n",
    "    \n",
    "    # Count before merge\n",
    "    n_ma_before = (finale['status'] == 'acquired').sum()\n",
    "    \n",
    "    # Merge\n",
    "    finale = finale.merge(\n",
    "        acq_clean,\n",
    "        left_on='id',\n",
    "        right_on='acquired_object_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Drop duplicate column\n",
    "    finale = finale.drop(columns=['acquired_object_id'], errors='ignore')\n",
    "    \n",
    "    # Check merge quality\n",
    "    n_matched = finale[finale['status'] == 'acquired']['acquired_at'].notna().sum()\n",
    "    \n",
    "    print(f\"M&A companies in dataset: {n_ma_before:,}\")\n",
    "    print(f\"Dates matched: {n_matched:,} ({100*n_matched/n_ma_before:.1f}%)\")\n",
    "    \n",
    "    if n_matched < n_ma_before:\n",
    "        print(f\"Missing dates: {n_ma_before - n_matched:,} (will be handled in Notebook 2)\")\n",
    "else:\n",
    "    print(f\"'acquisitions' not found - creating empty column\")\n",
    "    finale['acquired_at'] = pd.NaT\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Summary\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nDataset shape: {finale.shape}\")\n",
    "print(f\"\\nNew columns added:\")\n",
    "print(f\"  - public_at:   {finale['public_at'].notna().sum():,} non-null ({100*finale['public_at'].notna().mean():.1f}%)\")\n",
    "print(f\"  - acquired_at: {finale['acquired_at'].notna().sum():,} non-null ({100*finale['acquired_at'].notna().mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Step 1: Keep only startups with real first_funding_year\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nDataset before filtering:\")\n",
    "print(f\"Total startups:{len(finale):,}\")\n",
    "print(f\"With first_funding_year:{finale['first_funding_year'].notna().sum():,} ({100*finale['first_funding_year'].notna().mean():.1f}%)\")\n",
    "\n",
    "# Filter\n",
    "finale_real = finale[finale['first_funding_year'].notna()].copy()\n",
    "\n",
    "print(f\"\\nFiltered to real funding data only:\")\n",
    "print(f\"Kept:{len(finale_real):,} startups\")\n",
    "print(f\"Removed:{len(finale) - len(finale_real):,} startups without funding year\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 2: Keep only years in Ritter range (1975-2024)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nFiltering for Ritter range (1975-2024)...\")\n",
    "\n",
    "before_1975 = (finale_real['first_funding_year'] < 1975).sum()\n",
    "after_2024 = (finale_real['first_funding_year'] > 2024).sum()\n",
    "\n",
    "finale_real = finale_real[\n",
    "    (finale_real['first_funding_year'] >= 1975) &\n",
    "    (finale_real['first_funding_year'] <= 2013)\n",
    "].copy()\n",
    "\n",
    "if before_1975 + after_2024 > 0:\n",
    "    print(f\"Removed{before_1975:,} startups (funding < 1975)\")\n",
    "    print(f\"Removed{after_2024:,} startups (funding > 2024)\")\n",
    "print(f\"Kept:{len(finale_real):,} startups\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 3: Verify 100% market data coverage\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nVerifying market data coverage...\")\n",
    "\n",
    "coverage = 100 * finale_real['ipo_count_total'].notna().mean()\n",
    "missing = finale_real['ipo_count_total'].isna().sum()\n",
    "\n",
    "print(f\"Market data coverage:{coverage:.1f}%\")\n",
    "\n",
    "if missing > 0:\n",
    "    print(f\"Removing{missing:,} startups with missing market data...\")\n",
    "    finale_real = finale_real[finale_real['ipo_count_total'].notna()].copy()\n",
    "    print(f\"Final:{len(finale_real):,} startups\")\n",
    "else:\n",
    "    print(f\"Perfect! 100% coverage\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 4: Filter USA only\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "finale_usa = finale_real[finale_real['country_code'] == 'USA'].copy()\n",
    "\n",
    "print(f\"Global (with real funding):{len(finale_real):,}\")\n",
    "print(f\"USA only:{len(finale_usa):,} ({100*len(finale_usa)/len(finale_real):.1f}%)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FINAL DATASET SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nSample Size:{len(finale_usa):,} startups\")\n",
    "\n",
    "print(f\"\\nExit Distribution:\")\n",
    "status_counts = finale_usa['status'].value_counts()\n",
    "for status in ['ipo', 'acquired', 'closed', 'operating']:\n",
    "    if status in status_counts.index:\n",
    "        count = status_counts[status]\n",
    "        pct = 100 * count / len(finale_usa)\n",
    "        print(f\"{status:10s} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nFunding Year Range:\")\n",
    "print(f\"Min:{finale_usa['first_funding_year'].min():.0f}\")\n",
    "print(f\"25%:{finale_usa['first_funding_year'].quantile(0.25):.0f}\")\n",
    "print(f\"Median:{finale_usa['first_funding_year'].median():.0f}\")\n",
    "print(f\"75%:{finale_usa['first_funding_year'].quantile(0.75):.0f}\")\n",
    "print(f\"Max:{finale_usa['first_funding_year'].max():.0f}\")\n",
    "\n",
    "print(f\"\\nFunding Statistics:\")\n",
    "funded_usa = finale_usa[finale_usa['funding_total_usd'] > 0]\n",
    "if len(funded_usa) > 0:\n",
    "    print(f\"Startups with funding:{len(funded_usa):,} ({100*len(funded_usa)/len(finale_usa):.1f}%)\")\n",
    "    print(f\"Mean:${funded_usa['funding_total_usd'].mean()/1e6:.1f}M\")\n",
    "    print(f\"Median:${funded_usa['funding_total_usd'].median()/1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\nTop 5 Sectors:\")\n",
    "top_sectors = finale_usa['category_code'].value_counts().head(5)\n",
    "for sector, count in top_sectors.items():\n",
    "    pct = 100 * count / len(finale_usa)\n",
    "    print(f\"{sector:20s} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nMarket Conditions Distribution:\")\n",
    "for cycle in ['boom', 'normal', 'bust', 'contraction']:\n",
    "    count = (finale_usa['market_cycle'] == cycle).sum()\n",
    "    if count > 0:\n",
    "        pct = 100 * count / len(finale_usa)\n",
    "        print(f\"{cycle:15s} {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE HOT/COLD MARKET INDICATOR\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate threshold (top 25% of IPO activity)\n",
    "q75 = market_conditions['ipo_count_total'].quantile(0.75)\n",
    "finale_usa['hot_ipo_market'] = (finale_usa['ipo_count_total'] >= q75).astype(int)\n",
    "\n",
    "hot_count = (finale_usa['hot_ipo_market'] == 1).sum()\n",
    "cold_count = (finale_usa['hot_ipo_market'] == 0).sum()\n",
    "\n",
    "print(f\"Hot market threshold: {q75:.0f} IPOs/year\")\n",
    "print(f\"\\nMarket Hotness Distribution:\")\n",
    "print(f\"Hot (top 25%):{hot_count:>6,} ({100*hot_count/len(finale_usa):>5.1f}%)\")\n",
    "print(f\"Cold (bottom 75%):{cold_count:>6,} ({100*cold_count/len(finale_usa):>5.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# MARKET IMPACT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nEXIT RATES BY MARKET HOTNESS\")\n",
    "\n",
    "for hot_val in [1, 0]:\n",
    "    label = \"Hot market\" if hot_val == 1 else \"Cold market\"\n",
    "    subset = finale_usa[finale_usa['hot_ipo_market'] == hot_val]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        ipo_rate = 100 * (subset['status'] == 'ipo').sum() / len(subset)\n",
    "        acq_rate = 100 * (subset['status'] == 'acquired').sum() / len(subset)\n",
    "        success_rate = ipo_rate + acq_rate\n",
    "        \n",
    "        print(f\"\\n{label}:\")\n",
    "        print(f\"N startups:{len(subset):>8,}\")\n",
    "        print(f\"IPO rate:{ipo_rate:>7.2f}%\")\n",
    "        print(f\"Acquisition rate:{acq_rate:>7.2f}%\")\n",
    "        print(f\"Success rate (IPO+M&A):{success_rate:>7.2f}%\")\n",
    "\n",
    "# Calculate difference\n",
    "hot_subset = finale_usa[finale_usa['hot_ipo_market'] == 1]\n",
    "cold_subset = finale_usa[finale_usa['hot_ipo_market'] == 0]\n",
    "\n",
    "ipo_diff = 100 * ((hot_subset['status'] == 'ipo').mean() - (cold_subset['status'] == 'ipo').mean())\n",
    "\n",
    "print(f\"\\nINSIGHT:\")\n",
    "print(f\"  IPO rate difference: {ipo_diff:+.2f}% (hot vs cold)\")\n",
    "\n",
    "# Exit rates by market cycle\n",
    "print(\"\\nEXIT RATES BY MARKET CYCLE\")\n",
    "\n",
    "for cycle in ['boom', 'normal', 'bust', 'contraction']:\n",
    "    subset = finale_usa[finale_usa['market_cycle'] == cycle]\n",
    "    if len(subset) > 0:\n",
    "        ipo_rate = 100 * (subset['status'] == 'ipo').sum() / len(subset)\n",
    "        acq_rate = 100 * (subset['status'] == 'acquired').sum() / len(subset)\n",
    "        print(f\"{cycle:15s} N={len(subset):>7,}  IPO: {ipo_rate:>5.2f}%  M&A: {acq_rate:>5.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE FINAL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "# Create directory if doesn't exist\n",
    "config.PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save main dataset\n",
    "output_path = os.path.join(config.PROCESSED_PATH, 'finale_usa_real_funding.csv')\n",
    "finale_usa.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved: {output_path}\")\n",
    "print(f\"Shape: {finale_usa.shape}\")\n",
    "print(f\"Size: {os.path.getsize(output_path) / (1024**2):.1f} MB\")\n",
    "\n",
    "# Update finale for next steps\n",
    "finale = finale_usa.copy()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Startups: {len(finale):,} (USA only)\")\n",
    "print(f\"Real funding data: 100%\")\n",
    "print(f\"Market conditions: 100%\")\n",
    "print(f\"Years: 1975-2013\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Comprehensive analysis of the integrated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Status Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATUS DISTRIBUTION \n",
    "# =============================================================================\n",
    "\n",
    "print(\"STATUS DISTRIBUTION\")\n",
    "\n",
    "status_counts = finale_usa['status'].value_counts()\n",
    "status_pct = finale_usa['status'].value_counts(normalize=True) * 100\n",
    "\n",
    "for status in status_counts.index:\n",
    "    print(f\"{status:15s} {status_counts[status]:>10,} ({status_pct[status]:>6.2f}%)\")\n",
    "print(f\"{'TOTAL':15s} {status_counts.sum():>10,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION - 3 PANELS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PANEL 1: Bar plot (all statuses)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "status_counts_sorted = status_counts.reindex(['operating', 'acquired', 'closed', 'ipo'])\n",
    "colors_bar = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "status_counts_sorted.plot(kind='bar', ax=axes[0], color=colors_bar, alpha=0.8, width=0.7)\n",
    "axes[0].set_xlabel('Status', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Status Distribution (All Startups)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PANEL 2: Pie chart with EXPLODE + LEGEND\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Explode small slices for visibility\n",
    "explode = (0, 0.1, 0.1, 0.15)  # Explode ipo, closed, acquired\n",
    "\n",
    "wedges, texts, autotexts = axes[1].pie(\n",
    "    status_counts_sorted, \n",
    "    explode=explode,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors_bar, \n",
    "    startangle=90,\n",
    "    pctdistance=0.85,\n",
    "    textprops={'fontsize': 11, 'fontweight': 'bold'}\n",
    ")\n",
    "\n",
    "# Make percentage text white and bold\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(11)\n",
    "\n",
    "# Add legend outside\n",
    "axes[1].legend(\n",
    "    status_counts_sorted.index, \n",
    "    title=\"Status\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "    fontsize=11\n",
    ")\n",
    "\n",
    "axes[1].set_title('Status Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PANEL 3: Zoomed bar chart (EXITS ONLY - without operating)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Focus on exits (exclude operating)\n",
    "exits_only = status_counts.drop('operating', errors='ignore')\n",
    "exits_sorted = exits_only.reindex(['acquired', 'closed', 'ipo'])\n",
    "colors_exits = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "exits_sorted.plot(kind='bar', ax=axes[2], color=colors_exits, alpha=0.8, width=0.7)\n",
    "axes[2].set_xlabel('Exit Type', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Exit Distribution (Zoom)', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Helper Function: Cramer's V\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"\n",
    "    Calculate Cramer's V statistic for categorical-categorical association.\n",
    "    \n",
    "    V ranges from 0 (no association) to 1 (perfect association)\n",
    "    Interpretation:\n",
    "    - 0.00-0.10: Negligible\n",
    "    - 0.10-0.20: Weak\n",
    "    - 0.20-0.30: Moderate\n",
    "    - 0.30+: Strong\n",
    "    \"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    min_dim = min(confusion_matrix.shape) - 1\n",
    "    \n",
    "    if min_dim == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.sqrt(chi2 / (n * min_dim))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. CREATE FUNDING STAGE CATEGORIES (for this analysis)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Categorize funding_rounds into stages\n",
    "finale_usa['funding_stage'] = pd.cut(\n",
    "    finale_usa['funding_rounds'],\n",
    "    bins=[-1, 0, 1, 2, 4, 100],\n",
    "    labels=['Unfunded', 'Seed (1)', 'Early (2)', 'Growth (3-4)', 'Late (5+)']\n",
    ")\n",
    "\n",
    "print(\"\\nFunding Stage Distribution:\")\n",
    "for stage, count in finale_usa['funding_stage'].value_counts().sort_index().items():\n",
    "    pct = 100 * count / len(finale_usa)\n",
    "    bar = \"█\" * int(pct / 2)\n",
    "    print(f\"  {str(stage):15s} {count:>6,} ({pct:>5.1f}%) {bar}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. CONTINGENCY TABLE: Funding Stage × Status\n",
    "# ---------------------------------------------------------------------------\n",
    "# Absolute counts\n",
    "contingency = pd.crosstab(\n",
    "    finale_usa['funding_stage'],\n",
    "    finale_usa['status'],\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ")\n",
    "\n",
    "print(\"ABSOLUTE COUNTS:\")\n",
    "print(contingency)\n",
    "\n",
    "# Row percentages (most useful)\n",
    "print(\"ROW PERCENTAGES (% within each funding stage):\")\n",
    "\n",
    "contingency_pct = pd.crosstab(\n",
    "    finale_usa['funding_stage'],\n",
    "    finale_usa['status'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "print(contingency_pct.round(2))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. CRAMER'S V: Funding Stage × Status\n",
    "# ---------------------------------------------------------------------------\n",
    "# Calculate Cramer's V\n",
    "v_funding_status = cramers_v(\n",
    "    finale_usa['funding_stage'].dropna(), \n",
    "    finale_usa['status']\n",
    ")\n",
    "\n",
    "print(f\"Funding Stage x Status\")\n",
    "print(f\"Cramer's V = {v_funding_status:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if v_funding_status < 0.1:\n",
    "    strength = \"Negligible\"\n",
    "elif v_funding_status < 0.2:\n",
    "    strength = \"Weak\"\n",
    "elif v_funding_status < 0.3:\n",
    "    strength = \"Moderate\"\n",
    "else:\n",
    "    strength = \"Strong\"\n",
    "\n",
    "print(f\"{strength} association\")\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_value, dof, expected = chi2_contingency(\n",
    "    pd.crosstab(finale_usa['funding_stage'].dropna(), finale_usa['status'])\n",
    ")\n",
    "\n",
    "print(f\"\\nChi-square test:\")\n",
    "print(f\"χ² = {chi2:.2f}\")\n",
    "print(f\"df = {dof}\")\n",
    "print(f\"p-value = {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.001:\n",
    "    print(f\"Highly significant (p < 0.001)\")\n",
    "elif p_value < 0.01:\n",
    "    print(f\"Very significant (p < 0.01)\")\n",
    "elif p_value < 0.05:\n",
    "    print(f\"Significant (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"Not significant (p ≥ 0.05)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. ADDITIONAL CRAMER'S V CALCULATIONS\n",
    "# ---------------------------------------------------------------------------\n",
    "# Market cycle × Status\n",
    "if 'market_cycle' in finale_usa.columns:\n",
    "    v_market_status = cramers_v(\n",
    "        finale_usa['market_cycle'].dropna(),\n",
    "        finale_usa['status']\n",
    "    )\n",
    "    print(f\"Market Cycle x Status:V = {v_market_status:.4f}\")\n",
    "\n",
    "# Category × Status (top 20 categories only - to avoid too many sparse cells)\n",
    "top_20_categories = finale_usa['category_code'].value_counts().head(20).index\n",
    "finale_usa_top20 = finale_usa[finale_usa['category_code'].isin(top_20_categories)]\n",
    "\n",
    "v_sector_status = cramers_v(\n",
    "    finale_usa_top20['category_code'],\n",
    "    finale_usa_top20['status']\n",
    ")\n",
    "print(f\"Sector x Status (top 20):V = {v_sector_status:.4f}\")\n",
    "\n",
    "# Calculate IPO rates and counts\n",
    "stage_stats = []\n",
    "for stage in ['Unfunded', 'Seed (1)', 'Early (2)', 'Growth (3-4)', 'Late (5+)']:\n",
    "    subset = finale_usa[finale_usa['funding_stage'] == stage]\n",
    "    if len(subset) > 0:\n",
    "        ipo_rate = 100 * (subset['status'] == 'ipo').mean()\n",
    "        count = len(subset)\n",
    "        stage_stats.append({'stage': stage, 'ipo_rate': ipo_rate, 'count': count})\n",
    "\n",
    "stage_df = pd.DataFrame(stage_stats)\n",
    "x = range(len(stage_df))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6. KEY INSIGHTS SUMMARY\n",
    "# ---------------------------------------------------------------------------\n",
    "# Find stage with highest IPO rate\n",
    "best_stage = stage_df.loc[stage_df['ipo_rate'].idxmax()]\n",
    "worst_stage = stage_df.loc[stage_df['ipo_rate'].idxmin()]\n",
    "\n",
    "print(f\"Highest IPO rate:{best_stage['stage']:15s} {best_stage['ipo_rate']:.2f}%\")\n",
    "print(f\"Lowest IPO rate:{worst_stage['stage']:15s} {worst_stage['ipo_rate']:.2f}%\")\n",
    "\n",
    "# Overall association strength\n",
    "print(f\"\\nAssociation Strengths:\")\n",
    "print(f\"Funding Stage - Status:{v_funding_status:.4f} ({strength})\")\n",
    "print(f\"Sector - Status:{v_sector_status:.4f}\")\n",
    "if 'market_cycle' in finale_usa.columns:\n",
    "    print(f\"Market Cycle - Status:{v_market_status:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. Create Angel-Backed Binary Variable\n",
    "# ---------------------------------------------------------------------------\n",
    "# Binary: has angel funding or not\n",
    "finale_usa['angel_backed'] = (finale_usa['angel'] > 0).astype(int)\n",
    "\n",
    "print(\"\\n✓ Angel-backed Distribution:\")\n",
    "for val, label in [(0, 'No angel funding'), (1, 'Angel-backed')]:\n",
    "    count = (finale_usa['angel_backed'] == val).sum()\n",
    "    pct = 100 * count / len(finale_usa)\n",
    "    bar = \"█\" * int(pct / 2)\n",
    "    print(f\"{label:20s} {count:>6,} ({pct:>5.1f}%) {bar}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Create Angel Funding Amount Categories\n",
    "# ---------------------------------------------------------------------------\n",
    "# # For those with angel funding, categorize by amount\n",
    "finale_usa['angel_category'] = pd.cut(\n",
    "    finale_usa['angel'],\n",
    "    bins=[-1, 0, 250000, 1000000, 100000000],\n",
    "    labels=['No angel', 'Low (<$250k)', 'Medium ($250k-$1M)', 'High (>$1M)']\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Angel Funding Categories:\")\n",
    "for cat, count in finale_usa['angel_category'].value_counts().sort_index().items():\n",
    "    pct = 100 * count / len(finale_usa)\n",
    "    bar = \"█\" * int(pct / 2)\n",
    "    print(f\"{str(cat):25s} {count:>6,} ({pct:>5.1f}%) {bar}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Angel Funding by Status\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "for status in ['ipo', 'acquired', 'closed', 'operating']:\n",
    "    subset = finale_usa[finale_usa['status'] == status]\n",
    "    angel_rate = 100 * (subset['angel_backed'] == 1).mean()\n",
    "    count = (subset['angel_backed'] == 1).sum()\n",
    "    print(f\"{status:12s} {angel_rate:>5.1f}% angel-backed ({count:>5,} startups)\")\n",
    "\n",
    "for status in ['ipo', 'acquired', 'closed', 'operating']:\n",
    "    subset = finale_usa[(finale_usa['status'] == status) & (finale_usa['angel'] > 0)]\n",
    "    if len(subset) > 0:\n",
    "        mean_angel = subset['angel'].mean()\n",
    "        median_angel = subset['angel'].median()\n",
    "        print(f\"{status:12s} Mean: ${mean_angel/1e6:>5.1f}M  Median: ${median_angel/1e6:>5.1f}M  (N={len(subset):,})\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Contingency Table: Angel-Backed × Status\n",
    "# ---------------------------------------------------------------------------\n",
    "# Absolute counts\n",
    "contingency_angel = pd.crosstab(\n",
    "    finale_usa['angel_backed'].map({0: 'No angel', 1: 'Angel-backed'}),\n",
    "    finale_usa['status'],\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ")\n",
    "\n",
    "print(\"ABSOLUTE COUNTS:\")\n",
    "print(contingency_angel)\n",
    "\n",
    "# Row percentages\n",
    "\n",
    "contingency_angel_pct = pd.crosstab(\n",
    "    finale_usa['angel_backed'].map({0: 'No angel', 1: 'Angel-backed'}),\n",
    "    finale_usa['status'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "print(contingency_angel_pct.round(2))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Cramer's V: Angel-Backed × Status\n",
    "# ---------------------------------------------------------------------------\n",
    "# Calculate Cramer's V\n",
    "v_angel_status = cramers_v(finale_usa['angel_backed'], finale_usa['status'])\n",
    "\n",
    "# Interpretation\n",
    "if v_angel_status < 0.1:\n",
    "    strength = \"Negligible\"\n",
    "elif v_angel_status < 0.2:\n",
    "    strength = \"Weak\"\n",
    "elif v_angel_status < 0.3:\n",
    "    strength = \"Moderate\"\n",
    "else:\n",
    "    strength = \"Strong\"\n",
    "\n",
    "print(f\"{strength} association\")\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_value, dof, expected = chi2_contingency(\n",
    "    pd.crosstab(finale_usa['angel_backed'], finale_usa['status'])\n",
    ")\n",
    "\n",
    "print(f\"\\nChi-square test:\")\n",
    "print(f\"χ² = {chi2:.2f}\")\n",
    "print(f\"df = {dof}\")\n",
    "print(f\"p-value = {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.001:\n",
    "    print(f\"Highly significant (p < 0.001)\")\n",
    "elif p_value < 0.01:\n",
    "    print(f\"Very significant (p < 0.01)\")\n",
    "elif p_value < 0.05:\n",
    "    print(f\"Significant (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"Not significant (p ≥ 0.05)\")\n",
    "\n",
    "v_angel_amount = cramers_v(finale_usa['angel_category'].dropna(), finale_usa['status'])\n",
    "\n",
    "print(f\"Cramer's V = {v_angel_amount:.4f}\")\n",
    "\n",
    "# Calculate IPO rates\n",
    "angel_ipo = []\n",
    "for val, label in [(0, 'No angel'), (1, 'Angel-backed')]:\n",
    "    subset = finale_usa[finale_usa['angel_backed'] == val]\n",
    "    ipo_rate = 100 * (subset['status'] == 'ipo').mean()\n",
    "    count = len(subset)\n",
    "    angel_ipo.append({'group': label, 'ipo_rate': ipo_rate, 'count': count})\n",
    "\n",
    "angel_ipo_df = pd.DataFrame(angel_ipo)\n",
    "x = range(len(angel_ipo_df))\n",
    "\n",
    "# Calculate exit rates by angel category\n",
    "angel_exits = []\n",
    "for cat in ['No angel', 'Low (<$250k)', 'Medium ($250k-$1M)', 'High (>$1M)']:\n",
    "    subset = finale_usa[finale_usa['angel_category'] == cat]\n",
    "    if len(subset) > 0:\n",
    "        ipo_rate = 100 * (subset['status'] == 'ipo').mean()\n",
    "        ma_rate = 100 * (subset['status'] == 'acquired').mean()\n",
    "        angel_exits.append({'category': cat, 'ipo': ipo_rate, 'ma': ma_rate})\n",
    "\n",
    "exits_df = pd.DataFrame(angel_exits)\n",
    "x = range(len(exits_df))\n",
    "\n",
    "# Filter only angel-backed startups\n",
    "angel_funded = finale_usa[finale_usa['angel'] > 0].copy()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6. KEY INSIGHTS\n",
    "# ---------------------------------------------------------------------------\n",
    "# Angel-backed rate by status\n",
    "ipo_angel_rate = 100 * (finale_usa[finale_usa['status']=='ipo']['angel_backed']==1).mean()\n",
    "noipo_angel_rate = 100 * (finale_usa[finale_usa['status']!='ipo']['angel_backed']==1).mean()\n",
    "\n",
    "print(f\"Angel-Backed Rates:\")\n",
    "print(f\"IPO companies:{ipo_angel_rate:.1f}% angel-backed\")\n",
    "print(f\"Non-IPO companies:{noipo_angel_rate:.1f}% angel-backed\")\n",
    "print(f\"Difference:{ipo_angel_rate - noipo_angel_rate:+.1f}%\")\n",
    "\n",
    "# IPO rate comparison\n",
    "ipo_rate_angel = 100 * (finale_usa[finale_usa['angel_backed']==1]['status']=='ipo').mean()\n",
    "ipo_rate_no_angel = 100 * (finale_usa[finale_usa['angel_backed']==0]['status']=='ipo').mean()\n",
    "\n",
    "print(f\"\\nIPO Rates:\")\n",
    "print(f\"Angel-backed:{ipo_rate_angel:.2f}%\")\n",
    "print(f\"Not angel-backed:{ipo_rate_no_angel:.2f}%\")\n",
    "print(f\"Ratio:{ipo_rate_angel/ipo_rate_no_angel:.2f}x\")\n",
    "\n",
    "# Amount effect\n",
    "if len(finale_usa[(finale_usa['angel_category']=='High (>$1M)') & (finale_usa['status']=='ipo')]) > 0:\n",
    "    high_angel_ipo = 100 * (finale_usa[finale_usa['angel_category']=='High (>$1M)']['status']=='ipo').mean()\n",
    "    low_angel_ipo = 100 * (finale_usa[finale_usa['angel_category']=='Low (<$250k)']['status']=='ipo').mean()\n",
    "    \n",
    "    print(f\"\\nAngel Amount Impact:\")\n",
    "    print(f\"High angel (>$1M):{high_angel_ipo:.2f}% IPO rate\")\n",
    "    print(f\"Low angel (<$250k):{low_angel_ipo:.2f}% IPO rate\")\n",
    "\n",
    "print(f\"\\nAssociation Strength:\")\n",
    "print(f\"Angel-backed x Status:{v_angel_status:.4f} ({strength})\")\n",
    "print(f\"Angel amount x Status:{v_angel_amount:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sector in finale_usa['category_code'].cat.categories:\n",
    "    subset = finale_usa[finale_usa['category_code'] == sector]\n",
    "    \n",
    "    # Angel-backed\n",
    "    angel_subset = subset[subset['angel_backed'] == 1]\n",
    "    angel_ipo_rate = 100 * (angel_subset['status'] == 'ipo').mean() if len(angel_subset) > 0 else 0\n",
    "    angel_n = len(angel_subset)\n",
    "    \n",
    "    # Non-angel\n",
    "    no_angel_subset = subset[subset['angel_backed'] == 0]\n",
    "    no_angel_ipo_rate = 100 * (no_angel_subset['status'] == 'ipo').mean() if len(no_angel_subset) > 0 else 0\n",
    "    no_angel_n = len(no_angel_subset)\n",
    "    \n",
    "    # Only print if both groups exist\n",
    "    if angel_n > 10 and no_angel_n > 10:  # Min 10 startup per gruppo\n",
    "        diff = angel_ipo_rate - no_angel_ipo_rate\n",
    "        print(f\"{sector:20s} Angel: {angel_ipo_rate:>5.2f}% (N={angel_n:>4,})  \"\n",
    "              f\"No-angel: {no_angel_ipo_rate:>5.2f}% (N={no_angel_n:>4,})  \"\n",
    "              f\"Diff: {diff:>+6.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if angel + VC changes the pattern\n",
    "finale_usa['has_multiple_rounds'] = (finale_usa['funding_rounds'] >= 3).astype(int)\n",
    "\n",
    "# Angel + Multiple rounds vs Angel alone\n",
    "angel_multi = finale_usa[(finale_usa['angel_backed']==1) & \n",
    "                         (finale_usa['has_multiple_rounds']==1)]\n",
    "angel_solo = finale_usa[(finale_usa['angel_backed']==1) & \n",
    "                        (finale_usa['has_multiple_rounds']==0)]\n",
    "\n",
    "print(f\"Angel + VC: {(angel_multi['status']=='ipo').mean():.2%} IPO rate\")\n",
    "print(f\"Angel only: {(angel_solo['status']=='ipo').mean():.2%} IPO rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. Basic Relationship: angel vs funding_total_usd\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Check if angel is a SUBSET of funding_total_usd\n",
    "subset_check = (finale_usa['angel'] <= finale_usa['funding_total_usd']).all()\n",
    "\n",
    "print(f\"\\nIs angel ≤ funding_total_usd for ALL rows? {subset_check}\")\n",
    "\n",
    "if subset_check:\n",
    "    print(\"angel appears to be a SUBSET of total funding\")\n",
    "else:\n",
    "    # Find violations\n",
    "    violations = finale_usa[finale_usa['angel'] > finale_usa['funding_total_usd']]\n",
    "    print(f\"Found {len(violations)} cases where angel > total funding (data error?)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Proportion Analysis\n",
    "# ---------------------------------------------------------------------------\n",
    "# For angel-backed startups, what % of total funding is angel?\n",
    "angel_backed = finale_usa[finale_usa['angel'] > 0].copy()\n",
    "\n",
    "if len(angel_backed) > 0:\n",
    "    angel_backed['angel_pct'] = 100 * angel_backed['angel'] / angel_backed['funding_total_usd']\n",
    "    \n",
    "    print(f\"\\nFor angel-backed startups (N={len(angel_backed):,}):\")\n",
    "    print(f\"Angel as % of total funding:\")\n",
    "    print(f\"Mean:{angel_backed['angel_pct'].mean():.1f}%\")\n",
    "    print(f\"Median:{angel_backed['angel_pct'].median():.1f}%\")\n",
    "    print(f\"Q1:{angel_backed['angel_pct'].quantile(0.25):.1f}%\")\n",
    "    print(f\"Q3:{angel_backed['angel_pct'].quantile(0.75):.1f}%\")\n",
    "    \n",
    "    # Cases where angel = 100% of funding (angel-only)\n",
    "    angel_only = (angel_backed['angel_pct'] > 99).sum()\n",
    "    print(f\"\\nAngel-only (100% of funding):{angel_only:,} ({100*angel_only/len(angel_backed):.1f}%)\")\n",
    "    \n",
    "    # Cases where angel < 50% (has other funding)\n",
    "    angel_plus = (angel_backed['angel_pct'] < 50).sum()\n",
    "    print(f\"Angel + Other (angel <50%):{angel_plus:,} ({100*angel_plus/len(angel_backed):.1f}%)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Check for VC-related columns\n",
    "# ---------------------------------------------------------------------------\n",
    "# Search for columns with 'vc', 'series', 'round' in name\n",
    "vc_related = [col for col in finale_usa.columns if any(x in col.lower() for x in ['vc', 'series', 'round', 'seed'])]\n",
    "\n",
    "print(f\"\\nFound{len(vc_related)} potentially VC-related columns:\")\n",
    "for col in vc_related:\n",
    "    dtype = finale_usa[col].dtype\n",
    "    nunique = finale_usa[col].nunique()\n",
    "    non_null = finale_usa[col].notna().sum()\n",
    "    print(f\"  • {col:30s} {str(dtype):15s} {nunique:>6} unique ({non_null:>7,} non-null)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Compare angel vs funding characteristics\n",
    "# ---------------------------------------------------------------------------\n",
    "print(f\"\\nAngel funding:\")\n",
    "print(f\"Non-zero:{(finale_usa['angel'] > 0).sum():,} ({100*(finale_usa['angel'] > 0).mean():.1f}%)\")\n",
    "print(f\"Median (if >0):${finale_usa[finale_usa['angel'] > 0]['angel'].median()/1e6:.1f}M\")\n",
    "print(f\"Mean (if >0):${finale_usa[finale_usa['angel'] > 0]['angel'].mean()/1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\nTotal funding:\")\n",
    "print(f\"Non-zero:{(finale_usa['funding_total_usd'] > 0).sum():,} ({100*(finale_usa['funding_total_usd'] > 0).mean():.1f}%)\")\n",
    "print(f\"Median (if >0):${finale_usa[finale_usa['funding_total_usd'] > 0]['funding_total_usd'].median()/1e6:.1f}M\")\n",
    "print(f\"Mean (if >0):${finale_usa[finale_usa['funding_total_usd'] > 0]['funding_total_usd'].mean()/1e6:.1f}M\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Outlier Analysis\n",
    "# ---------------------------------------------------------------------------\n",
    "# Top 10 angel amounts\n",
    "print(\"\\nTop 10 angel funding amounts:\")\n",
    "top_angel = finale_usa.nlargest(10, 'angel')[['angel', 'funding_total_usd', 'funding_rounds', 'status']]\n",
    "\n",
    "for idx, row in top_angel.iterrows():\n",
    "    angel_amt = row['angel'] / 1e6\n",
    "    total_amt = row['funding_total_usd'] / 1e6\n",
    "    rounds = row['funding_rounds']\n",
    "    status = row['status']\n",
    "    pct = 100 * row['angel'] / row['funding_total_usd'] if row['funding_total_usd'] > 0 else 0\n",
    "    \n",
    "    print(f\"  Angel: ${angel_amt:>6.1f}M  Total: ${total_amt:>6.1f}M  \"\n",
    "          f\"({pct:>5.1f}%)  Rounds: {rounds:>2.0f}  Status: {status}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6. Sample Cases\n",
    "# ---------------------------------------------------------------------------\n",
    "# Case 1: High angel, low total (angel-only?)\n",
    "high_angel_low_total = finale_usa[\n",
    "    (finale_usa['angel'] > 1e6) & \n",
    "    (finale_usa['angel'] / finale_usa['funding_total_usd'] > 0.9)\n",
    "].head(3)\n",
    "\n",
    "print(\"\\nCase 1: High angel amount, mostly angel (>90% of total):\")\n",
    "for idx, row in high_angel_low_total.iterrows():\n",
    "    print(f\"  Angel: ${row['angel']/1e6:.1f}M  Total: ${row['funding_total_usd']/1e6:.1f}M  \"\n",
    "          f\"Rounds: {row['funding_rounds']:.0f}  Status: {row['status']}\")\n",
    "\n",
    "# Case 2: High angel, high total (angel + VC?)\n",
    "high_angel_high_total = finale_usa[\n",
    "    (finale_usa['angel'] > 500000) & \n",
    "    (finale_usa['funding_total_usd'] > 10e6) &\n",
    "    (finale_usa['angel'] / finale_usa['funding_total_usd'] < 0.1)\n",
    "].head(3)\n",
    "\n",
    "print(\"\\nCase 2: High angel, but small % of total (angel + big VC?):\")\n",
    "for idx, row in high_angel_high_total.iterrows():\n",
    "    pct = 100 * row['angel'] / row['funding_total_usd']\n",
    "    print(f\"  Angel: ${row['angel']/1e6:.1f}M ({pct:.1f}%)  Total: ${row['funding_total_usd']/1e6:.1f}M  \"\n",
    "          f\"Rounds: {row['funding_rounds']:.0f}  Status: {row['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NETWORK EFFECTS ANALYSIS (Alternative to Funding Sources)\n",
    "# =============================================================================\n",
    "print(\"NETWORK EFFECTS ANALYSIS\")\n",
    "\n",
    "# Create network categories\n",
    "finale_usa['has_person_financed'] = (finale_usa['person_financed'] > 0).astype(int)\n",
    "finale_usa['has_startup_financed'] = (finale_usa['startup_financed'] > 0).astype(int)\n",
    "finale_usa['has_fin_org_financed'] = (finale_usa['fin_org_financed'] > 0).astype(int)\n",
    "\n",
    "# Cramer's V for network variables\n",
    "v_person = cramers_v(finale_usa['has_person_financed'], finale_usa['status'])\n",
    "v_startup = cramers_v(finale_usa['has_startup_financed'], finale_usa['status'])\n",
    "v_org = cramers_v(finale_usa['has_fin_org_financed'], finale_usa['status'])\n",
    "\n",
    "print(f\"Person financed x Status:V = {v_person:.4f}\")\n",
    "print(f\"Startup financed x Status:V = {v_startup:.4f}\")\n",
    "print(f\"Fin org financed x Status:V = {v_org:.4f}\")\n",
    "\n",
    "# Mosaic plot equivalent\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (var, title) in enumerate([\n",
    "    ('has_person_financed', 'Person Financed'),\n",
    "    ('has_startup_financed', 'Startup Financed'),\n",
    "    ('has_fin_org_financed', 'Fin Org Financed')\n",
    "]):\n",
    "    \n",
    "    ct = pd.crosstab(finale_usa[var], finale_usa['status'], normalize='index')\n",
    "    ct.plot(kind='bar', stacked=True, ax=axes[idx],\n",
    "            color=['#f39c12', '#2ecc71', '#e74c3c', '#3498db'])\n",
    "    \n",
    "    axes[idx].set_title(f'{title} x Status', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(f'{title} (0=No, 1=Yes)', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Proportion', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xticklabels(['No', 'Yes'], rotation=0)\n",
    "    axes[idx].legend(title='Status', bbox_to_anchor=(1.05, 1))\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create investment intensity categories\n",
    "finale_usa['investment_intensity'] = pd.cut(\n",
    "    finale_usa['investment_rounds'],\n",
    "    bins=[-1, 0, 2, 5, 100],\n",
    "    labels=['No investment', 'Low (1-2)', 'Medium (3-5)', 'High (6+)']\n",
    ")\n",
    "\n",
    "# Cramer's V\n",
    "v_investment = cramers_v(finale_usa['investment_intensity'].dropna(), finale_usa['status'])\n",
    "print(f\"Investment intensity x Status: V = {v_investment:.4f}\")\n",
    "\n",
    "# Mosaic plot\n",
    "ct = pd.crosstab(finale_usa['investment_intensity'], finale_usa['status'], normalize='index')\n",
    "ct.plot(kind='bar', stacked=True, figsize=(16, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status by sector\n",
    "sector_data = finale_usa[\n",
    "    finale_usa['category_code'].notna() & (finale_usa['category_code'] != '')\n",
    "].copy()\n",
    "\n",
    "status_settore = sector_data.groupby('category_code').agg({\n",
    "    'id': 'count',\n",
    "    'milestones': 'mean'\n",
    "}).rename(columns={'id': 'n'})\n",
    "\n",
    "# Add status counts\n",
    "for status_val in ['acquired', 'closed', 'ipo', 'operating']:\n",
    "    status_settore[f'n_{status_val}'] = sector_data[\n",
    "        sector_data['status'] == status_val\n",
    "    ].groupby('category_code').size()\n",
    "\n",
    "status_settore = status_settore.fillna(0)\n",
    "\n",
    "# Calculate rates\n",
    "for status_val in ['acquired', 'closed', 'ipo', 'operating']:\n",
    "    status_settore[f'{status_val}_rate'] = status_settore[f'n_{status_val}'] / status_settore['n']\n",
    "\n",
    "status_settore = status_settore.reset_index()\n",
    "status_settore = status_settore.sort_values('n', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 sectors by number of startups:\")\n",
    "print(status_settore.head(20)[['category_code', 'n', 'ipo_rate', 'acquired_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four separate plots for IPO, acquired, closed, operating rates\n",
    "fig, axes = plt.subplots(4, 1, figsize=(12, 28))\n",
    "\n",
    "# Sort by different rates for each plot\n",
    "status_settore_ipo = status_settore.sort_values('ipo_rate', ascending=True)\n",
    "status_settore_acq = status_settore.sort_values('acquired_rate', ascending=True)\n",
    "status_settore_closed = status_settore.sort_values('closed_rate', ascending=True)\n",
    "status_settore_op = status_settore.sort_values('operating_rate', ascending=True)\n",
    "\n",
    "# IPO Rate\n",
    "axes[0].barh(status_settore_ipo['category_code'], \n",
    "             status_settore_ipo['ipo_rate']*100, \n",
    "             color='gold', alpha=0.7)\n",
    "axes[0].set_xlabel('Percentage of listed companies (%)')\n",
    "axes[0].set_title('Percentage of Listed Companies by Sector', fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Acquired Rate\n",
    "axes[1].barh(status_settore_acq['category_code'], \n",
    "             status_settore_acq['acquired_rate']*100,\n",
    "             color='green', alpha=0.7)\n",
    "axes[1].set_xlabel('Percentage of acquired companies (%)')\n",
    "axes[1].set_title('Percentage of Acquired Companies by Sector', fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Closed Rate\n",
    "axes[2].barh(status_settore_closed['category_code'], \n",
    "             status_settore_closed['closed_rate']*100,\n",
    "             color='red', alpha=0.7)\n",
    "axes[2].set_xlabel('Percentage of closed companies (%)')\n",
    "axes[2].set_title('Percentage of Closed Companies by Sector', fontweight='bold')\n",
    "axes[2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Operating Rate\n",
    "axes[3].barh(status_settore_op['category_code'], \n",
    "             status_settore_op['operating_rate']*100,\n",
    "             color='blue', alpha=0.7)\n",
    "axes[3].set_xlabel('Percentage of operating companies (%)')\n",
    "axes[3].set_title('Percentage of Operating Companies by Sector', fontweight='bold')\n",
    "axes[3].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Geographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use USA dataset, group by state\n",
    "status_state = finale_usa[\n",
    "    finale_usa['state_code'].notna() & (finale_usa['state_code'] != '')\n",
    "].groupby('state_code').agg({\n",
    "    'id': 'count'\n",
    "}).rename(columns={'id': 'n'})\n",
    "\n",
    "# Add status counts\n",
    "for status_val in ['ipo', 'acquired', 'closed', 'operating']:\n",
    "    status_state[f'n_{status_val}'] = finale_usa[\n",
    "        finale_usa['status'] == status_val\n",
    "    ].groupby('state_code').size()\n",
    "\n",
    "status_state = status_state.fillna(0)\n",
    "\n",
    "# Calculate rates\n",
    "for status_val in ['ipo', 'acquired', 'closed', 'operating']:\n",
    "    status_state[f'{status_val}_rate'] = (status_state[f'n_{status_val}'] / status_state['n']) * 100\n",
    "\n",
    "status_state = status_state.reset_index().sort_values('n', ascending=False)\n",
    "\n",
    "# Print top 20 states\n",
    "print(\"TOP 20 STATES\")\n",
    "print(f\"{'State':<8} {'N Startups':>12} {'IPO %':>8} {'M&A %':>8} {'Closed %':>8}\")\n",
    "\n",
    "\n",
    "for idx, row in status_state.head(20).iterrows():\n",
    "    print(f\"{row['state_code']:<8} {int(row['n']):>12,} {row['ipo_rate']:>7.2f}% {row['acquired_rate']:>7.2f}% {row['closed_rate']:>7.2f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Panel 1: Top 20 states by count\n",
    "top_20_states = status_state.nlargest(20, 'n').sort_values('n')\n",
    "\n",
    "axes[0,0].barh(top_20_states['state_code'], top_20_states['n'], color='blue', alpha=0.7)\n",
    "axes[0,0].set_xlabel('Number of Startups', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('State', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_title('Top 20 US States by Number of Startups', fontsize=14, fontweight='bold')\n",
    "axes[0,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(top_20_states.iterrows()):\n",
    "    axes[0,0].text(row['n'] + 100, i, f\"{int(row['n']):,}\", va='center', fontsize=9)\n",
    "\n",
    "# Panel 2: Top 10 states by IPO rate (min 50 startups)\n",
    "top_ipo_rate = status_state[status_state['n'] >= 50].nlargest(10, 'ipo_rate').sort_values('ipo_rate')\n",
    "\n",
    "axes[0,1].barh(top_ipo_rate['state_code'], top_ipo_rate['ipo_rate'], color='green', alpha=0.7)\n",
    "axes[0,1].set_xlabel('IPO Rate (%)', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylabel('State', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_title('Top 10 States by IPO Rate (min 50 startups)', fontsize=14, fontweight='bold')\n",
    "axes[0,1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(top_ipo_rate.iterrows()):\n",
    "    axes[0,1].text(row['ipo_rate'] + 0.1, i, f\"{row['ipo_rate']:.1f}%\", va='center', fontsize=9)\n",
    "\n",
    "# Panel 3: Top 10 states by M&A rate (min 50 startups)\n",
    "top_acq_rate = status_state[status_state['n'] >= 50].nlargest(10, 'acquired_rate').sort_values('acquired_rate')\n",
    "\n",
    "axes[1,0].barh(top_acq_rate['state_code'], top_acq_rate['acquired_rate'], color='yellow', alpha=0.7)\n",
    "axes[1,0].set_xlabel('M&A Rate (%)', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylabel('State', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_title('Top 10 States by M&A Rate (min 50 startups)', fontsize=14, fontweight='bold')\n",
    "axes[1,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(top_acq_rate.iterrows()):\n",
    "    axes[1,0].text(row['acquired_rate'] + 0.2, i, f\"{row['acquired_rate']:.1f}%\", va='center', fontsize=9)\n",
    "\n",
    "# Panel 4: Top 10 states by Success rate (IPO + M&A, min 50 startups)\n",
    "status_state['success_rate'] = status_state['ipo_rate'] + status_state['acquired_rate']\n",
    "top_success = status_state[status_state['n'] >= 50].nlargest(10, 'success_rate').sort_values('success_rate')\n",
    "\n",
    "axes[1,1].barh(top_success['state_code'], top_success['success_rate'], color='purple', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Success Rate (IPO + M&A %)', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylabel('State', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_title('Top 10 States by Success Rate (min 50 startups)', fontsize=14, fontweight='bold')\n",
    "axes[1,1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(top_success.iterrows()):\n",
    "    axes[1,1].text(row['success_rate'] + 0.3, i, f\"{row['success_rate']:.1f}%\", va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nUSA state analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED HEATMAP - PROFESSIONAL VERSION\n",
    "# =============================================================================\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MiniMap, Fullscreen, MarkerCluster\n",
    "import json\n",
    "\n",
    "# Load offices data\n",
    "offices_path = os.path.join(config.RAW_DIR, \"offices.csv\")\n",
    "\n",
    "if offices is not None:\n",
    "    # Merge finale_usa with offices to get coordinates\n",
    "    finale_with_geo = finale_usa.merge(\n",
    "        offices[['object_id', 'latitude', 'longitude', 'city', 'state_code']],\n",
    "        left_on='id',\n",
    "        right_on='object_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Filter only startups with valid coordinates\n",
    "    geo_data = finale_with_geo[\n",
    "        finale_with_geo['latitude'].notna() & \n",
    "        finale_with_geo['longitude'].notna() &\n",
    "        (finale_with_geo['latitude'].between(24, 50)) &  # USA bounds\n",
    "        (finale_with_geo['longitude'].between(-125, -65))\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Startups with coordinates: {len(geo_data):,} ({100*len(geo_data)/len(finale_usa):.1f}%)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CREATE BASE MAP\n",
    "    # =========================================================================\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[39.8283, -98.5795],\n",
    "        zoom_start=4,\n",
    "        tiles='CartoDB dark_matter',\n",
    "        control_scale=True\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LAYER 1: HEATMAP\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Create heatmap data with WEIGHTED by status\n",
    "    heat_data_weighted = []\n",
    "    \n",
    "    for idx, row in geo_data.iterrows():\n",
    "        # Weight by importance: IPO = 5x, Acquired = 3x, Others = 1x\n",
    "        if row['status'] == 'ipo':\n",
    "            weight = 5\n",
    "        elif row['status'] == 'acquired':\n",
    "            weight = 3\n",
    "        else:\n",
    "            weight = 1\n",
    "        \n",
    "        heat_data_weighted.append([row['latitude'], row['longitude'], weight])\n",
    "    \n",
    "    # Main heatmap layer\n",
    "    heatmap_layer = folium.FeatureGroup(name='Density Heatmap', show=True)\n",
    "    \n",
    "    HeatMap(\n",
    "        heat_data_weighted,\n",
    "        min_opacity=0.3,\n",
    "        max_zoom=13,\n",
    "        radius=15,\n",
    "        blur=20,\n",
    "        gradient={0.4: 'blue', 0.65: 'lime', 0.8: 'yellow', 1.0: 'red'}\n",
    "    ).add_to(heatmap_layer)\n",
    "    \n",
    "    heatmap_layer.add_to(m)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LAYER 2: TOP CITIES MARKERS \n",
    "    # =========================================================================\n",
    "    \n",
    "    cities_layer = folium.FeatureGroup(name='Top Cities', show=False)\n",
    "    \n",
    "    # Aggregate by city\n",
    "    city_stats = geo_data.groupby('city').agg({\n",
    "        'id': 'count',\n",
    "        'latitude': 'first',\n",
    "        'longitude': 'first',\n",
    "        'status': lambda x: (x.isin(['ipo', 'acquired'])).sum()\n",
    "    }).rename(columns={'id': 'total', 'status': 'success'})\n",
    "    \n",
    "    city_stats['success_rate'] = (city_stats['success'] / city_stats['total']) * 100\n",
    "    city_stats = city_stats.reset_index()\n",
    "    \n",
    "    # Top 20 cities\n",
    "    top_cities = city_stats.nlargest(20, 'total')\n",
    "    \n",
    "    for idx, city in top_cities.iterrows():\n",
    "        # Size by number of startups\n",
    "        radius = min(5 + (city['total'] / 100), 30)\n",
    "        \n",
    "        # Color by success rate\n",
    "        if city['success_rate'] > 15:\n",
    "            color = '#2ecc71'  # Green\n",
    "        elif city['success_rate'] > 10:\n",
    "            color = '#f39c12'  # Orange\n",
    "        else:\n",
    "            color = '#e74c3c'  # Red\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[city['latitude'], city['longitude']],\n",
    "            radius=radius,\n",
    "            popup=f\"\"\"\n",
    "            <div style=\"font-family: Arial; color: #2c3e50;\">\n",
    "                <h4>{city['city']}</h4>\n",
    "                <p><b>Total Startups:</b> {int(city['total']):,}<br>\n",
    "                <b>Successes:</b> {int(city['success'])}<br>\n",
    "                <b>Success Rate:</b> {city['success_rate']:.1f}%</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            tooltip=f\"{city['city']}: {int(city['total']):,} startups\",\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fillColor=color,\n",
    "            fillOpacity=0.7,\n",
    "            weight=2\n",
    "        ).add_to(cities_layer)\n",
    "    \n",
    "    cities_layer.add_to(m)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LAYER 3: SUCCESS STORIES (IPO + Acquired)\n",
    "    # =========================================================================\n",
    "    \n",
    "    success_layer = folium.FeatureGroup(name='Success Stories', show=False)\n",
    "    \n",
    "    success_cluster = MarkerCluster(\n",
    "        name='Success Cluster',\n",
    "        overlay=True,\n",
    "        control=False\n",
    "    ).add_to(success_layer)\n",
    "    \n",
    "    success_data = geo_data[geo_data['status'].isin(['ipo', 'acquired'])]\n",
    "    \n",
    "    for idx, row in success_data.iterrows():\n",
    "        icon_color = 'red' if row['status'] == 'ipo' else 'green'\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=3,\n",
    "            popup=f\"\"\"\n",
    "            <div style=\"font-family: Arial;\">\n",
    "                <h4 style=\"color: {icon_color};\">{row['status'].upper()}</h4>\n",
    "                <p><b>City:</b> {row.get('city', 'N/A')}<br>\n",
    "                <b>Sector:</b> {row.get('category_code', 'N/A')}<br>\n",
    "                <b>Funding:</b> ${row.get('funding_total_usd', 0)/1e6:.1f}M</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            color=icon_color,\n",
    "            fill=True,\n",
    "            fillColor=icon_color,\n",
    "            fillOpacity=0.8,\n",
    "            weight=1\n",
    "        ).add_to(success_cluster)\n",
    "    \n",
    "    success_layer.add_to(m)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ADD PROFESSIONAL FEATURES\n",
    "    # =========================================================================\n",
    "    \n",
    "    # 1. Fullscreen button\n",
    "    Fullscreen(\n",
    "        position='topleft',\n",
    "        title='Fullscreen',\n",
    "        title_cancel='Exit fullscreen',\n",
    "        force_separate_button=True\n",
    "    ).add_to(m)\n",
    "    \n",
    "    # 2. MiniMap (overview)\n",
    "    minimap = MiniMap(\n",
    "        tile_layer='CartoDB dark_matter',\n",
    "        position='bottomleft',\n",
    "        width=150,\n",
    "        height=150,\n",
    "        collapsed_width=25,\n",
    "        collapsed_height=25,\n",
    "        zoom_level_offset=-5\n",
    "    )\n",
    "    m.add_child(minimap)\n",
    "    \n",
    "    # 3. Statistics Panel (top right)\n",
    "    stats_html = f\"\"\"\n",
    "    <div style=\"position: fixed; \n",
    "                top: 10px; right: 10px; \n",
    "                width: 300px; \n",
    "                background-color: white; \n",
    "                border: 2px solid blue;\n",
    "                border-radius: 10px;\n",
    "                box-shadow: 0 4px 6px rgba(0,0,0,0.3);\n",
    "                z-index: 9999; \n",
    "                font-family: Arial;\n",
    "                padding: 15px;\n",
    "                color: black;\">\n",
    "        \n",
    "        <h3 style=\"margin-top: 0; color: blue; border-bottom: 2px solid blue; padding-bottom: 10px;\">\n",
    "            USA Startup Heatmap\n",
    "        </h3>\n",
    "        \n",
    "        <div style=\"font-size: 13px;\">\n",
    "            <p style=\"margin: 8px 0;\"><b>Total Mapped:</b> {len(geo_data):,} startups</p>\n",
    "            <p style=\"margin: 8px 0;\"><b>Coverage:</b> {100*len(geo_data)/len(finale_usa):.1f}% of dataset</p>\n",
    "            <p style=\"margin: 8px 0;\"><b>Period:</b> {int(geo_data['first_funding_year'].min())}-{int(geo_data['first_funding_year'].max())}</p>\n",
    "        </div>\n",
    "        \n",
    "        <h4 style=\"color: blue; margin-top: 15px; margin-bottom: 10px;\">Hotspots</h4>\n",
    "        \n",
    "        <h4 style=\"color: blue; margin-top: 15px; margin-bottom: 10px;\">Exit Metrics</h4>\n",
    "        \n",
    "        <div style=\"font-size: 11px;\">\n",
    "            <p style=\"margin: 5px 0;\"><b>IPO:</b> {(geo_data['status']=='ipo').sum():,} ({100*(geo_data['status']=='ipo').mean():.1f}%)</p>\n",
    "            <p style=\"margin: 5px 0;\"><b>M&A:</b> {(geo_data['status']=='acquired').sum():,} ({100*(geo_data['status']=='acquired').mean():.1f}%)</p>\n",
    "            <p style=\"margin: 5px 0;\"><b>Success:</b> {100*(geo_data['status'].isin(['ipo','acquired'])).mean():.1f}%</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    m.get_root().html.add_child(folium.Element(stats_html))\n",
    "    \n",
    "    # 4. Legend (bottom right)\n",
    "    legend_html = f\"\"\"\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 30px; right: 10px; \n",
    "                width: 250px; \n",
    "                background-color: white; \n",
    "                border: 2px solid blue;\n",
    "                border-radius: 10px;\n",
    "                box-shadow: 0 4px 6px rgba(0,0,0,0.3);\n",
    "                z-index: 9999; \n",
    "                font-family: Arial;\n",
    "                padding: 12px;\n",
    "                font-size: 12px;\n",
    "                color: balck;\">\n",
    "        \n",
    "        <h4 style=\"margin-top: 0; color: blue; border-bottom: 1px solid blue; padding-bottom: 8px;\">\n",
    "            Heatmap Legend\n",
    "        </h4>\n",
    "        \n",
    "        <p style=\"font-size: 11px; color: black; margin: 8px 0;\">\n",
    "            Density weighted by exit success:\n",
    "        </p>\n",
    "        \n",
    "        <div style=\"margin: 8px 0;\">\n",
    "            <div style=\"background: linear-gradient(to right, blue, lime, yellow, red); \n",
    "                        height: 20px; border-radius: 5px;\"></div>\n",
    "            <div style=\"display: flex; justify-content: space-between; font-size: 10px; margin-top: 5px;\">\n",
    "                <span>Low</span>\n",
    "                <span>High</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-top: 12px; font-size: 11px;\">\n",
    "            <p style=\"margin: 5px 0;\">• IPO = 5x weight</p>\n",
    "            <p style=\"margin: 5px 0;\">• M&A = 3x weight</p>\n",
    "            <p style=\"margin: 5px 0;\">• Others = 1x weight</p>\n",
    "        </div>\n",
    "        \n",
    "        <p style=\"margin-top: 10px; padding-top: 8px; border-top: 1px solid rgba(255,255,255,0.2); \n",
    "                  font-size: 10px; color: #95a5a6;\">\n",
    "            Zoom in to see individual startups<br>\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # 5. Layer Control\n",
    "    folium.LayerControl(position='topright', collapsed=False).add_to(m)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SAVE MAP\n",
    "    # =========================================================================\n",
    "    \n",
    "    heatmap_path = os.path.join(config.DATA_DIR, 'startup_heatmap_enhanced.html')\n",
    "    m.save(heatmap_path)\n",
    "    \n",
    "    print(f\"\\nENHANCED HEATMAP CREATED\")\n",
    "    print(f\"Saved to: {heatmap_path}\")\n",
    "    print(f\"File size: {os.path.getsize(heatmap_path) / 1024:.1f} KB\")\n",
    "    \n",
    "    # Display in notebook\n",
    "    display(m)\n",
    "\n",
    "else:\n",
    "    print(\"offices.csv not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
